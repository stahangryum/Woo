{
  
    
        "post0": {
            "title": "EDA HIGHLIGHT - 미완",
            "content": "5.4 &#54364;&#51456;&#54868; &#48320;&#54872; . 표준화 변환이란 통상적으로 한 자료묶음의 평균이 0, 표준편차가 1이 되도록 하는 선형변환을 말한다. | . $x_1, x_2, dots, x_n$을 자료 값이라고 할 때 이것의 표준화변환 $z_1, z_2, dots, z_n$은 다음과 같이 정한다. . $z_i = dfrac{x_i - bar{x}}{s_x}, , i = 1,2, dots,n. qquad(1)$ . 그런데 (1)은 로버스트하지 않은 $ bar{x}$와 $s_x$에 의존하므로 EDA의 관점에서는 믿고 사용하기 어렵다. [^1] . 왜냐하면 표본평균과 표본표준편차는 극단적인 이상점에 의해 크게 변동될 수 있기 때문이다. | 그러나 중앙값 또는 사분위수범위(IQR)은 비교적 로버스트하다. | . 즉, 평균 $ bar{x}$ 대신에 중앙값 $med_x$를, 표준편차 $s_x$ 대신에 사분위수범위 $IQR$을 보정한 $ tilde{ sigma_x} = dfrac{IQR}{1.35}$을 쓰는 것이 좋을 것이다. . 따라서 로버스트 표준화 변환은 다음과 같다. . $ bar{z_i} = dfrac{x_i - med_x}{ tilde{ sigma_x}} , i = 1,2, dots,n. qquad(1)$ . 표준화 변환을 사용하는 예시 상황은 다음과 같다. . [^1] 로버스트(robust) 한 통계량은 이상치/에러값으로 부터 영향을 크게 받지 않는 (건장한) 통계량 .",
            "url": "https://stahangryum.github.io/Woo/eda/r/2022/04/12/eda.html",
            "relUrl": "/eda/r/2022/04/12/eda.html",
            "date": " • Apr 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "numpy Highlight",
            "content": "Reference . ref. https://numpy.org/doc/stable/user/index.html# . ref. https://github.com/guebin/IP2022 . NumPy . Numerical Python | . Import . import numpy as np . Array Creation(ndarray) . N Dimensional Array | . A = np.array([1,2,3]) A . array([1, 2, 3]) . mylist = [3, 4, 5] array_from_list = np.array(mylist) array_from_list . array([3, 4, 5]) . mytuple = (4,5,6) array_from_tuple = np.array(mytuple) array_from_tuple . array([4, 5, 6]) . np.array(range(10)) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.linspace(0,1,12) # 0 ~ 1을 12등분하여 만듬 (끝점을 포함) . array([0. , 0.09090909, 0.18181818, 0.27272727, 0.36363636, 0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182, 0.90909091, 1. ]) . len(np.linspace(0,1,12)) . 12 . np.arange(5) . array([0, 1, 2, 3, 4]) . np.arange(1,6) . array([1, 2, 3, 4, 5]) . np.zeros(3) # 0을 3개 . array([0., 0., 0.]) . np.zeros((3,3)) . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . np.ones(3) # 1을 3개 . array([1., 1., 1.]) . np.ones((3,3)) . array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) . np.eye(3) # 단위 행렬 . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.diag([1,2,3]) # 대각선이 1,2,3인 행렬 . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . Broadcasting and Operation . A+1 # 덧셈 . array([2, 3, 4]) . A-4 # 뺄셈 . array([-3, -2, -1]) . A*2 # 곱셈 . array([2, 4, 6]) . A/2 # 나눗셈 . array([0.5, 1. , 1.5]) . A**2 # 제곱 . array([1, 4, 9]) . A%2 # 나머지 . array([1, 0, 1], dtype=int32) . np.sqrt(A) . array([1. , 1.41421356, 1.73205081]) . np.log(A) . array([0. , 0.69314718, 1.09861229]) . np.exp(A) . array([ 2.71828183, 7.3890561 , 20.08553692]) . np.sin(A) . array([0.84147098, 0.90929743, 0.14112001]) . A = np.array([11,22,33,44,55,66]) . Indexing . A[2] . 33 . A[5] . 66 . A[1:4] . array([22, 33, 44]) . A[[0,2,4]] . array([11, 33, 55]) . A[[True, False, True, False, False, True]] . array([11, 33, 66]) . A &lt; 33 . array([ True, True, False, False, False, False]) . A[A&lt;33] . array([11, 22]) . Matrix Indexing . A2 = np.array([[1,2,3,4],[-1,-2,-3,-4],[5,6,7,8],[-5,-6,-7,-8]]) A2 . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[1][3] . -4 . A2[1,3] . -4 . A2[0, 0:2] . array([1, 2]) . A2[0] . array([1, 2, 3, 4]) . A2[0, 2:] . array([3, 4]) . A2[:, :] . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[[0,2], :] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . A2[[0,2]] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . Useful Functions . .reshape . . Tip: R의 dim 함수와 유사하다. . A = np.array([11,22,33,44,55,66]) A . array([11, 22, 33, 44, 55, 66]) . A.reshape(2,3) . array([[11, 22, 33], [44, 55, 66]]) . A . array([11, 22, 33, 44, 55, 66]) . A = A.reshape(2,3) A . array([[11, 22, 33], [44, 55, 66]]) . note :reshape with -1 . A = np.arange(24) A . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . A.reshape(2,-1) # 행의 수가 2인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . A.reshape(4, -1) # 행의 수가 4인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . A.reshape(-1, 4) # 열의 수가 4인 행렬, 행은 알아서 맞춰 . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . A.reshape(-1) # 다시 길이가 24인 벡터로 . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . .shape . A = np.array(3.14) # Scalar, 0d array A.shape . () . A = np.array([3.14]) # Vector, 1d array A.shape . (1,) . A = np.array([[3.14]]) # Matrix, 2d array A.shape . (1, 1) . A = np.array([[[3.14]]]) # Tensor, 3d array A.shape . (1, 1, 1) . A = np.array([[1,2,3],[2,5,6],[4,4,2]]) A.shape . (3, 3) . np.random . np.random.randn(10) # 표준정규분포에서 10개 추출 . array([ 0.86477179, -1.55636485, 0.41814322, 0.70156177, 0.37819201, -0.37819944, 0.73782409, -1.07122884, -0.06755145, 0.39287591]) . np.random.rand(10) # 0~1사이에서 10개를 추출 . array([0.85979352, 0.77170578, 0.65476807, 0.59388851, 0.13706046, 0.78560181, 0.75412534, 0.62169035, 0.46108912, 0.2804098 ]) . np.random.randn(4).reshape(2,2) # 표준정규분포에서 10개를 추출하고 (2,2)형태의 ndarray로 변환 . array([[-0.37292687, -0.60824118], [-0.53553137, 1.73804684]]) . .T . A = np.arange(4).reshape(2,2) A . array([[0, 1], [2, 3]]) . A.T # 전치행렬 . array([[0, 2], [1, 3]]) . np.linalg.inv . np.linalg.inv(A) # 역행렬 . array([[-1.5, 0.5], [ 1. , 0. ]]) . @ . A @ np.linalg.inv(A) . array([[1., 0.], [0., 1.]]) . np.concatenate . A = np.array([1,2]) B = np.array([4,5]) np.concatenate([A, B]) . array([1, 2, 4, 5]) . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B]) # axis = 0이 생략되어 있다. . array([[ 0, 1], [ 2, 3], [10, 11], [12, 13]]) . A=np.array(range(4)).reshape(2,2) B=np.array(range(2)).reshape(2,1) np.concatenate([a,b],axis=1) # 꼭 같은 차원일 필요는 없고, 붙여지는 부분의 길이만 같으면 됨 . array([[0, 1, 0], [2, 3, 1]]) . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B], axis=1) . array([[ 0, 1, 10, 11], [ 2, 3, 12, 13]]) . A = np.array(range(2*3*4)).reshape(2,3,4) B = - A A, B . (array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]), array([[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]])) . . np.concatenate([A,B], axis=0) . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]]) . . np.concatenate([A,B], axis=1) . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]]) . . np.concatenate([A,B], axis=2) . array([[[ 0, 1, 2, 3, 0, -1, -2, -3], [ 4, 5, 6, 7, -4, -5, -6, -7], [ 8, 9, 10, 11, -8, -9, -10, -11]], [[ 12, 13, 14, 15, -12, -13, -14, -15], [ 16, 17, 18, 19, -16, -17, -18, -19], [ 20, 21, 22, 23, -20, -21, -22, -23]]]) . . np.stack . Warning : . A = np.array([1,2,3]) B = np.array([2,3,4]) np.concatenate([A,B], axis = 1) . AxisError Traceback (most recent call last) Input In [129], in &lt;cell line: 3&gt;() 1 A = np.array([1,2,3]) 2 B = np.array([2,3,4]) -&gt; 3 np.concatenate([A,B], axis = 1) File &lt;__array_function__ internals&gt;:180, in concatenate(*args, **kwargs) AxisError: axis 1 is out of bounds for array of dimension 1 . A = np.array([1,2,3]) B = np.array([2,3,4]) np.stack([A,B], axis=0) . array([[1, 2, 3], [2, 3, 4]]) . np.stack([A,B], axis=1) . array([[1, 2], [2, 3], [3, 4]]) . A = np.arange(3*4*5).reshape(3,4,5) B = - A A.shape, B.shape . ((3, 4, 5), (3, 4, 5)) . np.stack([A,B], axis = 0) . np.stack([A,B], axis = 0).shape # axis = 0 &lt;==&gt; axis = -4 . (2, 3, 4, 5) . np.stack([A,B], axis = 1).shape # axis = 1 &lt;==&gt; axis = -3 . (3, 2, 4, 5) . np.stack([A,B], axis = 2).shape # axis = 2 &lt;==&gt; axis = -2 . (3, 4, 2, 5) . np.stack([A,B], axis = 3).shape # axis = 3 &lt;==&gt; axis = -1 . (3, 4, 5, 2) . Difference between np.concatenate and np.stack . np.concatenate는 축의 총 개수를 유지하면서 결합한다. . np.stack은 축의 개수를 하나 증가시키면서 결합한다. . .sum . A = np.array([1,2,3]) A.sum() . 6 . .mean . A = np.array([1,2,3]) A.mean() . 2.0 . .min . A = np.array([1,2,3]) A.min() . 1 . .max . A = np.array([1,2,3]) A.max() . 3 . .prod . A = np.array([1,2,3]) A.prod() . 6 . .std . A = np.arange(1,20) A.std() # 분포를 n으로 나누는 것이 default이다. . 5.477225575051661 . A = A = np.array([1,2,3]) A.std(ddof=1) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.0 . A = A = np.array([1,2,3]) A.std(ddof=2) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.4142135623730951 . .argmin . A = np.array([1,2,3]) # 가장 작은 값의 인덱스를 리턴 A.argmin() . 0 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmin(axis = 0) . array([2, 1, 1, 1, 2], dtype=int64) . A.argmin(axis = 1) . array([4, 3, 4, 3], dtype=int64) . .argmax . A = np.array([1,2,3]) # 가장 큰 값의 인덱스를 리턴 A.argmax() . 2 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmax(axis = 0) . array([3, 0, 0, 2, 0], dtype=int64) . A.argmax(axis = 1) . array([2, 0, 3, 0], dtype=int64) . .cumsum . A = np.array([1,2,3,4]) A.cumsum() # 누적합 . array([ 1, 3, 6, 10]) . .cumprod . A = np.array([1,2,3,4]) A.cumprod() # 누적곱 . array([ 1, 2, 6, 24]) . np.diff . A = np.array([1,2,4,6,7]) np.diff(A) . array([1, 2, 2, 1]) . np.diff(np.diff(A)) . array([ 1, 0, -1]) . np.diff(A, prepend=100) # np.diff(np.array([100] + A.tolist()) ), 즉 맨 앞에 100을 추가하여 차분 . array([-99, 1, 2, 2, 1]) . np.diff(A, append=100) # np.diff(np.array(A.tolist() + [100]) ), 즉 맨 뒤에 100을 추가하여 차분 . array([ 1, 2, 2, 1, 93]) . A = np.arange(24).reshape(4,6) A . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . np.diff(A, axis = 0) . array([[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6]]) . np.diff(A, axis = 1) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . Applications . Solving System of Equations . $ begin{cases} y+z+w = 3 x+z+w = 3 x+y+w = 3 x+y+z = 3 end{cases}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . v = &#39;x&#39;, &#39;y&#39;, &#39;z&#39; A = np.linalg.inv(np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])) B = np.array([3,3,3,3]).reshape(4,1) answer = A@B.reshape(-1) for v, i in zip(v, answer): print(v, &#39;:&#39;, i) . x : 1.0 y : 1.0 z : 1.0 .",
            "url": "https://stahangryum.github.io/Woo/python/python%20tutorial/2022/04/11/numpy_highlight.html",
            "relUrl": "/python/python%20tutorial/2022/04/11/numpy_highlight.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Pima Indians Diabetes Prediction",
            "content": "Reference . ref. https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database . Pima Indians Diabetes Prediction . Variable Definition . Pregnancies | 임신 횟수 | . Glucose | 포도당 부하 검사 수치 | . BloodPressure | 혈압 | . SkinThickness | 팔 삼두근 뒤쪽의 피하지방 측정값(mm) | . Inlulin | 혈청 인슐린(mu U/ml) | . BMI | 체질량지수$( frac{kg}{m^2})$ | . DiabetesPredigreeFunction | 당뇨 내력 가중치 값 | . Age | 나이 | . Outcome | 클래스 결정 값(0 또는 1) | . Packages . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import Binarizer import warnings warnings.filterwarnings(action=&#39;ignore&#39;) . Preprocessing . diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) diabetes_data.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes_data.Outcome.value_counts() . 0 500 1 268 Name: Outcome, dtype: int64 . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . Glucose, BloodPressure, SkinThickness, Insulin, Bmi은 0이면 안된다. | . zero_features = [&#39;Glucose&#39;, &#39;BloodPressure&#39;, &#39;SkinThickness&#39;, &#39;Insulin&#39;, &#39;BMI&#39;] for i, feature in enumerate(zero_features): plt.subplot(3,2,i+1) plt.hist(diabetes_data[feature]) . diabetes_data[diabetes_data[&#39;Glucose&#39;] == 0][&#39;Glucose&#39;].count() . 5 . for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature] == 0][feature].count() print(&#39;{0} 0 건수는 {1}, 퍼센트는 {2:.2f}%&#39;.format(feature, zero_count, 100*zero_count / diabetes_data[feature].count())) . Glucose 0 건수는 5, 퍼센트는 0.65% BloodPressure 0 건수는 35, 퍼센트는 4.56% SkinThickness 0 건수는 227, 퍼센트는 29.56% Insulin 0 건수는 374, 퍼센트는 48.70% BMI 0 건수는 11, 퍼센트는 1.43% . SkinThickness, Insulin feature가 0인 행을 지우면 데이터 손실이 너무 크므로 평균값으로 대체한다. . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features] = diabetes_data[zero_features].replace(0, mean_zero_features) . Prediction . def precision_recall_curve_plot(y_test, pred_proba_c1): precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1) plt.figure(figsize=(8,6)) threshold_boundary = thresholds.shape[0] plt.plot(thresholds, precisions[0:threshold_boundary], linestyle = &#39;-&#39;, label = &#39;precision&#39;) plt.plot(thresholds, recalls[0:threshold_boundary], label = &#39;recall&#39;) start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1), 2)) plt.xlabel(&#39;Threshold value&#39;); plt.ylabel(&#39;Precision and Recall value&#39;) plt.legend(); plt.grid() plt.show() def get_clf_eval(y_test, pred=None, pred_proba=None): # 모델 평가 함수 confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) f1 = f1_score(y_test, pred) roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) print(&#39;정확도 : {0:.3f}, 정밀도 : {1:.3f}, 재현율 : {2:.3f}, F1 : {3:.3f}, AUC : {4:.3f}&#39;.format( accuracy, precision, recall, f1, roc_auc)) def get_eval_by_threshold(y_test, pred_proba_c1, thresholds): for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_predict = binarizer.transform(pred_proba_c1) print(&#39;-&#39;) print(&#39;임계값:&#39;, round(custom_threshold,2)) get_clf_eval(y_test, custom_predict, pred_proba_c1) . . feature_name = diabetes_data.columns[:-1] target_name = diabetes_data.columns[-1] X = diabetes_data.loc[:, feature_name] y = diabetes_data.loc[:, target_name] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y) . lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba = lr_clf.predict_proba(X_test)[:, 1] get_clf_eval(y_test, pred, pred_proba) . 오차 행렬 [[90 10] [21 33]] 정확도 : 0.799, 정밀도 : 0.767, 재현율 : 0.611, F1 : 0.680, AUC : 0.845 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1] precision_recall_curve_plot(y_test, pred_proba_c1) . thresholds = np.arange(0.3, 0.5, 0.03) pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:, 1].reshape(-1,1), thresholds) . - 임계값: 0.3 오차 행렬 [[67 33] [11 43]] 정확도 : 0.714, 정밀도 : 0.566, 재현율 : 0.796, F1 : 0.662, AUC : 0.845 - 임계값: 0.33 오차 행렬 [[73 27] [12 42]] 정확도 : 0.747, 정밀도 : 0.609, 재현율 : 0.778, F1 : 0.683, AUC : 0.845 - 임계값: 0.36 오차 행렬 [[76 24] [15 39]] 정확도 : 0.747, 정밀도 : 0.619, 재현율 : 0.722, F1 : 0.667, AUC : 0.845 - 임계값: 0.39 오차 행렬 [[79 21] [17 37]] 정확도 : 0.753, 정밀도 : 0.638, 재현율 : 0.685, F1 : 0.661, AUC : 0.845 - 임계값: 0.42 오차 행렬 [[84 16] [18 36]] 정확도 : 0.779, 정밀도 : 0.692, 재현율 : 0.667, F1 : 0.679, AUC : 0.845 - 임계값: 0.45 오차 행렬 [[85 15] [18 36]] 정확도 : 0.786, 정밀도 : 0.706, 재현율 : 0.667, F1 : 0.686, AUC : 0.845 - 임계값: 0.48 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 . . binarizer = Binarizer(threshold=0.48) pred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) get_clf_eval(y_test, pred_th_048, pred_proba[:, 1]) . 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/07/pima.html",
            "relUrl": "/kaggle/2022/04/07/pima.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Titanic Survivor Prediction",
            "content": "Reference . ref. https://www.kaggle.com/c/titanic/ . Titanic Survivor Prediction . 타이타닉호 침몰 사고 당시 탑승자들의 정보를 활용하여 생존자를 예측하라. . Data Dictionary . Variable Definition Key . Survived | Survival | 0 = No, 1 = Yes | . Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . Sex | Sex | . Age | Age in years | . SibSp | # of siblings / spouses aboard the Titanic | . Parch | # of parents / children aboard the Titanic | . Ticket | Ticket number | . Fare | Passenger fare | . Cabin | Cabin number | . Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . Variable Notes . pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower . age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 . sibsp: The dataset defines family relations in this way... Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fiancés were ignored) . parch: The dataset defines family relations in this way... Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them. . Stage I . import . import numpy as np import pandas as pd . code . import os print(os.getcwd()) . C: Users godgk Desktop Project kaggle Titanic . train = pd.read_csv(&#39;data/train.csv&#39;) test = pd.read_csv(&#39;data/test.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . print(&#39;train data shape&#39;, train.shape) print(&#39;test data shape&#39;, test.shape) print(&#39;--[train infomation]--&#39;) print(train.info()) print(&#39;--[test infomation]--&#39;) print(test.info()) . train data shape (891, 12) test data shape (418, 11) --[train infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB None --[test infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB None . train.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . test.isnull().sum() . PassengerId 0 Pclass 0 Name 0 Sex 0 Age 86 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 . Stage II . import . import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set() . def pie_chart(feature): feature_ratio = train[feature].value_counts(sort=False) feature_size = feature_ratio.size feature_index = feature_ratio.index survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() plt.plot(aspect=&#39;auto&#39;) plt.pie(feature_ratio, labels=feature_index, autopct=&#39;%1.1f%%&#39;) plt.title(feature + &#39; &#39;s ratio in total&#39;) plt.show() for i, index in enumerate(feature_index): plt.subplot(1, feature_size + 1, i + 1, aspect=&#39;equal&#39;) plt.pie([survived[index], dead[index]], labels=[&#39;Survivied&#39;, &#39;Dead&#39;], autopct=&#39;%1.1f%%&#39;) plt.title(str(index) + &#39; &#39;s ratio&#39;) plt.show() . pie_chart(&quot;Sex&quot;) . 남성 탑승객이 여성 탑승객보다 많다. . | 여성 탑승객의 생존 비율이 남성 탑승객보다 높다. . | . pie_chart(&quot;Pclass&quot;) . 1등실 2등실 3등실 순으로 생존 비율이 높다. | . pie_chart(&quot;Embarked&quot;) . train[&#39;Ticket&#39;][0:50] . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 5 330877 6 17463 7 349909 8 347742 9 237736 10 PP 9549 11 113783 12 A/5. 2151 13 347082 14 350406 15 248706 16 382652 17 244373 18 345763 19 2649 20 239865 21 248698 22 330923 23 113788 24 349909 25 347077 26 2631 27 19950 28 330959 29 349216 30 PC 17601 31 PC 17569 32 335677 33 C.A. 24579 34 PC 17604 35 113789 36 2677 37 A./5. 2152 38 345764 39 2651 40 7546 41 11668 42 349253 43 SC/Paris 2123 44 330958 45 S.C./A.4. 23567 46 370371 47 14311 48 2662 49 349237 Name: Ticket, dtype: object . train.Ticket . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 ... 886 211536 887 112053 888 W./C. 6607 889 111369 890 370376 Name: Ticket, Length: 891, dtype: object . Stage 3 . def bar_chart(feature): survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() df = pd.DataFrame([survived, dead]) df.index = [&#39;Survived&#39;, &#39;Dead&#39;] df.plot(kind=&#39;bar&#39;, stacked=True, figsize=(10,5)) . bar_chart(&quot;SibSp&quot;) . bar_chart(&quot;Parch&quot;) . Data Preprocessing . train_and_test = [train, test] . Name Feature . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | Mr | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | Mrs | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | Miss | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | Mrs | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | Mr | . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Capt&#39;, &#39;Col&#39;, &#39;Countess&#39;, &#39;Don&#39;,&#39;Dona&#39;, &#39;Dr&#39;, &#39;Jonkheer&#39;,&#39;Lady&#39;,&#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;], &#39;Other&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Mlle&#39;, &#39;Ms&#39;], &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Master 0 | 40 | . Miss 185 | 0 | . Mr 0 | 517 | . Mrs 126 | 0 | . Other 3 | 20 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;).mean() . Survived . Title . Master 0.575000 | . Miss 0.702703 | . Mr 0.156673 | . Mrs 0.793651 | . Other 0.347826 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;, as_index = False).mean() # as_index = True이면 Title이 index로 작용한다. . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Other | 0.347826 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].astype(str) . Sex Feature . for dataset in train_and_test: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].astype(str) . Embarked Feature . train.Embarked.value_counts(dropna=False) . S 644 C 168 Q 77 NaN 2 Name: Embarked, dtype: int64 . for dataset in train_and_test: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(&#39;S&#39;) dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].astype(str) . Age Feature . Binning . train.Age.isna().sum() . 177 . for dataset in train_and_test: dataset[&#39;Age&#39;].fillna(dataset[&#39;Age&#39;].mean(), inplace=True) dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].astype(int) train[&#39;AgeBand&#39;] = pd.cut(train[&#39;Age&#39;], 5) train[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;], as_index=False).mean() . AgeBand Survived . 0 (-0.08, 16.0] | 0.550000 | . 1 (16.0, 32.0] | 0.344762 | . 2 (32.0, 48.0] | 0.403226 | . 3 (48.0, 64.0] | 0.434783 | . 4 (64.0, 80.0] | 0.090909 | . for dataset in train_and_test: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] = 4 dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].map( { 0:&#39;Child&#39;, 1:&#39;Young&#39;, 2:&#39;Middle&#39;, 3:&#39;Prime&#39;, 4:&#39;Old&#39; } ).astype(str) . Fare Feature . for dataset in train_and_test: print(dataset[&#39;Fare&#39;].isna().sum()) . 0 1 . train[[&#39;Pclass&#39;, &#39;Fare&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean() . Pclass Fare . 0 1 | 84.154687 | . 1 2 | 20.662183 | . 2 3 | 13.675550 | . test[test[&#39;Fare&#39;].isna()][&#39;Pclass&#39;] . 152 3 Name: Pclass, dtype: int64 . for dataset in train_and_test: dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].fillna(13.675) # Pclass가 3인 승객의 평균 Fare . train[&#39;FareBand&#39;] = pd.qcut(train[&#39;Fare&#39;], 5) for dataset in train_and_test: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.854, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.854) &amp; (dataset[&#39;Fare&#39;] &lt;= 10.5), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 10.5) &amp; (dataset[&#39;Fare&#39;] &lt;= 21.679), &#39;Fare&#39;] = 2 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 21.679) &amp; (dataset[&#39;Fare&#39;] &lt;= 39.688), &#39;Fare&#39;] = 3 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 39.688, &#39;Fare&#39;] = 4 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].map( { 0:&#39;XS&#39;, 1:&#39;S&#39;, 2:&#39;M&#39;, 3:&#39;L&#39;, 4:&#39;XL&#39; } ).astype(str) . SibSp &amp; Parch Feature (Family) . for dataset in train_and_test: dataset[&#39;Family&#39;] = dataset[&#39;Parch&#39;] + dataset[&#39;SibSp&#39;] dataset[&#39;Family&#39;] = dataset[&#39;Family&#39;].astype(int) . Other Feature . features_drop = [&#39;Name&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;SibSp&#39;, &#39;Parch&#39;] train = train.drop(features_drop, axis = 1) test = test.drop(features_drop, axis = 1) train = train.drop([&#39;PassengerId&#39;, &#39;AgeBand&#39;, &#39;FareBand&#39;], axis = 1) . train.head() . Survived Pclass Sex Age Fare Embarked Title Family . 0 0 | 3 | male | Young | XS | S | Mr | 1 | . 1 1 | 1 | female | Middle | XL | C | Mrs | 1 | . 2 1 | 3 | female | Young | S | S | Miss | 0 | . 3 1 | 1 | female | Middle | XL | S | Mrs | 1 | . 4 0 | 3 | male | Middle | S | S | Mr | 0 | . test.head() . PassengerId Pclass Sex Age Fare Embarked Title Family . 0 892 | 3 | male | Middle | XS | Q | Mr | 0 | . 1 893 | 3 | female | Middle | XS | S | Mrs | 1 | . 2 894 | 2 | male | Prime | S | Q | Mr | 0 | . 3 895 | 3 | male | Young | S | S | Mr | 0 | . 4 896 | 3 | female | Young | M | S | Mrs | 2 | . train = pd.get_dummies(train) test = pd.get_dummies(test) train_label = train[&#39;Survived&#39;] train_data = train.drop(&#39;Survived&#39;, axis = 1) test_data = test.drop(&#39;PassengerId&#39;, axis = 1).copy() . print(train_data.shape, train_label.shape, test_data.shape) . (891, 22) (891,) (418, 22) . train . Survived Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 0 | 3 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 1 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 3 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 4 0 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 887 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 888 0 | 3 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 889 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 890 0 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 891 rows × 23 columns . test . PassengerId Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 892 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 893 | 3 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 2 894 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 3 895 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 4 896 | 3 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 414 1306 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 415 1307 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 416 1308 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 417 1309 | 3 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 418 rows × 23 columns . Learning . import . !pip install scikit-learn . Requirement already satisfied: scikit-learn in c: users godgk anaconda3 envs py39r40 lib site-packages (1.0.2) Requirement already satisfied: numpy&gt;=1.14.6 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.20.3) Requirement already satisfied: scipy&gt;=1.1.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.7.1) Requirement already satisfied: joblib&gt;=0.11 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (3.1.0) . from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn.utils import shuffle . train_data, train_label = shuffle(train_data, train_label, random_state = 5) . def train_and_test(model): model.fit(train_data, train_label) prediction = model.predict(test_data) accuracy = round(model.score(train_data, train_label) * 100, 2) print(&quot;Accuracy : &quot;, accuracy, &quot;%&quot;) return prediction . log_pred = train_and_test(LogisticRegression()) # SVM svm_pred = train_and_test(SVC()) #kNN knn_pred_4 = train_and_test(KNeighborsClassifier(n_neighbors = 4)) # Random Forest rf_pred = train_and_test(RandomForestClassifier(n_estimators=100)) # Navie Bayes nb_pred = train_and_test(GaussianNB()) . Accuracy : 82.27 % Accuracy : 83.61 % Accuracy : 84.74 % Accuracy : 88.55 % Accuracy : 79.35 % . submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: rf_pred }) submission.to_csv(&#39;submission_rf.csv&#39;, index=False) .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/06/Titanic.html",
            "relUrl": "/kaggle/2022/04/06/Titanic.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Anaconda Install Tutorial",
            "content": "https://www.anaconda.com/products/individual . conda create -n py39r41 python=3.9 . | conda activate py39r41 . | conda install -c conda-forge r-essentials=4.1 . | conda install -c conda-forge jupyterlab . | R . | install.packages(&quot;IRkernel&quot;) . | library(IRkernel) . | installspec() . | q() . | pip install numpy . | pip install SciPy . | pip install matplotlib pip install pandas . | . . {python} !git add . !git commit -m . !git push .",
            "url": "https://stahangryum.github.io/Woo/2022/03/15/anaconda_install.html",
            "relUrl": "/2022/03/15/anaconda_install.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Monte Carlo Integration with Python",
            "content": "&#47788;&#53580; &#52852;&#47484;&#47196; &#51201;&#48516;(Monte Carlo Integration) . Problem . Find $ int_0^1 (x + sin( pi x)) ,dx$. . Solution I - Analytic Sol . $ int_0^1 (x + sin( pi x)) ,dx = left[ cfrac{1}{2} x^2- cfrac{1}{ pi}cos( pi x) right]_0^1 = cfrac{1}{2} + cfrac{1}{ pi} + cfrac{1}{ pi} = cfrac{1}{2}+ cfrac{2}{ pi}$ . Solution II - Monte Carlo Integration in Python . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . def function(x): return x + np.sin(np.pi*x) x = np.linspace(0, 1, 10000) y = [function(x) for x in x] plt.plot(x, y) plt.show() . def function(x): #함수 정의 return x + np.sin(np.pi*x) N = 5000 # Random Sampling 시행 횟수 width = 1 # 사각형의 가로 길이 height = 1.6 #사각형의 세로 길이 X = np.random.random(N) # 0~1까지의 x 좌표 Random Sampling을 N번 시행 rand_Y = height * np.random.random(N) # 그래프상 최솟값 ~ 최댓값까지의 y 좌표 Randon Sampling을 N번 시행 in_or_out = rand_Y &lt; function(X) # rand_Y &lt; FX (IN)이면 True, Y &gt; F (OUT)이면 False A = height * width * np.sum(in_or_out) / N # 영역 S의 넓이 print(&#39;이론적으로 구한 값 : {0} n샘플링으로 구한 값 : {1}&#39;.format(1/2 + 2/np.pi, A)) . 이론적으로 구한 값 : 1.1366197723675815 샘플링으로 구한 값 : 1.14528 . Visualization . from matplotlib.animation import FuncAnimation . color = list(map(lambda x: &#39;blue&#39; if x == True else &#39;red&#39;, in_or_out)) #색 정하기 x = np.linspace(0, 1, 10000) #함수 그리기 y = [function(x) for x in x] plt.plot(x, y, color = &#39;black&#39;) plt.scatter(X, rand_Y, color = color, s=1, label=&#39;A = {0}&#39;.format(np.round(A, 4))) plt.legend(loc = &#39;lower right&#39;) #범례(legend) 위치 plt.plot([0, width], [0, 0], color=&#39;black&#39;) # 사각형 영역 plt.plot([width, width], [0, height], color=&#39;black&#39;) plt.plot([0, width], [height, height], color=&#39;black&#39;) plt.plot([0, 0], [0, height], color=&#39;black&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() .",
            "url": "https://stahangryum.github.io/Woo/python/2021/10/03/%EB%AA%AC%ED%85%8C-%EC%B9%B4%EB%A5%BC%EB%A1%9C.html",
            "relUrl": "/python/2021/10/03/%EB%AA%AC%ED%85%8C-%EC%B9%B4%EB%A5%BC%EB%A1%9C.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Taylor & Maclaurin Series",
            "content": "Taylor Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(a)}{n!}(x-a)^n qquad quad= f(a) + cfrac{f&#39;(a)}{1!}(x-a) + cfrac{f&#39;&#39;(a)}{2!}(x-a) + cfrac{f&#39;&#39;&#39;(a)}{3!}(x-a)+ cdots $ . Maclaurin Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(0)}{n!}(x-0)^n qquad quad = f(a) + cfrac{f^{ prime}(0)}{1!}(x-0) + cfrac{f^{ prime prime}(0)}{2!}(x-0) + cfrac{f^{ prime prime prime}(0)}{3!}(x-0)+ cdots $ . Examples . $ cfrac{1}{1-x} = sum_{n=0}^ infty{x^n} = 1+x+x^2+x^3+ cdots qquad R = 1 , e^x = sum_{n=0}^ infty cfrac{x^n}{n!} = 1 + cfrac{x}{1!} + cfrac{x^2}{2!} + cfrac{x^3}{3!}+ cdots qquad R = infty , sin ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)!} = x - cfrac{x^3}{3!} + cfrac{x^5}{5!} - cfrac{x^7}{7!}+ cdots qquad R = infty , cos ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n}}{(2n)!} = 1 - cfrac{x^2}{2!} + cfrac{x^4}{4!} - cfrac{x^6}{6!}+ cdots qquad R = infty , tan^{-1} ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)} = x - cfrac{x^3}{3} + cfrac{x^5}{5} - cfrac{x^7}{7}+ cdots qquad R = 1 , ln(1+x) = sum_{n=1}^ infty(-1)^{n-1} cfrac{x^{n}}{n} = x - cfrac{x^2}{2} + cfrac{x^3}{3} - cfrac{x^4}{4}+ cdots qquad R = 1 $ .",
            "url": "https://stahangryum.github.io/Woo/calculus/2021/10/02/%ED%85%8C%EC%9D%BC%EB%9F%AC-%EA%B8%89%EC%88%98.html",
            "relUrl": "/calculus/2021/10/02/%ED%85%8C%EC%9D%BC%EB%9F%AC-%EA%B8%89%EC%88%98.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://stahangryum.github.io/Woo/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://stahangryum.github.io/Woo/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Kim Jeewoo . Jeonbuk Nation University. Statistics . GitHub . | LinkedIn . | Blog . | . . Contact . stahangryum@gmail.com .",
          "url": "https://stahangryum.github.io/Woo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stahangryum.github.io/Woo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}