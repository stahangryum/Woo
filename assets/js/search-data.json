{
  
    
        "post0": {
            "title": "2021-1학기 수학 1 중간고사",
            "content": "Problem 1 . $ displaystyle int dfrac{1}{9x^2 + 6x + 5} dx$ 를 역치환을 활용하여 부정적분을 계산한다면 어떠한 삼각함수를 역치환에 활용할지 치환방법만 구체적으로 적고, 풀이과정은 적지 않는다. . Problem 2 . 주어진 적분을 하여라. 단, 답은 반드시 x에 관해 정리해서 마무리하고, 풀이과정은 자세히 적는다. . $ displaystyle int dfrac{2x^4 + 6x^3 + 15x^2 + x - 21}{x^2(x^2+2x+7)}dx$ . Problem 3 . 주어진 적분을 하여라. 단, 답은 반드시 x에 관해 정리해서 마무리하고, 풀이과정은 자세히 적는다. . $ displaystyle int xsin^{-1}x^2dx$ . Problem 5 . 주어진 부정적분 $ displaystyle int dfrac{x^2}{ sqrt{4-2x-x^2}}dx$의 풀이과정을 자세히 적고 답을 x에 관하여 정리하라. . Problem 6 . $ displaystyle int dfrac{x^2-5+6}{(2x+1)(x-4)^2}dx$를 찾아라. . Problem 7 . $ displaystyle int _ frac{1}{x^2+5} ^{ frac{ pi}{2}} sqrt{ sin{x} + 3}$를 미분하여라. .",
            "url": "https://stahangryum.github.io/Woo/mathematics/2022/06/04/calculus_one_final.html",
            "relUrl": "/mathematics/2022/06/04/calculus_one_final.html",
            "date": " • Jun 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "음악 제작사&유통사 영향력과 가온지수의 관계",
            "content": "&#44060;&#50836; . 가온 차트(Gaon Chart)는 대한민국의 대중음악 공인차트이다. . 가온 주간 디지털 차트에서는 노래의 제작사와 배급사를 명시하고 있는데 제작사 또는 배급사의 영향력이 강할 수록 가온지수가 높지 않을까? 또한 별개로 특수한 경우가 있을까? . TOP 200 차트에서 제작사와 배급사의 출현 빈도를 영향력으로 간주하고 가온지수와의 상관관계를 관찰해보자. . 가온 차트 데이터는 아래 링크에서 얻을 수 있다. . http://gaonchart.co.kr/main/section/chart/online.gaon?nationGbn=T&amp;serviceGbn=ALL . *참고로 크롤링 코드는 해당 파일에 포함하지 않았다. . 크롤링 코드 : https://github.com/stahangryum/Woo/blob/529c964bf569b2e7341e046bab138bc515af3aa6/_notebooks/2022-05-29-GAON.ipynb . imports . 분석에 필요한 데이터 및 라이브러리를 가져온다. . if (!require(pacman)) install.packages(&#39;pacman&#39;) library(pacman) pacman::p_load(&quot;tidyverse&quot;, &quot;stringr&quot;, &quot;repr&quot;) # 필요한 파일을 불러온다. gaon_month &lt;- read.table(&#39;results/gaon_month.txt&#39;, header = TRUE) gaon_week &lt;- read.table(&#39;results/gaon_week.txt&#39;, header = TRUE) # 가온지수가 존재하지 않는 행은 제거한다. gaon_month_true &lt;- gaon_month %&gt;% drop_na(gaon_index) gaon_week_true &lt;- gaon_week %&gt;% drop_na(gaon_index) options(repr.plot.width=12, repr.plot.height=7) options(scipen=999) . &#51452;&#44036; &#44032;&#50728;&#51648;&#49688; - &#50976;&#53685;&#49324;(distribution) . dist_by &lt;- gaon_week_true %&gt;% group_by(distribution) %&gt;% summarise(median = median(gaon_index), size = length(gaon_index)) %&gt;% filter(size &gt; 3) head(dist_by) . A tibble: 6 × 3 distributionmediansize . &lt;chr&gt;&lt;dbl&gt;&lt;int&gt; . 2%엔터테인먼트 | 2202147 | 7 | . Dreamus | 6413856 | 5812 | . Kakao Entertainment | 6063353 | 14778 | . NHN벅스 | 5041087 | 1279 | . Sony Music | 3724498 | 1674 | . Stone Music Entertainment | 9973699 | 1155 | . 위 데이터프레임에서 median 컬럼은 유통사의 가온지수 중앙값을 의미하고 size 컬럼은 차트 내 유통사 출현 빈도를 의미한다. 이제 산점도를 그려보자. . p &lt;- ggplot(data = dist_by, aes(x = size, y = median)) + geom_point() p + xlab(&#39;차트 내 유통사 출현 빈도&#39;) + ylab(&#39;유통사의 가온지수 중앙값&#39;) . 위 그래프는 차트 내 유통사 출현 빈도와 유통사의 가온지수 중앙값을 산점도로 나타낸 것이다. . 그런데 대부분의 점이 좌측 하단에 몰려있어 변수 사이의 관계를 파악하기 어렵다. . 따라서 로그 변환을 취한 뒤 다시 산점도를 그려보고 회귀선도 그려보자. . p + scale_x_continuous(trans = &#39;log2&#39;) + scale_y_continuous(trans = &#39;log2&#39;) + geom_smooth(method = &#39;lm&#39;, col = &#39;red&#39;) + xlab(&#39;log(차트 내 유통사 출현 빈도)&#39;) + ylab(&#39;log(제작사의 가온지수 중앙값)&#39;) . `geom_smooth()` using formula &#39;y ~ x&#39; . 로그 변환을 취한 위 산점도에서 양의 상관관계를 보이고 있다. . 그러나 좌측 상단에 선형성을 크게 벗어나는 두 점이 존재하는데 케미컬레코드 와 미러볼뮤직 이다. . 이는 각각 롤린 - 브레이브걸스 (Brave Girls), 비행운 - 문문 (MoonMoon) 두 곡이 역주행하며 발생한 현상이다. . 역주행 : 발매 후 상당 시간 주목받지 못하던 노래가 어떤 사유로 갑자기 재조명되어 음악 차트 순위가 크게 상승하는 현상. | . 대부분의 곡들은 발매 직후 적당한 순위를 기록하였다가 인기가 점차 식으면서 순위가 내려가는 경우가 보통이지만 이 경우는 반대인 것이다. . 실제로 롤린의 곡 발매일은 2017년 3월 7일이지만 가온차트 첫 진입은 2021년 10주차이다. . 당시 브레이브걸스는 5년차 무명 아이돌 그룹으로, 계속되는 부진에 해체 수순을 밟고 있었으나 2021년 Youtube에 업로드된 3분 길이의 영상(https://www.youtube.com/watch?v=cfHWIqJkEf4)이 우연히 주목받게 되었고 엄청난 돌풍을 일으키며 국내 모든 음원차트에서 1위를 달성하였다. . 비행운 또한 발매일은 2016년 11월 10일이지만 가온차트 첫 진입은 2018년 1주차이다. . 문문은 무명 가수였으나 한 유명 프로듀서가 곡을 언급하자 입소문을 타며 큰 인기를 얻었고 역주행하여 지니뮤직 차트 1위를 달성하고 다른 차트에서도 상위 랭킹을 달성했다. . dist_by[order(dist_by$median, decreasing = T),] %&gt;% head(4) . A tibble: 4 × 3 distributionmediansize . &lt;chr&gt;&lt;dbl&gt;&lt;int&gt; . 케미컬레코드 | 25105123 | 23 | . 미러볼뮤직 | 19128320 | 28 | . Stone Music Entertainment | 9973699 | 1155 | . 퍼플파인 엔터테인먼트 | 8326852 | 210 | . &#51452;&#44036; &#44032;&#50728;&#51648;&#49688; - &#51228;&#51089;&#49324;(production) . prod_by &lt;- gaon_week_true %&gt;% group_by(production) %&gt;% summarise(median = median(gaon_index), size = length(gaon_index)) %&gt;% filter(size &gt;= 3) . 위 데이터프레임에서 median 컬럼은 제작사의 가온지수 중앙값을 의미하고 size 컬럼은 차트 내 제작사 출현 빈도를 의미한다. 이제 산점도를 그려보자. . p &lt;- ggplot(data = prod_by, aes(x = size, y = median)) + geom_point() p + xlab(&#39;차트 내 제작사 출현 빈도&#39;) + ylab(&#39;제작사의 가온지수 중앙값&#39;) . 위 그래프는 차트 내 제작사 출현 빈도와 유통사의 가온지수 중앙값을 산점도로 나타낸 것이다. . 그런데 대부분의 점이 좌측 하단에 몰려있어 변수 사이의 관계를 파악하기 어렵다. . 따라서 로그 변환을 취하여 다시 산점도를 그려보고 회귀선도 그려보자. . p + scale_x_continuous(trans = &#39;log10&#39;) + scale_y_continuous(trans = &#39;log10&#39;) + geom_smooth(method = &#39;lm&#39;, col = &#39;red&#39;) + xlab(&#39;log(차트 내 제작사 출현 빈도)&#39;) + ylab(&#39;log(제작사의 가온지수 중앙값)&#39;) . `geom_smooth()` using formula &#39;y ~ x&#39; . 위 산점도에서 양의 상관관계를 보이고 있다. .",
            "url": "https://stahangryum.github.io/Woo/r/2022/06/03/gaon_production.html",
            "relUrl": "/r/2022/06/03/gaon_production.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "코로나 전후 예술 산업 변화 관찰",
            "content": "&#44060;&#50836; . 코로나 발생으로 인해 예술 산업은 타격을 받았을까? . TOP 200 가온음악차트에서 시간에 따른 가온지수 총량 변화를 관찰해보자. . 영화 산업은 또 어떠한가? 박스오피스 데이터에서 코로나 전후 매출액, 관객수 변화를 관찰해보자. . &#53076;&#47196;&#45208; &#51204;&#54980; &#44032;&#50728;&#51648;&#49688; &#52509;&#47049; &#48320;&#54868; . imports . 분석에 필요한 데이터 및 라이브러리를 가져온다. . 가온 차트 데이터는 아래 링크에서 얻을 수 있다. . http://gaonchart.co.kr/main/section/chart/online.gaon?nationGbn=T&amp;serviceGbn=ALL . *참고로 크롤링 코드는 해당 파일에 포함하지 않았다. . 크롤링 코드 : https://github.com/stahangryum/Woo/blob/529c964bf569b2e7341e046bab138bc515af3aa6/_notebooks/2022-05-29-GAON.ipynb . if (!require(pacman)) install.packages(&#39;pacman&#39;) library(pacman) pacman::p_load(&quot;tidyverse&quot;, &quot;stringr&quot;, &quot;readxl&quot;, &quot;xlsx&quot;, &quot;repr&quot;) # 필요한 파일을 불러온다. gaon_month &lt;- read.table(&#39;results/gaon_month.txt&#39;, header = TRUE) gaon_week &lt;- read.table(&#39;results/gaon_week.txt&#39;, header = TRUE) . gaon_month_true &lt;- gaon_month %&gt;% drop_na(gaon_index) gaon_week_true &lt;- gaon_week %&gt;% drop_na(gaon_index) . &#50900;&#48324; &#52264;&#53944; &#48320;&#54868; &#44288;&#52272; . df &lt;- gaon_month_true %&gt;% group_by(year_month) %&gt;% summarise(sum = sum(gaon_index)) . df_date_splited &lt;- df %&gt;% mutate(year = str_sub(year_month, 1, 4), month = str_sub(year_month, 5, 6), .before = sum) %&gt;% select(-year_month) options(repr.plot.width=12, repr.plot.height=7) options(scipen=999) df_date_splited %&gt;% filter(2019 &lt;= year, year &lt;= 2021) %&gt;% ggplot(aes(x = month, y = sum, group = year, color = year)) + geom_point() + geom_freqpoly(stat=&#39;identity&#39;) + ylim(4000000000, 9000000000) . 위 그래프는 월별 가온지수 합계를 나타낸 것이다. (2019년 1월 가온지수 총합, 2019년 2월 가온지수 총합 $ dots$ 2021년 12월 가온지수 총합) . 코로나 확산은 2020년 1월에 처음 발생하였으며 위 그래프에서 이를 기점으로 가온지수 합계가 크게 감소하였음을 볼 수 있다. . 가온지수 합계의 하락 이후 2년간 코로나 이전 수준으로 회복되지 못하고 있다. . &#51452;&#48324; &#52264;&#53944; &#48320;&#54868; &#44288;&#52272; . df &lt;- gaon_week_true %&gt;% group_by(year_week) %&gt;% summarise(sum = sum(gaon_index)) df_date_splited &lt;- df %&gt;% mutate(year = str_sub(year_week, 1, 4), week = str_sub(year_week, 5, 6), .before = sum) %&gt;% select(-year_week) df_date_splited %&gt;% filter(2019 &lt;= year, year &lt;= 2021) %&gt;% ggplot(aes(x = week, y = smooth(sum), group = year, color = year)) + geom_point() + geom_freqpoly(stat=&#39;identity&#39;) + theme(axis.ticks.x = element_blank(), axis.text.x = element_blank()) . Don&#39;t know how to automatically pick scale for object of type tukeysmooth. Defaulting to continuous. . 위 그래프는 주별 가온지수 합계를 나타낸 것이다. (2019년 1주차 가온지수 총합, 2019년 2주차 가온지수 총합 $ dots$ 2021년 52주차 가온지수 총합) . 주별 차트 변화도 월별 차트 변화와 유사한 양상을 보이고 있음을 알 수 있다. . &#53076;&#47196;&#45208; &#51204;&#54980; &#48149;&#49828;&#50724;&#54588;&#49828; &#48320;&#54868; . imports . 분석에 필요한 데이터 및 라이브러리를 가져온다. . 연도별 박스오피스 데이터는 KOBIS(https://www.kobis.or.kr/kobis/business/stat/them/findYearlyTotalList.do)에서 구할 수 있다. . years &lt;- 2004:2021 box_office &lt;- read.xlsx(&quot;results/box_office.xlsx&quot;, sheetIndex = 1, startRow = 5, colIndex = c(1,4,5)) %&gt;% as_tibble() %&gt;% slice(1:length(years)) colnames(box_office) &lt;- c(&#39;연도&#39;, &#39;매출액&#39;, &#39;관객수&#39;) . &#50672;&#46020;&#48324; &#47588;&#52636;&#50529; &#48320;&#54868; &#44288;&#52272; . box_office %&gt;% ggplot(aes(x = 연도, y = 매출액, group = 연도, color = 연도, fill = 연도)) + geom_point() + geom_bar(stat=&#39;identity&#39;, width = .8) + ggtitle(&quot;박스오피스 연도별 매출액&quot;) + theme(axis.text.x = element_text(angle = 90), plot.title=element_text(size = 30)) . 코로나가 확산되기 시작한 2020년부터 연도별 매출액이 매우 크게 감소하였음을 볼 수 있다. . 2년간 회복되지 못하고 있다. . &#50672;&#46020;&#48324; &#44288;&#44061;&#49688; &#48320;&#54868; &#44288;&#52272; . box_office %&gt;% ggplot(aes(x = 연도, y = 관객수, group = 연도, color = 연도, fill = 연도)) + geom_point() + geom_bar(stat=&#39;identity&#39;, width = .8) + ggtitle(&quot;박스오피스 연도별 관객수&quot;) + theme(axis.text.x = element_text(angle = 90), plot.title=element_text(size = 30)) . 연도별 매출액 변화와 유사하게 큰 감소를 보이고 있다. . &#50836;&#50557; . 결론적으로 음악 스트리밍 산업과 영화 산업은 코로나 확산 이후 크게 쇠퇴하였음을 확인하였다. .",
            "url": "https://stahangryum.github.io/Woo/r/2022/06/03/covid_art.html",
            "relUrl": "/r/2022/06/03/covid_art.html",
            "date": " • Jun 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "네이버 영화 랭킹 크롤링",
            "content": "&#45348;&#51060;&#48260; &#50689;&#54868; &#47021;&#53433; &#53356;&#47204;&#47553; . 네이버 영화 순위를 크롤링한다. | . Code . if (!require(rvest)) install.packages(&#39;rvest&#39;) library(rvest) if (!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(tidyverse) # 날짜와 페이지를 입력하면 조건에 따른 영화 코드를 반환하는 함수를 정의함 get_movie_code &lt;- function(date, page){ # ex) date = 20220502, page = 1 base_url &lt;- &#39;https://movie.naver.com/movie/sdb/rank/rmovie.naver?sel=pnt&amp;date=&#39; target_url &lt;- paste0(base_url, date, &#39;&amp;page=&#39;, page) tables &lt;- target_url %&gt;% read_html(encoding = &#39;UTF-8&#39;) %&gt;% html_nodes(&#39;table&#39;) hrefs &lt;- tables[[1]] %&gt;% html_nodes(&#39;a&#39;) %&gt;% html_attr(&#39;href&#39;) hrefs_odd &lt;- hrefs[c(TRUE, FALSE)] # 같은 코드이나 서로 다른링크가 짝으로 존재함을 확인했다. 홀수번째 원소만 인덱싱한다. codes &lt;- substr(hrefs_odd, unlist(gregexpr(&#39;=&#39;, hrefs_odd)) + 1, nchar(hrefs_odd)) # 다섯자리 코드와 여섯자리 코드가 혼재하므로 등호를 기준으로 인덱싱한다. return(codes) } # 영화의 코드를 입력하면 영화의 정보를 반환하는 함수를 정의함 get_movie_info &lt;- function(code){ base_url &lt;- &#39;https://movie.naver.com/movie/bi/mi/point.nhn?code=&#39; target_url &lt;- paste0(base_url, code) html &lt;- read_html(target_url) title_unclean &lt;- html %&gt;% html_nodes(&quot;title&quot;) %&gt;% html_text() title &lt;- substr(title_unclean, 1, unlist(gregexpr(&#39; : 네이버 영화&#39;, title_unclean))-1) exist &lt;- html %&gt;% html_nodes(&#39;dl[class=info_spec]&#39;) %&gt;% html_nodes(&#39;dt&#39;) %&gt;% html_text() steps &lt;- html %&gt;% html_nodes(&#39;dl[class=info_spec]&#39;) %&gt;% html_nodes(&#39;dd&#39;) # 개요, 감독, 출연, 등급 중 결측값이 존재하는 경우를 대비함 step1 = NA step2 = NA step3 = NA step4 = NA for (i in 1:length(exist)){ if (exist[i] == &#39;개요()&#39;){ step1_unclean &lt;- steps[i] %&gt;% html_nodes(&#39;p&#39;) %&gt;% html_nodes(&#39;span&#39;) %&gt;% html_text() step1 &lt;- gsub(&#39; t| n| r&#39;, &#39;&#39;, step1_unclean) }else if (exist[i] == &#39;감독&#39;){ step2 &lt;- steps[i] %&gt;% html_nodes(&#39;p&#39;) %&gt;% html_text() }else if (exist[i] == &#39;출연&#39;){ step3 &lt;- steps[i] %&gt;% html_text() }else if (exist[i] == &#39;등급&#39;){ step4_unclean &lt;- steps[i] %&gt;% html_nodes(&#39;p&#39;) %&gt;% html_text() step4 &lt;- gsub(&#39; t| n| r&#39;, &#39;&#39;, step4_unclean) } } if (length(step1) == 3){ # 개봉일자가 존재하지 않는 경우 결측값으로 처리함 ex)먼 훗날 우리 step1 = c(step1, NA) } tdt &lt;- html %&gt;% html_nodes(&#39;div[class=viewing_graph]&#39;) # 성별, 나이별 관람추이가 존재하지 않는 경우 관람객 통계가 존재하지 않으므로 결측값으로 처리함 if (length(tdt) == 0){ audience_age_10 &lt;- NA audience_age_20 &lt;- NA audience_age_30 &lt;- NA audience_age_40 &lt;- NA audience_age_50 &lt;- NA audience_score &lt;- NA audience_count &lt;- NA audience_male &lt;- NA audience_female &lt;- NA audience_10 &lt;- NA audience_20 &lt;- NA audience_30 &lt;- NA audience_40 &lt;- NA audience_50 &lt;- NA } else { audi_age &lt;- html %&gt;% html_nodes(&#39;strong[class=graph_percent]&#39;) %&gt;% html_text() audience_age_10 &lt;- audi_age[1] audience_age_20 &lt;- audi_age[2] audience_age_30 &lt;- audi_age[3] audience_age_40 &lt;- audi_age[4] audience_age_50 &lt;- audi_age[5] audience_score &lt;- html %&gt;% html_nodes(&#39;div[class=grade_audience]&#39;) %&gt;% html_nodes(&#39;div[class=star_score]&#39;) %&gt;% html_nodes(&#39;em&#39;) %&gt;% html_text() %&gt;% paste(collapse=&#39;&#39;) audience_count &lt;- html %&gt;% html_nodes(&#39;div[class=grade_audience]&#39;) %&gt;% html_nodes(&#39;span[class=user_count]&#39;) %&gt;% html_nodes(&#39;em&#39;) %&gt;% html_text() %&gt;% paste(collapse=&#39;&#39;) audience_male &lt;- (html %&gt;% html_nodes(&#39;div[class=graph_area]&#39;) %&gt;% html_nodes(&#39;div[class=grp_male]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text())[2] audience_female &lt;- (html %&gt;% html_nodes(&#39;div[class=graph_area]&#39;) %&gt;% html_nodes(&#39;div[class=grp_female]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text())[2] audience_age &lt;- html %&gt;% html_nodes(&#39;div[class=grp_age]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text() audience_10 &lt;- audience_age[6] audience_20 &lt;- audience_age[7] audience_30 &lt;- audience_age[8] audience_40 &lt;- audience_age[9] audience_50 &lt;- audience_age[10] } netizen_score &lt;- html %&gt;% html_nodes(&#39;div[class=grade_netizen]&#39;) %&gt;% html_nodes(&#39;div[class=star_score]&#39;) %&gt;% html_nodes(&#39;em&#39;) %&gt;% html_text() %&gt;% paste(collapse=&#39;&#39;) netizen_count &lt;- html %&gt;% html_nodes(&#39;div[class=grade_netizen]&#39;) %&gt;% html_nodes(&#39;span[class=user_count]&#39;) %&gt;% html_nodes(&#39;em&#39;) %&gt;% html_text() %&gt;% paste(collapse=&#39;&#39;) ntz_male &lt;- (html %&gt;% html_nodes(&#39;div[class=graph_area]&#39;) %&gt;% html_nodes(&#39;div[class=grp_male]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text())[1] ntz_female &lt;- (html %&gt;% html_nodes(&#39;div[class=graph_area]&#39;) %&gt;% html_nodes(&#39;div[class=grp_female]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text())[1] ntz_age &lt;- html %&gt;% html_nodes(&#39;div[class=grp_age]&#39;) %&gt;% html_nodes(&#39;strong[class=graph_point]&#39;) %&gt;% html_text() ntz_10 &lt;- ntz_age[1] ntz_20 &lt;- ntz_age[2] ntz_30 &lt;- ntz_age[3] ntz_40 &lt;- ntz_age[4] ntz_50 &lt;- ntz_age[5] return(c(title, code, step1, step2, step3, step4, audience_age_10, audience_age_20, audience_age_30, audience_age_40, audience_age_50, netizen_score, netizen_count, ntz_male, ntz_female, ntz_10, ntz_20, ntz_30, ntz_40, ntz_50, audience_score, audience_count, audience_male, audience_female, audience_10, audience_20, audience_30, audience_40, audience_50)) } info &lt;- vector(&#39;list&#39;, 100) top_100_codes &lt;- c(get_movie_code(20220502, 1), get_movie_code(20220502, 2)) for (i in 1:length(top_100_codes)){ # 한 줄씩 차곡차곡 쌓는다. info[[i]] &lt;- get_movie_info(top_100_codes[i]) } final_info &lt;- do.call(&#39;rbind&#39;, info) # 컬럼명을 지정함 colnames(final_info) &lt;- c(&quot;title&quot;,&quot;code&quot;,&quot;genre&quot;,&quot;country&quot;,&quot;runtime&quot;,&quot;release&quot;, &quot;director&quot;,&quot;actor&quot;,&quot;view_class&quot;,&quot;audience_age_10&quot;,&quot;audience_age_20&quot;, &quot;audience_age_30&quot;,&quot;audience_age_40&quot;,&quot;audience_age_50&quot;, &quot;netizen_score&quot;,&quot;netizen_count&quot;,&quot;ntz_male&quot;,&quot;ntz_female&quot;,&quot;ntz_10&quot;, &quot;ntz_20&quot;,&quot;ntz_30&quot;,&quot;ntz_40&quot;,&quot;ntz_50&quot;,&quot;audience_score&quot;, &quot;audience_count&quot;,&quot;audience_male&quot;,&quot;audience_female&quot;,&quot;audience_10&quot;, &quot;audience_20&quot;,&quot;audience_30&quot;,&quot;audience_40&quot;,&quot;audience_50&quot;) write.csv(final_info, &#39;movie.csv&#39;, row.names=T) # 최종 csv파일 생성함 . 필요한 패키지를 로딩중입니다: rvest Warning message: &#34;패키지 &#39;rvest&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; 필요한 패키지를 로딩중입니다: tidyverse Warning message: &#34;패키지 &#39;tidyverse&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 Warning message: &#34;패키지 &#39;ggplot2&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tibble&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tidyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;readr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;purrr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;dplyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;stringr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;forcats&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x readr::guess_encoding() masks rvest::guess_encoding() x dplyr::lag() masks stats::lag() . final_info . A matrix: 100 × 32 of type chr titlecodegenrecountryruntimereleasedirectoractorview_classaudience_age_10⋯ntz_50audience_scoreaudience_countaudience_maleaudience_femaleaudience_10audience_20audience_30audience_40audience_50 . 클라우스 | 191613 | 애니메이션, 코미디, 가족 | 스페인, 영국 | 96분 | 2019.11.15 개봉 | 서지오 파블로스, 카를로스 마르티네즈 로페즈 | 제이슨 슈왈츠먼, J.K. 시몬스, 라시다 존스더보기 | [국내] 전체 관람가 | NA | ⋯ | 9.17 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 그린 북 | 171539 | 드라마 | 미국 | 130분 | 2019.01.09 개봉 | 피터 패럴리 | 비고 모텐슨(토니 발레롱가), 마허샬라 알리(돈 셜리 박사)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | 0% | ⋯ | 9.46 | 9.55 | 2,070 | 9.52 | 9.57 | 9.60 | 9.50 | 9.58 | 9.53 | 9.58 | . 가버나움 | 174830 | 드라마 | 레바논, 프랑스 | 126분 | 2019.01.24 개봉 | 나딘 라바키 | 자인 알 라피아(자인), 요르다노스 시프로우(라힐)더보기 | [국내] 15세 관람가 [해외] R도움말 | 2% | ⋯ | 9.55 | 9.54 | 1,393 | 9.39 | 9.61 | 9.64 | 9.56 | 9.51 | 9.58 | 9.49 | . 밥정 | 186114 | 다큐멘터리, 드라마 | 한국 | 82분 | 2020.10.07 개봉 | 박혜령 | 임지호(본인)더보기 | [국내] 전체 관람가 | 5% | ⋯ | 9.53 | 9.70 | 20 | 9.75 | 9.69 | 10.0 | 9.43 | 9.80 | 10.0 | 9.75 | . 장민호 드라마 최종회 | 213746 | 공연실황 | 한국 | 106분 | 2022.01.24 개봉 | NA | 장민호더보기 | [국내] 전체 관람가 | 0% | ⋯ | 9.89 | 9.89 | 9 | 9.00 | 10.0 | 0.00 | 0.00 | 10.0 | 9.75 | 10.0 | . 디지몬 어드벤처 라스트 에볼루션 : 인연 | 192613 | 애니메이션, 모험 | 일본 | 114분 | 2021.02.17 개봉 | 타구치 토모히사 | 하나에 나츠키(야가미 타이치), 호소야 요시마사(이시다 야마토), 사카모토 치카(아구몬)더보기 | [국내] 12세 관람가 | NA | ⋯ | 8.80 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 베일리 어게인 | 144906 | 모험, 코미디, 드라마 | 미국 | 100분 | 2018.11.22 개봉 | 라세 할스트롬 | 조시 게드(베일리/ 엘리/ 티노/ 버디 목소리), 데니스 퀘이드(이든), K.J. 아파(십대 이든)더보기 | [국내] 전체 관람가 [해외] PG도움말 | 3% | ⋯ | 9.35 | 9.42 | 463 | 9.36 | 9.44 | 9.71 | 9.52 | 9.36 | 9.14 | 9.58 | . 원더 | 151196 | 드라마 | 미국 | 113분 | 2021.02.11 재개봉, 2017.12.27 개봉 | 스티븐 크보스키 | 제이콥 트렘블레이(어기 풀먼), 줄리아 로버츠(이자벨 풀먼), 오웬 윌슨(네이트 풀먼)더보기 | [국내] 전체 관람가 [해외] PG도움말 | 2% | ⋯ | 9.33 | 9.43 | 319 | 9.37 | 9.46 | 10.0 | 9.45 | 9.47 | 9.32 | 9.56 | . 아일라 | 169240 | 드라마, 전쟁 | 한국, 터키 | 123분 | 2018.06.21 개봉 | 잔 울카이 | 김설(아일라), 이스마일 하지오글루(슐레이만)더보기 | [국내] 15세 관람가 | 0% | ⋯ | 9.44 | 9.13 | 70 | 8.81 | 9.32 | 0.00 | 9.45 | 8.74 | 9.33 | 8.17 | . 극장판 바이올렛 에버가든 | 196843 | 애니메이션, 드라마, 판타지 | 일본 | 140분 | 2020.11.12 개봉 | 이시다테 타이치 | 이시카와 유이(바이올렛 에버가든 목소리), 나미카와 다이스케(길베르트 부겐빌리아 목소리)더보기 | [국내] 전체 관람가 | 17% | ⋯ | 9.46 | 9.68 | 436 | 9.69 | 9.67 | 9.95 | 9.66 | 9.62 | 9.42 | 9.86 | . 먼 훗날 우리 | 175092 | 드라마, 멜로/로맨스 | 중국 | 120분 | NA | 유약영 | 정백연, 주동우, 톈좡좡더보기 | NA | NA | ⋯ | 9.20 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 당갈 | 157243 | 드라마, 액션 | 인도 | 161분 | 2018.04.25 개봉 | 니테쉬 티와리 | 아미르 칸(마하비르 싱 포갓), 파티마 사나 셰이크(기타), 산야 말호트라(바비타)더보기 | [국내] 12세 관람가 | 6% | ⋯ | 9.36 | 9.62 | 306 | 9.26 | 9.73 | 9.65 | 9.79 | 9.50 | 8.85 | 9.38 | . 포드 V 페라리 | 181710 | 액션, 드라마 | 미국 | 152분 | 2019.12.04 개봉 | 제임스 맨골드 | 맷 데이먼(캐롤 셸비), 크리스찬 베일(켄 마일스)더보기 | [국내] 12세 관람가 | 2% | ⋯ | 9.42 | 9.31 | 1,112 | 9.34 | 9.25 | 9.57 | 9.40 | 9.24 | 9.41 | 8.90 | . 주전장 | 179518 | 다큐멘터리 | 미국 | 121분 | 2019.07.25 개봉 | 미키 데자키 | NA | [국내] 전체 관람가 | 3% | ⋯ | 8.85 | 9.67 | 317 | 9.64 | 9.68 | 9.90 | 9.64 | 9.65 | 9.82 | 9.47 | . 그대, 고맙소 : 김호중 생애 첫 팬미팅 무비 | 196828 | 공연실황 | 한국 | 80분 | 2020.09.29 개봉 | 오윤동 | 김호중더보기 | [국내] 전체 관람가 | NA | ⋯ | 9.91 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 쇼생크 탈출 | 17421 | 드라마 | 미국 | 142분 | 2016.02.24 재개봉, 1995.01.28 개봉 | 프랭크 다라본트 | 팀 로빈스(앤디 듀프레인), 모건 프리먼(엘리스 보이드 레드 레딩)더보기 | [국내] 15세 관람가 [해외] R도움말 | 0% | ⋯ | 8.68 | 9.88 | 17 | 10.0 | 9.75 | 0.00 | 9.86 | 10.0 | 10.0 | 0.00 | . 터미네이터 2:오리지널 | 10200 | SF, 액션, 스릴러 | 미국, 프랑스 | 137분 | 2019.10.24 재개봉, 2013.11.14 재개봉, 1991.07.06 개봉 | 제임스 카메론 | 아놀드 슈왈제네거(터미네이터)더보기 | [국내] 15세 관람가 [해외] R도움말 | 0% | ⋯ | 9.00 | 9.51 | 39 | 9.42 | 9.88 | 0.00 | 9.92 | 9.92 | 8.75 | 9.33 | . 덕구 | 154667 | 드라마 | 한국 | 91분 | 2018.04.05 개봉 | 방수인 | 이순재(덕구할배), 정지훈(덕구)더보기 | [국내] 전체 관람가 | 3% | ⋯ | 9.18 | 9.30 | 265 | 9.12 | 9.40 | 10.0 | 9.23 | 9.46 | 9.14 | 9.08 | . 클래식 | 35939 | 멜로/로맨스, 드라마 | 한국 | 132분 | 2003.01.30 개봉 | 곽재용 | 손예진(지혜/주희), 조승우(준하), 조인성(상민)더보기 | [국내] 12세 관람가 | 7% | ⋯ | 9.40 | 9.79 | 14 | 9.86 | 9.71 | 10.0 | 9.62 | 10.0 | 10.0 | 0.00 | . 나 홀로 집에 | 10016 | 모험, 범죄, 가족, 코미디 | 미국 | 105분 | 1991.07.06 개봉 | 크리스 콜럼버스 | 맥컬리 컬킨(케빈 맥콜리스터), 조 페시(좀도둑 해리 림), 다니엘 스턴(좀도둑 마브 머챈츠)더보기 | [국내] 전체 관람가 [해외] PG도움말 | NA | ⋯ | 8.47 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 라이언 일병 구하기 | 18988 | 전쟁, 액션, 드라마 | 미국 | 170분 | 1998.09.12 개봉 | 스티븐 스필버그 | 톰 행크스(존 밀러 대위), 에드워드 번즈(Pvt. 리처드 레이번), 톰 시즈모어(Sgt. 마이클 호바스)더보기 | [국내] 15세 관람가 [해외] R도움말 | NA | ⋯ | 9.15 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 월-E | 69105 | 애니메이션, SF, 가족, 코미디, 멜로/로맨스, 모험 | 미국 | 104분 | 2008.08.06 개봉 | 앤드류 스탠튼 | 벤 버트(월-E / M-O 목소리), 엘리사 나이트(이브 목소리), 제프 갈린(선장 목소리)더보기 | [국내] 전체 관람가 [해외] G도움말 | NA | ⋯ | 9.25 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 빽 투 더 퓨쳐 | 10002 | SF, 코미디 | 미국 | 120분 | 2015.10.21 재개봉, 1987.07.17 개봉 | 로버트 저메키스 | 마이클 J. 폭스(마티 맥플라이), 크리스토퍼 로이드(에메트 브라운 박사), 리 톰슨(로레인 베인스 맥플라이)더보기 | [국내] 12세 관람가 [해외] PG도움말 | 2% | ⋯ | 8.82 | 9.39 | 49 | 9.64 | 9.19 | 10.0 | 9.23 | 9.78 | 9.62 | 8.00 | . 보헤미안 랩소디 | 156464 | 드라마 | 미국, 영국 | 134분 | 2018.10.31 개봉 | 브라이언 싱어 | 라미 말렉(프레디 머큐리), 루시 보인턴(메리 오스틴), 귈림 리(브라이언 메이)더보기 | [국내] 12세 관람가 | 3% | ⋯ | 9.36 | 9.45 | 13,104 | 9.39 | 9.50 | 9.45 | 9.47 | 9.47 | 9.46 | 9.35 | . 사운드 오브 뮤직 | 10102 | 멜로/로맨스, 뮤지컬, 드라마 | 미국 | 172분 | 2017.02.02 재개봉, 2012.01.23 재개봉, 1995.09.30 재개봉, 1978.02.04 재개봉, 1969.10.29 개봉 | 로버트 와이즈 | 줄리 앤드류스(마리아), 크리스토퍼 플러머(캡틴 조지 본 트랩), 엘레노 파커(남작 부인)더보기 | [국내] 전체 관람가 [해외] NR도움말 | 10% | ⋯ | 8.99 | 9.60 | 10 | 10.0 | 9.50 | 10.0 | 9.50 | 0.00 | 10.0 | 0.00 | . 포레스트 검프 | 17159 | 드라마, 코미디 | 미국 | 142분 | 2016.09.07 재개봉, 1994.10.15 개봉 | 로버트 저메키스 | 톰 행크스(포레스트 검프)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | 5% | ⋯ | 8.80 | 9.52 | 120 | 9.56 | 9.48 | 9.67 | 9.42 | 9.66 | 9.57 | 10.0 | . 글래디에이터 | 29217 | 액션, 드라마 | 미국, 영국 | 154분 | 2000.06.03 개봉 | 리들리 스콧 | 러셀 크로우(막시무스), 호아킨 피닉스(코모두스), 코니 닐슨(루실라)더보기 | [국내] 15세 관람가 [해외] R도움말 | NA | ⋯ | 9.12 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 타이타닉 | 18847 | 멜로/로맨스, 드라마 | 미국 | 194분 | 2018.02.01 재개봉, 2012.04.05 재개봉, 1998.02.20 개봉 | 제임스 카메론 | 레오나르도 디카프리오(잭 도슨), 케이트 윈슬렛(로즈 드윗 부카더)더보기 | [국내] 15세 관람가 [해외] PG-13도움말 | 14% | ⋯ | 9.08 | 9.88 | 269 | 9.92 | 9.86 | 9.82 | 9.92 | 9.91 | 9.65 | 9.92 | . 위대한 쇼맨 | 106360 | 드라마, 뮤지컬 | 미국 | 104분 | 2020.05.21 재개봉, 2017.12.20 개봉 | 마이클 그레이시 | 휴 잭맨(P.T. 바넘), 잭 에프론(필립 칼라일), 미셸 윌리엄스(채러티 바넘)더보기 | [국내] 12세 관람가 [해외] PG도움말 | 6% | ⋯ | 9.46 | 9.31 | 3,877 | 9.42 | 9.24 | 9.34 | 9.37 | 9.29 | 9.11 | 9.18 | . 인생은 아름다워 | 22126 | 드라마, 코미디 | 이탈리아 | 116분 | 2016.04.13 재개봉, 1999.03.06 개봉 | 로베르토 베니니 | 로베르토 베니니(귀도), 니콜레타 브라스키(도라)더보기 | [국내] 전체 관람가 [해외] PG-13도움말 | 0% | ⋯ | 8.35 | 9.54 | 28 | 9.38 | 9.67 | 0.00 | 9.73 | 9.67 | 9.00 | 9.00 | . ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | . 에이리언 2 | 10038 | SF, 공포, 스릴러, 액션 | 미국 | 137분 | 1986.12.24 개봉 | 제임스 카메론 | 시고니 위버(엘렌 리플리), 마이클 빈(Cpl. 드웨인 힉스), 폴 레이저(카터 버크)더보기 | [국내] 15세 관람가 [해외] R도움말 | NA | ⋯ | 9.33 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 로망 | 182348 | 멜로/로맨스 | 한국 | 112분 | 2019.04.03 개봉 | 이창근 | 이순재(조남봉), 정영숙(이매자), 조한철(조진수)더보기 | [국내] 전체 관람가 | 4% | ⋯ | 8.77 | 8.98 | 83 | 8.83 | 9.08 | 9.33 | 9.31 | 8.89 | 8.80 | 8.20 | . 미세스 다웃파이어 | 16210 | 코미디, 가족, 드라마 | 미국 | 125분 | 1994.01.22 개봉 | 크리스 콜럼버스 | 로빈 윌리엄스(다니엘/미세스 다웃파이어), 샐리 필드(미란다)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | NA | ⋯ | 9.19 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 더 록 | 17135 | 액션 | 미국 | 131분 | 1996.07.13 개봉 | 마이클 베이 | 숀 코네리(존 패트릭 메이슨), 니콜라스 케이지(닥터 스탠리 굿스피드), 에드 해리스(프란시스 X. 험멜 장군)더보기 | [국내] 15세 관람가 [해외] R도움말 | NA | ⋯ | 9.22 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 울지마 톤즈 | 76667 | 다큐멘터리 | 한국 | 91분 | 2010.09.09 개봉 | 구수환 | 이금희(나레이션), 이태석(본인)더보기 | [국내] 전체 관람가 | 0% | ⋯ | 9.30 | 10.00 | 1 | 0.00 | 10.0 | 0.00 | 0.00 | 0.00 | 10.0 | 0.00 | . 다크 나이트 | 62586 | 액션, 범죄, 드라마, 미스터리 | 미국 | 152분 | 2020.07.01 재개봉, 2017.07.12 재개봉, 2009.02.19 재개봉, 2008.08.06 개봉 | 크리스토퍼 놀란 | 크리스찬 베일(브루스 웨인/배트맨), 히스 레저(조커), 아론 에크하트(하비 덴트/투 페이스)더보기 | [국내] 15세 관람가 [해외] PG-13도움말 | 17% | ⋯ | 9.12 | 9.65 | 46 | 9.78 | 9.36 | 9.88 | 9.68 | 9.62 | 9.00 | 9.00 | . 아마데우스 | 10114 | 드라마 | 미국 | 180분 | 2015.10.29 재개봉, 1985.11.23 개봉 | 밀로스 포만 | 톰 헐스(볼프강 아마데우스 모짜르트), F. 머레이 아브라함(안토니오 살리에리), 엘리자베스 베리지(콘스탄츠 모짜르트)더보기 | [국내] 12세 관람가 [해외] PG도움말 | 3% | ⋯ | 8.95 | 9.70 | 151 | 9.75 | 9.68 | 10.0 | 9.64 | 9.75 | 9.77 | 9.67 | . 알라딘 | 13008 | 애니메이션, 뮤지컬, 코미디, 가족, 모험, 드라마, 멜로/로맨스 | 미국 | 90분 | 1993.07.03 개봉 | 존 머스커, 론 클레멘츠 | 스콧 와인거(알라딘/알리바브와 왕자 목소리), 로빈 윌리엄스(램프의 요정 지니 목소리), 린다 라킨(쟈스민 공주 목소리)더보기 | [국내] 전체 관람가 [해외] G도움말 | NA | ⋯ | 8.97 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 자산어보 | 189075 | 드라마 | 한국 | 126분 | 2021.03.31 개봉 | 이준익 | 설경구(정약전), 변요한(창대)더보기 | [국내] 12세 관람가 | 1% | ⋯ | 9.55 | 9.03 | 323 | 8.83 | 9.24 | 10.0 | 9.12 | 8.80 | 8.99 | 9.27 | . 빌리 엘리어트 | 31013 | 드라마, 가족, 코미디 | 영국, 프랑스 | 110분 | 2017.01.18 재개봉, 2001.02.17 개봉 | 스티븐 달드리 | 제이미 벨(빌리 엘리어트), 줄리 월터스(윌킨슨 부인), 게리 루이스(아버지 재키 엘리어트)더보기 | [국내] 12세 관람가 [해외] R도움말 | 4% | ⋯ | 8.66 | 9.75 | 79 | 9.76 | 9.74 | 10.0 | 9.69 | 9.85 | 9.70 | 9.33 | . 두 교황 | 189111 | 드라마 | 미국, 영국, 이탈리아, 아르헨티나 | 126분 | 2019.12.11 개봉 | 페르난도 메이렐레스 | 안소니 홉킨스(교황 베네딕토 16세), 조나단 프라이스(교황 프란치스코)더보기 | [국내] 12세 관람가 | 0% | ⋯ | 9.58 | 9.32 | 131 | 9.26 | 9.36 | 0.00 | 8.95 | 9.31 | 9.46 | 9.47 | . 그대를 사랑합니다 | 73476 | 드라마 | 한국 | 118분 | 2011.02.17 개봉 | 추창민 | 이순재(김만석), 윤소정(송이뿐), 송재호(장군봉)더보기 | [국내] 15세 관람가 | NA | ⋯ | 8.79 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 아이 엠 샘 | 34227 | 드라마 | 미국 | 132분 | 2002.10.18 개봉 | 제시 넬슨 | 숀 펜(샘 도슨), 미셸 파이퍼(리타 해리슨)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | NA | ⋯ | 9.24 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 언터처블: 1%의 우정 | 87566 | 코미디, 드라마 | 프랑스 | 112분 | 2012.03.22 개봉 | 올리비에르 나카체, 에릭 토레다노 | 프랑수아 클루제(필립), 오마 사이(드리스)더보기 | [국내] 12세 관람가 | NA | ⋯ | 9.46 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 피아니스트 | 35187 | 드라마, 전쟁 | 프랑스, 독일, 폴란드, 영국, 네덜란드 | 148분 | 2015.06.18 재개봉, 2003.01.03 개봉 | 로만 폴란스키 | 애드리언 브로디(블라디슬로프 스필만), 토마스 크레취만(빌름 호젠펠트)더보기 | [국내] 12세 관람가 [해외] R도움말 | 8% | ⋯ | 9.23 | 9.51 | 53 | 9.61 | 9.43 | 9.25 | 9.58 | 9.33 | 9.67 | 10.0 | . 오세암 | 34181 | 애니메이션, 드라마 | 한국 | 75분 | 2003.05.01 개봉 | 성백엽 | 김서영(길손이 목소리), 박선영(감이 목소리)더보기 | [국내] 전체 관람가 | NA | ⋯ | 8.51 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 언더독 | 144318 | 애니메이션 | 한국 | 102분 | 2019.01.16 개봉 | 오성윤, 이춘백 | 디오(뭉치 목소리), 박소담(밤이 목소리), 박철민(짱아 목소리)더보기 | [국내] 전체 관람가 | 3% | ⋯ | 9.18 | 9.47 | 362 | 9.53 | 9.44 | 9.60 | 9.65 | 9.32 | 9.46 | 9.74 | . 천공의 성 라퓨타 | 18782 | 애니메이션, 판타지, 모험 | 일본 | 124분 | 2004.04.30 개봉 | 미야자키 하야오 | 타나카 마유미(파즈 목소리), 요코자와 케이코(쉬타 목소리), 하츠이 코토에(돌라 목소리)더보기 | [국내] 전체 관람가 | NA | ⋯ | 8.43 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 프리 윌리 | 16466 | 가족, 모험, 드라마 | 미국 | 112분 | 1994.08.06 개봉 | 사이먼 윈서 | 케이코(윌리), 제이슨 제임스 리처(제시), 로리 페티(래 린들리)더보기 | [국내] 전체 관람가 [해외] PG도움말 | NA | ⋯ | 8.95 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 아이언 자이언트 | 25915 | 애니메이션, SF, 액션, 가족 | 미국 | 90분 | 2019.10.09 재개봉, 2000.05 개봉 | 브래드 버드 | 제니퍼 애니스톤(애니 휴즈 목소리), 빈 디젤(아이언 자이언트 목소리), 엘리 마리엔탈(호가드 휴즈 목소리)더보기 | [국내] 전체 관람가 [해외] PG도움말 | 0% | ⋯ | 9.06 | 9.47 | 15 | 9.33 | 9.67 | 0.00 | 9.50 | 9.00 | 9.75 | 0.00 | . 업 | 52120 | 애니메이션, 가족, 모험, 코미디, 액션 | 미국 | 101분 | 2009.07.29 개봉 | 피트 닥터, 밥 피터슨 | 에드워드 애스너(칼 프레드릭슨), 조던 나가이(러셀 목소리), 크리스토퍼 플러머(찰스 먼츠 목소리)더보기 | [국내] 전체 관람가 [해외] PG도움말 | NA | ⋯ | 9.05 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 라푼젤 | 75470 | 애니메이션, 코미디, 가족, 판타지, 뮤지컬, 멜로/로맨스 | 미국 | 100분 | 2011.02.10 개봉 | 네이슨 그레노, 바이론 하워드 | 맨디 무어(라푼젤 목소리), 제커리 레비(플린 라이더 목소리)더보기 | [국내] 전체 관람가 [해외] PG도움말 | NA | ⋯ | 9.10 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 시네마 천국 | 10001 | 드라마, 멜로/로맨스 | 프랑스, 이탈리아 | 124분 | 2020.04.22 재개봉, 2013.09.26 재개봉, 1993.11.13 재개봉, 1990.07.07 개봉 | 쥬세페 토르나토레 | 마르코 레오나르디(청년 살바토레), 필립 느와레(알프레도), 자끄 페렝(중년 살바토레)더보기 | [국내] 전체 관람가 [해외] PG도움말 | 9% | ⋯ | 9.17 | 9.63 | 32 | 9.73 | 9.57 | 9.67 | 9.40 | 9.88 | 10.0 | 9.75 | . 프리퀀시 | 29657 | 범죄, 드라마, SF, 스릴러 | 미국 | 117분 | 2000.11.25 개봉 | 그레고리 호블릿 | 데니스 퀘이드(프랭크 설리반), 제임스 카비젤(존 설리반)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | NA | ⋯ | 8.97 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 해리 포터와 죽음의 성물 - 2부 | 47528 | 모험, 판타지, 미스터리 | 영국, 미국 | 131분 | 2011.07.13 개봉 | 데이빗 예이츠 | 다니엘 래드클리프(해리 포터), 엠마 왓슨(헤르미온느), 루퍼트 그린트(론 위즐리)더보기 | [국내] 전체 관람가 [해외] PG-13도움말 | NA | ⋯ | 9.10 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 달링 | 152655 | 드라마, 멜로/로맨스 | 영국 | 118분 | 2018.04.12 개봉 | 앤디 서키스 | 앤드류 가필드(로빈), 클레어 포이(다이애나)더보기 | [국내] 12세 관람가 [해외] PG-13도움말 | 0% | ⋯ | 9.47 | 9.08 | 59 | 9.46 | 8.83 | 0.00 | 9.36 | 9.00 | 8.33 | 9.00 | . 드래곤 길들이기 | 70457 | 애니메이션, 모험, 코미디, 가족, 판타지 | 미국 | 98분 | 2019.01.17 재개봉, 2010.05.20 개봉 | 딘 데블로이스, 크리스 샌더스 | 제이 바루첼(히컵 목소리), 제라드 버틀러(스토이크 목소리), 아메리카 페레라(아스트리드 목소리)더보기 | [국내] 전체 관람가 [해외] PG도움말 | 0% | ⋯ | 8.98 | 8.00 | 1 | 0.00 | 8.00 | 0.00 | 0.00 | 8.00 | 0.00 | 0.00 | . 러브 유어셀프 인 서울 | 180220 | 공연실황 | 한국 | 113분 | 2019.01.26 개봉 | NA | RM(본인), 진(본인), 슈가(본인)더보기 | [국내] 전체 관람가 | NA | ⋯ | 8.85 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 소원 | 103535 | 드라마 | 한국 | 122분 | 2013.10.02 개봉 | 이준익 | 설경구(동훈), 엄지원(미희), 이레(소원)더보기 | [국내] 12세 관람가 | NA | ⋯ | 9.13 | NA | NA | NA | NA | NA | NA | NA | NA | NA | . 브레이크 더 사일런스: 더 무비 | 195975 | 다큐멘터리 | 한국 | 89분 | 2020.09.24 개봉 | 박준수 | RM(본인), 진(본인), 슈가(본인)더보기 | [국내] 전체 관람가 | NA | ⋯ | 8.78 | NA | NA | NA | NA | NA | NA | NA | NA | NA | .",
            "url": "https://stahangryum.github.io/Woo/crawling/r/2022/05/30/naver_movie.html",
            "relUrl": "/crawling/r/2022/05/30/naver_movie.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "명나라, 청나라 황제 수명 데이터 분석",
            "content": "Reference . - 명나라 황제 데이터 . https://en.wikipedia.org/wiki/List_of_emperors_of_the_Ming_dynasty . - 청나라 황제 데이터 . https://en.wikipedia.org/wiki/List_of_emperors_of_the_Qing_dynasty . Crawling . if (!require(rvest)) install.packages(&#39;rvest&#39;) library(rvest) if (!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(tidyverse) get_kings_info = function(order, nation){ # ex) order = 1, nation = &#39;Ming&#39; or &#39;King&#39; ming_url = &#39;https://en.wikipedia.org/wiki/List_of_emperors_of_the_Ming_dynasty&#39; qing_url = &#39;https://en.wikipedia.org/wiki/List_of_emperors_of_the_Qing_dynasty&#39; target_url = ifelse(nation == &#39;Ming&#39;, ming_url, qing_url) dynasty = ifelse(nation == &#39;Ming&#39;, &#39;Ming&#39;, &#39;Qing&#39;) unclean_table = (target_url %&gt;% read_html %&gt;% html_nodes(&#39;table[class=wikitable]&#39;))[1] %&gt;% html_nodes(&#39;tbody&#39;) %&gt;% html_nodes(&#39;tr&#39;) %&gt;% html_text() # 정규표현식을 이용해 양식을 맞춘다 requiredRows_index = str_detect(unclean_table, &#39;[A-Za-z]{5,} ([0-9]{1,2} s[A-Z]{1}&#39;) requiredRows = unclean_table[requiredRows_index] clean_table = gsub(&#39; n&#39;, &#39;&#39;, requiredRows[order]) %&gt;% strsplit(&#39;&#39;) %&gt;% unlist() name_start_index = 1 name_end_index = grep(&#39; (&#39;, clean_table)[1]-1 # 괄호보다 한 칸 이전에 있으므로 -1 name = clean_table[name_start_index:name_end_index] %&gt;% paste(collapse = &#39;&#39;) only_numbers = requiredRows[order] %&gt;% # 숫자 존재하는 벡터 strsplit(split = &#39;[^0-9]&#39;) %&gt;% unlist() year_index = grep(&#39;.{4}&#39;, only_numbers)[1:2] # 월, 일은 2글자를 초과하지 못하므로 자연스럽게 네글자만 연도이다. year = only_numbers[year_index] return(c(dynasty, name, year)) } ming_last_order = 16 qing_last_order = 12 ming_kings_info &lt;- vector(&#39;list&#39;, ming_last_order) qing_kings_info &lt;- vector(&#39;list&#39;, qing_last_order) for (i in 1:ming_last_order) ming_kings_info[[i]] = get_kings_info(i, &#39;Ming&#39;) for (i in 1:qing_last_order) qing_kings_info[[i]] = get_kings_info(i, &#39;Qing&#39;) kings_info = do.call(&#39;rbind&#39;, c(ming_kings_info, qing_kings_info)) colnames(kings_info) &lt;- c(&#39;dynasty&#39;, &#39;name&#39;, &#39;birth&#39;, &#39;death&#39;) write.table(kings_info, &#39;China_king.txt&#39;, row.names = FALSE) . 필요한 패키지를 로딩중입니다: rvest Warning message: &#34;패키지 &#39;rvest&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; 필요한 패키지를 로딩중입니다: tidyverse Warning message: &#34;패키지 &#39;tidyverse&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 Warning message: &#34;패키지 &#39;ggplot2&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tibble&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tidyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;readr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;purrr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;dplyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;stringr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;forcats&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x readr::guess_encoding() masks rvest::guess_encoding() x dplyr::lag() masks stats::lag() . kings_info . A data.frame: 28 × 5 dynastynamebirthdeathage . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . Ming | Hongwu Emperor | 1328 | 1398 | 70 | . Ming | Jianwen Emperor | 1377 | 1402 | 25 | . Ming | Yongle Emperor | 1360 | 1424 | 64 | . Ming | Hongxi Emperor | 1378 | 1425 | 47 | . Ming | Xuande Emperor | 1399 | 1435 | 36 | . Ming | Emperor Yingzong | 1427 | 1464 | 37 | . Ming | Jingtai Emperor | 1428 | 1457 | 29 | . Ming | Chenghua Emperor | 1447 | 1487 | 40 | . Ming | Hongzhi Emperor | 1470 | 1505 | 35 | . Ming | Zhengde Emperor | 1491 | 1521 | 30 | . Ming | Jiajing Emperor | 1507 | 1567 | 60 | . Ming | Longqing Emperor | 1537 | 1572 | 35 | . Ming | Wanli Emperor | 1563 | 1620 | 57 | . Ming | Taichang Emperor | 1582 | 1620 | 38 | . Ming | Tianqi Emperor | 1605 | 1627 | 22 | . Ming | Chongzhen Emperor | 1611 | 1644 | 33 | . Qing | Nurhaci | 1559 | 1626 | 67 | . Qing | Hong Taiji | 1592 | 1643 | 51 | . Qing | Shunzhi Emperor | 1638 | 1661 | 23 | . Qing | Kangxi Emperor | 1654 | 1722 | 68 | . Qing | Yongzheng Emperor | 1678 | 1735 | 57 | . Qing | Qianlong Emperor | 1711 | 1799 | 88 | . Qing | Jiaqing Emperor | 1760 | 1820 | 60 | . Qing | Daoguang Emperor | 1782 | 1850 | 68 | . Qing | Xianfeng Emperor | 1831 | 1861 | 30 | . Qing | Tongzhi Emperor | 1856 | 1875 | 19 | . Qing | Guangxu Emperor | 1871 | 1908 | 37 | . Qing | Xuantong Emperor | 1906 | 1967 | 61 | . Analysis . kings_info &lt;- data.frame(kings_info) kings_info[, c(&#39;birth&#39;, &#39;death&#39;)] = kings_info[, c(&#39;birth&#39;, &#39;death&#39;)] %&gt;% unlist() %&gt;% as.numeric() kings_info = kings_info %&gt;% mutate(age = death - birth) plot(kings_info$birth, kings_info$age, xlab = &#39;birth&#39;, ylab = &#39;age&#39;) abline(lm(kings_info$age ~ kings_info$birth)) boxplot(kings_info$age ~ factor(kings_info$dynasty), xlab = &#39;dynasty&#39;, ylab = &#39;age&#39;) .",
            "url": "https://stahangryum.github.io/Woo/crawling/r/2022/05/30/ming_qing.html",
            "relUrl": "/crawling/r/2022/05/30/ming_qing.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "조선 국왕 수명 데이터 분석",
            "content": "Reference . - 조선 국왕 데이터 . https://ko.wikipedia.org/wiki/%EC%A1%B0%EC%84%A0_%EA%B5%AD%EC%99%95 . Crawling . - 위키피디아에서 조선의 국와 정보를 크롤링합니다. . if (!require(rvest)) install.packages(&#39;rvest&#39;) library(rvest) if (!require(tidyverse)) install.packages(&#39;tidyverse&#39;) library(tidyverse) get_required_info = function(rows, order){ clean_table = rows[order] %&gt;% # 한글자씩 쪼갠다. strsplit(split = &#39;&#39;) %&gt;% unlist() %&gt;% str_replace(&#39;[^가-힣0-9]&#39;, &#39;N&#39;) %&gt;% # (한글, 숫자)가 아닌 문자는 불필요하므로 N으로 치환한다. paste(collapse = &#39;&#39;) %&gt;% # 모든 문자를 합친 다음 N을 기준으로 다시 나눈다. strsplit(&#39;N&#39;) %&gt;% unlist() order_index = grep(&#39;^제.{1,}대$&#39;, clean_table) order = clean_table[order_index] name_index = grep(&#39;^.{1,2}[조|종|군]$&#39;, clean_table)[1] name = clean_table[name_index] years_index = grep(&#39;^.{1,}년$&#39;, clean_table) years &lt;- # 순서대로 출생, 사망, 즉위, 퇴위 clean_table[years_index] %&gt;% substr(1, 4) grave_index = grep(&#39;^.{1,3}[릉|묘]$&#39;, clean_table) grave = clean_table[grave_index] return(c(order, name, years, grave)) } target_url &lt;- &#39;https://ko.wikipedia.org/wiki/%EC%A1%B0%EC%84%A0_%EA%B5%AD%EC%99%95&#39; unclean_table &lt;- read_html(target_url, encoding = &#39;UTF-8&#39;) %&gt;% html_nodes(&#39;table[class=wikitable]&#39;) %&gt;% html_nodes(&#39;tbody&#39;) %&gt;% html_nodes(&#39;tr&#39;) %&gt;% html_text() # 필요로 하는 데이터는 모두 &#39;제&#39;로 시작한다. # &#39;제&#39;로 시작하지 않는 데이터는 불필요하므로 제거한다. requiredRows_index &lt;- str_detect(unclean_table, &#39;^제&#39;) requiredRows &lt;- unclean_table[requiredRows_index] last_order = 26 kings_info &lt;- vector(&#39;list&#39;, last_order) for (i in 1:last_order){ kings_info[[i]] = get_required_info(requiredRows, i) } kings_info = do.call(&#39;rbind&#39;, kings_info) colnames(kings_info) = c(&#39;order&#39;, &#39;name&#39;, &#39;birth&#39;, &#39;death&#39;, &#39;king_start&#39;, &#39;king_end&#39;, &#39;grave&#39;) write.table(kings_info, &#39;Joseon_king.txt&#39;, row.names=FALSE) . 필요한 패키지를 로딩중입니다: rvest Warning message: &#34;패키지 &#39;rvest&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; 필요한 패키지를 로딩중입니다: tidyverse Warning message: &#34;패키지 &#39;tidyverse&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 Warning message: &#34;패키지 &#39;ggplot2&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tibble&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tidyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;readr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;purrr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;dplyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;stringr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;forcats&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x readr::guess_encoding() masks rvest::guess_encoding() x dplyr::lag() masks stats::lag() . kings_info . A data.frame: 26 × 9 ordernamebirthdeathking_startking_endgraveageclass . &lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;fct&gt; . 제1대 | 태조 | 1335 | 1408 | 1392 | 1398 | 건원릉 | 73 | Before | . 제2대 | 정종 | 1357 | 1419 | 1398 | 1400 | 후릉 | 62 | Before | . 제3대 | 태종 | 1367 | 1422 | 1400 | 1418 | 헌릉 | 55 | Before | . 제4대 | 세종 | 1397 | 1450 | 1418 | 1450 | 영릉 | 53 | Before | . 제5대 | 문종 | 1414 | 1452 | 1450 | 1452 | 현릉 | 38 | Before | . 제6대 | 단종 | 1441 | 1457 | 1452 | 1455 | 장릉 | 16 | Before | . 제7대 | 세조 | 1417 | 1468 | 1455 | 1468 | 광릉 | 51 | Before | . 제8대 | 예종 | 1450 | 1469 | 1468 | 1469 | 창릉 | 19 | Before | . 제9대 | 성종 | 1457 | 1494 | 1469 | 1494 | 선릉 | 37 | Before | . 제10대 | 연산군 | 1476 | 1506 | 1494 | 1506 | 연산군묘 | 30 | Before | . 제11대 | 중종 | 1488 | 1544 | 1506 | 1544 | 정릉 | 56 | Before | . 제12대 | 인종 | 1515 | 1545 | 1544 | 1545 | 효릉 | 30 | Before | . 제13대 | 명종 | 1534 | 1567 | 1545 | 1567 | 강릉 | 33 | Before | . 제14대 | 선조 | 1552 | 1608 | 1567 | 1608 | 목릉 | 56 | Before | . 제15대 | 광해군 | 1575 | 1641 | 1608 | 1623 | 광해군묘 | 66 | Before | . 제16대 | 인조 | 1595 | 1649 | 1623 | 1649 | 장릉 | 54 | Before | . 제17대 | 효종 | 1619 | 1659 | 1649 | 1659 | 영릉 | 40 | After | . 제18대 | 현종 | 1641 | 1674 | 1659 | 1674 | 숭릉 | 33 | After | . 제19대 | 숙종 | 1661 | 1720 | 1674 | 1720 | 명릉 | 59 | After | . 제20대 | 경종 | 1688 | 1724 | 1720 | 1724 | 의릉 | 36 | After | . 제21대 | 영조 | 1694 | 1776 | 1724 | 1776 | 원릉 | 82 | After | . 제22대 | 정조 | 1752 | 1800 | 1776 | 1800 | 건릉 | 48 | After | . 제23대 | 순조 | 1790 | 1834 | 1800 | 1834 | 인릉 | 44 | After | . 제24대 | 헌종 | 1827 | 1849 | 1834 | 1849 | 경릉 | 22 | After | . 제25대 | 철종 | 1831 | 1863 | 1849 | 1863 | 예릉 | 32 | After | . 제26대 | 고종 | 1852 | 1919 | 1863 | 1897 | 홍릉 | 67 | After | . Analysis . - 왕의 수명을 출생년도 기준으로 동의보감 편찬 이전과 이후로 나누었을 때, 유의미한 차이가 있는가? . kings_info &lt;- data.frame(kings_info) kings_info[, c(&#39;birth&#39;, &#39;death&#39;)] = kings_info[, c(&#39;birth&#39;, &#39;death&#39;)] %&gt;% unlist() %&gt;% as.numeric() kings_info = kings_info %&gt;% mutate(age = death - birth) plot(kings_info$birth, kings_info$age, xlab = &#39;birth_year&#39;, ylab = &#39;age&#39;, main = &#39;동의보감 1610년&#39;) Donguibogam = 1610 abline(v = Donguibogam, col = &#39;red&#39;) abline(lm(kings_info$age ~ kings_info$birth)) # 회귀선 # 상자그림 kings_info[&#39;class&#39;] = ifelse(kings_info[&#39;birth&#39;] &lt;= 1610, &#39;Before&#39;, &#39;After&#39;) %&gt;% factor(levels = c(&#39;Before&#39;, &#39;After&#39;)) boxplot(kings_info$age ~ kings_info$class, xlab = &#39;group&#39;, ylab = &#39;age&#39;, main = &#39;동의보감 이전과 이후&#39;) .",
            "url": "https://stahangryum.github.io/Woo/crawling/r/2022/05/30/Joseon_king.html",
            "relUrl": "/crawling/r/2022/05/30/Joseon_king.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Chudnovsky Algorithm",
            "content": "Chudnovsky Algorithm . The Chudnovsky algorithm is a fast method for calculating the digits of π. . Reference . https://en.wikipedia.org/wiki/Chudnovsky_algorithm . Code . import math from gmpy2 import mpz from time import time import gmpy2 def pi_chudnovsky_bs(digits): &quot;&quot;&quot; Compute int(pi * 10**digits) This is done using Chudnovsky&#39;s series with binary splitting &quot;&quot;&quot; C = 640320 C3_OVER_24 = C**3 // 24 def bs(a, b): &quot;&quot;&quot; Computes the terms for binary splitting the Chudnovsky infinite series a(a) = +/- (13591409 + 545140134*a) p(a) = (6*a-5)*(2*a-1)*(6*a-1) b(a) = 1 q(a) = a*a*a*C3_OVER_24 returns P(a,b), Q(a,b) and T(a,b) &quot;&quot;&quot; if b - a == 1: # Directly compute P(a,a+1), Q(a,a+1) and T(a,a+1) if a == 0: Pab = Qab = mpz(1) else: Pab = mpz((6*a-5)*(2*a-1)*(6*a-1)) Qab = mpz(a*a*a*C3_OVER_24) Tab = Pab * (13591409 + 545140134*a) # a(a) * p(a) if a &amp; 1: Tab = -Tab else: # Recursively compute P(a,b), Q(a,b) and T(a,b) # m is the midpoint of a and b m = (a + b) // 2 # Recursively calculate P(a,m), Q(a,m) and T(a,m) Pam, Qam, Tam = bs(a, m) # Recursively calculate P(m,b), Q(m,b) and T(m,b) Pmb, Qmb, Tmb = bs(m, b) # Now combine Pab = Pam * Pmb Qab = Qam * Qmb Tab = Qmb * Tam + Pam * Tmb return Pab, Qab, Tab # how many terms to compute DIGITS_PER_TERM = math.log10(C3_OVER_24/6/2/6) N = int(digits/DIGITS_PER_TERM + 1) # Calclate P(0,N) and Q(0,N) P, Q, T = bs(0, N) one_squared = mpz(10)**(2*digits) sqrtC = gmpy2.isqrt(10005*one_squared) return (Q*426880*sqrtC) // T # The last 5 digits or pi for various numbers of digits check_digits = { 100 : 70679, 1000 : 1989, 10000 : 75678, 100000 : 24646, 1000000 : 58151, 10000000 : 55897, } if __name__ == &quot;__main__&quot;: #raise SystemExit for log10_digits in range(1,8): # 10자리 ~ 1000만자리 digits = 10**log10_digits start =time() pi = pi_chudnovsky_bs(digits) f = open(&#39;d_&#39; + str(digits), &#39;w&#39;) f.write(str(pi)) f.close() print(&quot;chudnovsky_gmpy_mpz_bs: digits&quot;,digits,&quot;time&quot;,time()-start) if digits in check_digits: last_five_digits = pi % 100000 if check_digits[digits] == last_five_digits: print(&quot;Last 5 digits %05d OK&quot; % last_five_digits) else: print(&quot;Last 5 digits %05d wrong should be %05d&quot; % (last_five_digits, check_digits[digits])) . chudnovsky_gmpy_mpz_bs: digits 10 time 0.002651214599609375 chudnovsky_gmpy_mpz_bs: digits 100 time 0.0008170604705810547 Last 5 digits 70679 OK chudnovsky_gmpy_mpz_bs: digits 1000 time 0.0013988018035888672 Last 5 digits 01989 OK chudnovsky_gmpy_mpz_bs: digits 10000 time 0.0020301342010498047 Last 5 digits 75678 OK chudnovsky_gmpy_mpz_bs: digits 100000 time 0.03248786926269531 Last 5 digits 24646 OK chudnovsky_gmpy_mpz_bs: digits 1000000 time 0.5917906761169434 Last 5 digits 58151 OK chudnovsky_gmpy_mpz_bs: digits 10000000 time 11.120594263076782 Last 5 digits 55897 OK .",
            "url": "https://stahangryum.github.io/Woo/mathematics/python/2022/05/29/chudnovsky.html",
            "relUrl": "/mathematics/python/2022/05/29/chudnovsky.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Gaon Chart Crawling",
            "content": "Intro . - 가온 차트(Gaon Chart)는 대한민국의 대중음악 공인차트이다. 가온이라는 단어는 가운데, 중심이라는 뜻의 순우리말로, 중심이 되는 차트라는 의미에서 명명되었다. 한국음악콘텐츠협회가 운영하고 문화체육관광부가 후원하는 사업으로, 2년여 준비기간을 걸쳐 2010년 2월 23일 출범했다. . Crawling . if (!require(pacman)) install.packages(&#39;pacman&#39;); library(pacman) pacman::p_load(&quot;rvest&quot;, &quot;tidyverse&quot;) getLinks &lt;- function(termGbn){ # Input &#39;week&#39; or &#39;month&#39; base_url &lt;- &#39;http://gaonchart.co.kr/main/section/chart/online.gaon?nationGbn=T&amp;serviceGbn=ALL&#39; sample_url &lt;- paste0(base_url, &#39;&amp;termGbn=&#39;, termGbn) dates &lt;- read_html(sample_url) %&gt;% html_nodes(&#39;div[class=fr]&#39;) %&gt;% html_nodes(&#39;select&#39;) %&gt;% html_nodes(&#39;option&#39;) %&gt;% html_attr(&#39;value&#39;) %&gt;% str_subset(pattern = &#39;^[0-9]{6}$&#39;) # &#39;dates&#39;(YYYYMM) will be divided into hitYear(YYYY) and targetTime(MM). # Ex) dates &lt;- 202221. hitYear &lt;- dates %&gt;% substr(1,4) # Ex) hitYear &lt;- 2022. targetTime &lt;- dates %&gt;% substr(5,6) # Ex) hitYear &lt;- 21. complete_url &lt;- paste0(base_url, &#39;&amp;targetTime=&#39;, targetTime, &#39;&amp;hitYear=&#39;, hitYear, &#39;&amp;termGbn=&#39;, termGbn) return(complete_url) # Return all urls } getPage &lt;- function(target_url){ # Input complete URL including targetTime, hitYear, and termGbn url_splited &lt;- target_url %&gt;% strsplit(&#39;=|&amp;&#39;) %&gt;% unlist() year_temp &lt;- url_splited %&gt;% str_subset(&#39;^[0-9]{2,4}$&#39;) %&gt;% rev() %&gt;% paste0(collapse = &#39;&#39;) termGbn &lt;- url_splited %&gt;% &#39;[&#39;(length(url_splited)) html_chart &lt;- read_html(target_url) %&gt;% html_nodes(&#39;div[class=chart]&#39;) td &lt;- html_chart %&gt;% html_nodes(&#39;td&#39;) %&gt;% html_text() %&gt;% str_split(&#39; n| || r| t&#39;) %&gt;% unlist() %&gt;% str_subset(&#39;^$&#39;, negate = TRUE) change_grp &lt;- html_chart %&gt;% html_nodes(&#39;td[class=change]&#39;) %&gt;% html_nodes(&#39;span&#39;) %&gt;% html_attr(&#39;class&#39;) # sort by &#39;PLAY&#39; ## Ex) ## &quot;~~&quot;, &quot;~~&quot;, &quot;PLAY&quot;, &quot;~~&quot;, &quot;~~&quot;, &quot;PLAY&quot;, &quot;~~&quot;, &quot;~~&quot;, &quot;PLAY&quot; ## to ## |~~|~~|&#39;PLAY&#39;| ## |~~|~~|&#39;PLAY&#39;| ## |~~|~~|&#39;PLAY&#39;| # &#39;Gaon Score&#39; was newly created in Jan 2018. ## Before Jan 2018 : chart_piece has 11 columns. ## After Jan 2018 : chart_piece has 12 columns. index_PLAY &lt;- str_which(td, &#39;PLAY&#39;) PLAY_ZONE &lt;- c(11, 12) index_TRUE_PLAY &lt;- index_PLAY[index_PLAY %in% PLAY_ZONE][1] chart_piece &lt;- td %&gt;% matrix(ncol = index_TRUE_PLAY, byrow = TRUE) %&gt;% as_tibble() # Note : ifelse always returns an object of the same length as the condition. so we use if/else for this case. chart &lt;- tibble(year_temp = year_temp, ranking = chart_piece$V1, change_grp = change_grp, change_val = chart_piece$V2, title = chart_piece$V3, artist = chart_piece$V4, gaon_index = if (ncol(chart_piece) == 12) chart_piece$V6 else NA, production = if(ncol(chart_piece) == 12) chart_piece$V7 else chart_piece$V6, distribution = if(ncol(chart_piece) == 12) chart_piece$V8 else chart_piece$V7) chart$gaon_index &lt;- chart$gaon_index %&gt;% str_replace_all(&#39;[^0-9]&#39;, &#39;&#39;) %&gt;% as.numeric() colnames(chart)[1] &lt;- ifelse(termGbn == &#39;week&#39;, &#39;year_week&#39;, &#39;year_month&#39;) return(chart) # Return chart of the &#39;target_url&#39;. } # Weekly Rankings. # Time to run : 7~8min. weekLinks &lt;- getLinks(&#39;week&#39;) all_week_pages &lt;- vector(&#39;list&#39;, length(weekLinks)) for (i in 1:length(all_week_pages)) all_week_pages[[i]] &lt;- getPage(weekLinks[i]) week_final &lt;- do.call(&#39;rbind&#39;, all_week_pages) write.table(week_final, &#39;gaon_week.txt&#39;, row.names = FALSE) # Monthly Rankings. # Time to run : 1~2min. monthLinks &lt;- getLinks(&#39;month&#39;) all_month_pages &lt;- vector(&#39;list&#39;, length(monthLinks)) for (i in 1:length(all_month_pages)) all_month_pages[[i]] &lt;- getPage(monthLinks[i]) month_final &lt;- do.call(&#39;rbind&#39;, all_month_pages) write.table(month_final, &#39;gaon_month.txt&#39;, row.names = FALSE) . 필요한 패키지를 로딩중입니다: pacman Warning message: &#34;패키지 &#39;pacman&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. Using compatibility `.name_repair`. This warning is displayed once every 8 hours. Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.&#34; . head(week_final) . A tibble: 6 × 9 year_weekrankingchange_grpchange_valtitleartistgaon_indexproductiondistribution . &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt; . 202221 | 1 | &lt;span style=white-space:pre-wrap&gt;NA &lt;/span&gt; | - | That That (prod. &amp; feat. SUGA of BTS) | &lt;span style=white-space:pre-wrap&gt;싸이 (Psy) &lt;/span&gt; | 31776501 | &lt;span style=white-space:pre-wrap&gt;피네이션 &lt;/span&gt; | &lt;span style=white-space:pre-wrap&gt;Dreamus &lt;/span&gt; | . 202221 | 2 | up | 2 | LOVE DIVE | IVE (아이브) | 27807024 | 스타쉽엔터테인먼트 | Kakao Entertainment | . 202221 | 3 | up | 2 | TOMBOY | (여자)아이들 | 27241781 | 큐브엔터테인먼트 | Kakao Entertainment | . 202221 | 4 | down | 2 | 봄여름가을겨울 (Still Life) | BIGBANG (빅뱅) | 25590419 | YG Entertainment | YG PLUS | . 202221 | 5 | up | 1 | 사랑인가 봐 | 멜로망스(Melomance) | 22846960 | 플렉스엠 | Kakao Entertainment | . 202221 | 6 | down | 3 | 다시 만날 수 있을까 | 임영웅 | 19074593 | 물고기뮤직 | Dreamus | . head(month_final) . A tibble: 6 × 9 year_monthrankingchange_grpchange_valtitleartistgaon_indexproductiondistribution . &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt; . 202204 | 1 | new | new | 봄여름가을겨울 (Still Life) | BIGBANG (빅뱅) | 168901052 | YG Entertainment | YG PLUS | . 202204 | 2 | up | 1 | TOMBOY | (여자)아이들 | 133304108 | 큐브엔터테인먼트 | Kakao Entertainment | . 202204 | 3 | up | 23 | Feel My Rhythm | 레드벨벳(Red Velvet) | 111957588 | SM Entertainment | Dreamus | . 202204 | 4 | new | new | LOVE DIVE | IVE (아이브) | 105286003 | 스타쉽엔터테인먼트 | Kakao Entertainment | . 202204 | 5 | up | 3 | 사랑인가 봐 | 멜로망스(Melomance) | 100850288 | 플렉스엠 | Kakao Entertainment | . 202204 | 6 | down | 4 | GANADARA (Feat. 아이유) | 박재범 | 96954973 | MORE VISION | Kakao Entertainment | . Analysis . 미완성 | .",
            "url": "https://stahangryum.github.io/Woo/crawling/r/2022/05/29/GAON.html",
            "relUrl": "/crawling/r/2022/05/29/GAON.html",
            "date": " • May 29, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Santander Customer Satisfaction",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib . cust_df = pd.read_csv(&#39;dataset/santander/train_santander.csv&#39;, encoding = &#39;latin-1&#39;) cust_df.head(3) . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . 0 1 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 39205.17 | 0 | . 1 3 | 2 | 34 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 49278.03 | 0 | . 2 4 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 67333.77 | 0 | . 3 rows × 371 columns . cust_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 76020 entries, 0 to 76019 Columns: 371 entries, ID to TARGET dtypes: float64(111), int64(260) memory usage: 215.2 MB . cust_df.shape . (76020, 371) . print(cust_df[&#39;TARGET&#39;].value_counts()) unsatisfied_cnt = cust_df[cust_df[&#39;TARGET&#39;] == 1].TARGET.count() total_cnt = cust_df.TARGET.count() print(&#39;unsatisfied 비율 :&#39;, np.round(unsatisfied_cnt / total_cnt, 3)) . 0 73012 1 3008 Name: TARGET, dtype: int64 unsatisfied 비율 : 0.04 . 불만족 비율은 전체의 4%에 불과하다. | . cust_df.describe() . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . count 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | ... | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 7.602000e+04 | 76020.000000 | . mean 75964.050723 | -1523.199277 | 33.212865 | 86.208265 | 72.363067 | 119.529632 | 3.559130 | 6.472698 | 0.412946 | 0.567352 | ... | 7.935824 | 1.365146 | 12.215580 | 8.784074 | 31.505324 | 1.858575 | 76.026165 | 56.614351 | 1.172358e+05 | 0.039569 | . std 43781.947379 | 39033.462364 | 12.956486 | 1614.757313 | 339.315831 | 546.266294 | 93.155749 | 153.737066 | 30.604864 | 36.513513 | ... | 455.887218 | 113.959637 | 783.207399 | 538.439211 | 2013.125393 | 147.786584 | 4040.337842 | 2852.579397 | 1.826646e+05 | 0.194945 | . min 1.000000 | -999999.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 5.163750e+03 | 0.000000 | . 25% 38104.750000 | 2.000000 | 23.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.787061e+04 | 0.000000 | . 50% 76043.000000 | 2.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.064092e+05 | 0.000000 | . 75% 113748.750000 | 2.000000 | 40.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.187563e+05 | 0.000000 | . max 151838.000000 | 238.000000 | 105.000000 | 210000.000000 | 12888.030000 | 21024.810000 | 8237.820000 | 11073.570000 | 6600.000000 | 6600.000000 | ... | 50003.880000 | 20385.720000 | 138831.630000 | 91778.730000 | 438329.220000 | 24650.010000 | 681462.900000 | 397884.300000 | 2.203474e+07 | 1.000000 | . 8 rows × 371 columns . cust_df[&#39;var3&#39;].replace(-999999, 2, inplace = True) cust_df.drop(&#39;ID&#39;, axis = 1, inplace = True) X_features = cust_df.iloc[:, :-1] y_labels = cust_df.iloc[:, -1] print(&#39;피쳐 데이터 shape :&#39;, X_features.shape) . 피쳐 데이터 shape : (76020, 369) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size = 0.2, random_state = 0) train_cnt = y_train.count() test_cnt = y_test.count() print(&#39;학습 세트 Shape : {0}, 테스트 세트 Shape : {1}&#39;.format(X_train.shape, X_test.shape)) print(&#39; 학습 세트 레이블 값 분포 비율&#39;) print(y_train.value_counts()/train_cnt) print(&#39; n 테스트 세트 레이블 값 분포 비율&#39;) print(y_test.value_counts()/test_cnt) . 학습 세트 Shape : (60816, 369), 테스트 세트 Shape : (15204, 369) 학습 세트 레이블 값 분포 비율 0 0.960964 1 0.039036 Name: TARGET, dtype: float64 테스트 세트 레이블 값 분포 비율 0 0.9583 1 0.0417 Name: TARGET, dtype: float64 . X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0) . from xgboost import XGBClassifier from sklearn.metrics import roc_auc_score xgb_clf = XGBClassifier(n_estimators = 500, random_state = 156) # 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행 xgb_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = &#39;logloss&#39;, eval_set = [(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average =&#39;macro&#39;) print(&#39;ROC AUC :&#39;, xgb_roc_score) . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-logloss:0.47251 validation_1-logloss:0.47431 [1] validation_0-logloss:0.35180 validation_1-logloss:0.35486 [2] validation_0-logloss:0.27740 validation_1-logloss:0.28183 [3] validation_0-logloss:0.22891 validation_1-logloss:0.23460 [4] validation_0-logloss:0.19656 validation_1-logloss:0.20328 [5] validation_0-logloss:0.17445 validation_1-logloss:0.18210 [6] validation_0-logloss:0.15934 validation_1-logloss:0.16792 [7] validation_0-logloss:0.14870 validation_1-logloss:0.15822 [8] validation_0-logloss:0.14125 validation_1-logloss:0.15182 [9] validation_0-logloss:0.13608 validation_1-logloss:0.14740 [10] validation_0-logloss:0.13231 validation_1-logloss:0.14457 [11] validation_0-logloss:0.12940 validation_1-logloss:0.14263 [12] validation_0-logloss:0.12715 validation_1-logloss:0.14133 [13] validation_0-logloss:0.12546 validation_1-logloss:0.14054 [14] validation_0-logloss:0.12434 validation_1-logloss:0.14000 [15] validation_0-logloss:0.12319 validation_1-logloss:0.13974 [16] validation_0-logloss:0.12237 validation_1-logloss:0.13947 [17] validation_0-logloss:0.12181 validation_1-logloss:0.13941 [18] validation_0-logloss:0.12139 validation_1-logloss:0.13942 [19] validation_0-logloss:0.12091 validation_1-logloss:0.13939 [20] validation_0-logloss:0.12018 validation_1-logloss:0.13936 [21] validation_0-logloss:0.11999 validation_1-logloss:0.13934 [22] validation_0-logloss:0.11947 validation_1-logloss:0.13933 [23] validation_0-logloss:0.11929 validation_1-logloss:0.13930 [24] validation_0-logloss:0.11876 validation_1-logloss:0.13931 [25] validation_0-logloss:0.11847 validation_1-logloss:0.13924 [26] validation_0-logloss:0.11811 validation_1-logloss:0.13900 [27] validation_0-logloss:0.11783 validation_1-logloss:0.13903 [28] validation_0-logloss:0.11766 validation_1-logloss:0.13908 [29] validation_0-logloss:0.11697 validation_1-logloss:0.13931 [30] validation_0-logloss:0.11663 validation_1-logloss:0.13943 [31] validation_0-logloss:0.11587 validation_1-logloss:0.13948 [32] validation_0-logloss:0.11521 validation_1-logloss:0.13967 [33] validation_0-logloss:0.11460 validation_1-logloss:0.13973 [34] validation_0-logloss:0.11446 validation_1-logloss:0.13971 [35] validation_0-logloss:0.11440 validation_1-logloss:0.13974 [36] validation_0-logloss:0.11385 validation_1-logloss:0.13981 [37] validation_0-logloss:0.11368 validation_1-logloss:0.13986 [38] validation_0-logloss:0.11361 validation_1-logloss:0.13986 [39] validation_0-logloss:0.11351 validation_1-logloss:0.13986 [40] validation_0-logloss:0.11331 validation_1-logloss:0.13998 [41] validation_0-logloss:0.11291 validation_1-logloss:0.14007 [42] validation_0-logloss:0.11281 validation_1-logloss:0.14013 [43] validation_0-logloss:0.11269 validation_1-logloss:0.14011 [44] validation_0-logloss:0.11255 validation_1-logloss:0.14015 [45] validation_0-logloss:0.11250 validation_1-logloss:0.14011 [46] validation_0-logloss:0.11246 validation_1-logloss:0.14014 [47] validation_0-logloss:0.11238 validation_1-logloss:0.14017 [48] validation_0-logloss:0.11220 validation_1-logloss:0.14014 [49] validation_0-logloss:0.11200 validation_1-logloss:0.14022 [50] validation_0-logloss:0.11190 validation_1-logloss:0.14028 [51] validation_0-logloss:0.11108 validation_1-logloss:0.14027 [52] validation_0-logloss:0.11041 validation_1-logloss:0.14031 [53] validation_0-logloss:0.10983 validation_1-logloss:0.14020 [54] validation_0-logloss:0.10961 validation_1-logloss:0.14015 [55] validation_0-logloss:0.10949 validation_1-logloss:0.14020 [56] validation_0-logloss:0.10889 validation_1-logloss:0.14026 [57] validation_0-logloss:0.10875 validation_1-logloss:0.14030 [58] validation_0-logloss:0.10831 validation_1-logloss:0.14025 [59] validation_0-logloss:0.10778 validation_1-logloss:0.14045 [60] validation_0-logloss:0.10756 validation_1-logloss:0.14051 [61] validation_0-logloss:0.10745 validation_1-logloss:0.14064 [62] validation_0-logloss:0.10702 validation_1-logloss:0.14070 [63] validation_0-logloss:0.10694 validation_1-logloss:0.14072 [64] validation_0-logloss:0.10689 validation_1-logloss:0.14072 [65] validation_0-logloss:0.10683 validation_1-logloss:0.14070 [66] validation_0-logloss:0.10654 validation_1-logloss:0.14073 [67] validation_0-logloss:0.10648 validation_1-logloss:0.14073 [68] validation_0-logloss:0.10628 validation_1-logloss:0.14087 [69] validation_0-logloss:0.10622 validation_1-logloss:0.14086 [70] validation_0-logloss:0.10616 validation_1-logloss:0.14091 [71] validation_0-logloss:0.10609 validation_1-logloss:0.14090 [72] validation_0-logloss:0.10581 validation_1-logloss:0.14109 [73] validation_0-logloss:0.10575 validation_1-logloss:0.14106 [74] validation_0-logloss:0.10562 validation_1-logloss:0.14110 [75] validation_0-logloss:0.10557 validation_1-logloss:0.14119 [76] validation_0-logloss:0.10490 validation_1-logloss:0.14114 [77] validation_0-logloss:0.10436 validation_1-logloss:0.14136 [78] validation_0-logloss:0.10396 validation_1-logloss:0.14137 [79] validation_0-logloss:0.10374 validation_1-logloss:0.14146 [80] validation_0-logloss:0.10347 validation_1-logloss:0.14155 [81] validation_0-logloss:0.10301 validation_1-logloss:0.14175 [82] validation_0-logloss:0.10283 validation_1-logloss:0.14162 [83] validation_0-logloss:0.10250 validation_1-logloss:0.14169 [84] validation_0-logloss:0.10239 validation_1-logloss:0.14176 [85] validation_0-logloss:0.10214 validation_1-logloss:0.14184 [86] validation_0-logloss:0.10211 validation_1-logloss:0.14185 [87] validation_0-logloss:0.10206 validation_1-logloss:0.14185 [88] validation_0-logloss:0.10188 validation_1-logloss:0.14194 [89] validation_0-logloss:0.10178 validation_1-logloss:0.14200 [90] validation_0-logloss:0.10174 validation_1-logloss:0.14204 [91] validation_0-logloss:0.10162 validation_1-logloss:0.14209 [92] validation_0-logloss:0.10159 validation_1-logloss:0.14209 [93] validation_0-logloss:0.10122 validation_1-logloss:0.14221 [94] validation_0-logloss:0.10118 validation_1-logloss:0.14222 [95] validation_0-logloss:0.10113 validation_1-logloss:0.14231 [96] validation_0-logloss:0.10082 validation_1-logloss:0.14250 [97] validation_0-logloss:0.10077 validation_1-logloss:0.14253 [98] validation_0-logloss:0.10035 validation_1-logloss:0.14265 [99] validation_0-logloss:0.09991 validation_1-logloss:0.14271 [100] validation_0-logloss:0.09964 validation_1-logloss:0.14277 [101] validation_0-logloss:0.09931 validation_1-logloss:0.14291 [102] validation_0-logloss:0.09928 validation_1-logloss:0.14291 [103] validation_0-logloss:0.09885 validation_1-logloss:0.14307 [104] validation_0-logloss:0.09873 validation_1-logloss:0.14323 [105] validation_0-logloss:0.09853 validation_1-logloss:0.14331 [106] validation_0-logloss:0.09799 validation_1-logloss:0.14359 [107] validation_0-logloss:0.09796 validation_1-logloss:0.14366 [108] validation_0-logloss:0.09782 validation_1-logloss:0.14385 [109] validation_0-logloss:0.09776 validation_1-logloss:0.14388 [110] validation_0-logloss:0.09756 validation_1-logloss:0.14400 [111] validation_0-logloss:0.09752 validation_1-logloss:0.14398 [112] validation_0-logloss:0.09728 validation_1-logloss:0.14405 [113] validation_0-logloss:0.09701 validation_1-logloss:0.14429 [114] validation_0-logloss:0.09689 validation_1-logloss:0.14426 [115] validation_0-logloss:0.09668 validation_1-logloss:0.14434 [116] validation_0-logloss:0.09663 validation_1-logloss:0.14439 [117] validation_0-logloss:0.09649 validation_1-logloss:0.14446 [118] validation_0-logloss:0.09641 validation_1-logloss:0.14450 [119] validation_0-logloss:0.09613 validation_1-logloss:0.14447 [120] validation_0-logloss:0.09606 validation_1-logloss:0.14455 [121] validation_0-logloss:0.09593 validation_1-logloss:0.14451 [122] validation_0-logloss:0.09566 validation_1-logloss:0.14469 [123] validation_0-logloss:0.09547 validation_1-logloss:0.14479 [124] validation_0-logloss:0.09542 validation_1-logloss:0.14476 [125] validation_0-logloss:0.09524 validation_1-logloss:0.14471 ROC AUC : 0.8410117370942843 . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 xgb_clf = XGBClassifier(n_estimators = 100) params = {&#39;max_depth&#39;:[5,7], &#39;min_child_weight&#39;:[1,3], &#39;colsample_bytree&#39;:[0.5, 0.75] } gridcv = GridSearchCV(xgb_clf, param_grid=params, cv = 3) # 2*2*2*3 gridcv.fit(X_train, y_train, early_stopping_rounds = 30, eval_metric = &#39;auc&#39;, eval_set=[(X_tr, y_tr), (X_val, y_val)]) print(&#39;GridSearchCV 최적 파라미터:&#39;, gridcv.best_params_) xgb_rod_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1], average = &#39;macro&#39;) print(&#39;ROC AUC :&#39;, xgb_roc_score) . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79161 validation_1-auc:0.79321 [1] validation_0-auc:0.81865 validation_1-auc:0.81375 [2] validation_0-auc:0.82586 validation_1-auc:0.81846 [3] validation_0-auc:0.82789 validation_1-auc:0.82226 [4] validation_0-auc:0.83249 validation_1-auc:0.82677 [5] validation_0-auc:0.83477 validation_1-auc:0.83225 [6] validation_0-auc:0.83340 validation_1-auc:0.82654 [7] validation_0-auc:0.84223 validation_1-auc:0.83486 [8] validation_0-auc:0.84586 validation_1-auc:0.83682 [9] validation_0-auc:0.84557 validation_1-auc:0.83472 [10] validation_0-auc:0.84423 validation_1-auc:0.83181 [11] validation_0-auc:0.84428 validation_1-auc:0.82920 [12] validation_0-auc:0.85176 validation_1-auc:0.83433 [13] validation_0-auc:0.85540 validation_1-auc:0.83565 [14] validation_0-auc:0.85718 validation_1-auc:0.83696 [15] validation_0-auc:0.85851 validation_1-auc:0.83561 [16] validation_0-auc:0.85964 validation_1-auc:0.83578 [17] validation_0-auc:0.86091 validation_1-auc:0.83570 [18] validation_0-auc:0.86188 validation_1-auc:0.83595 [19] validation_0-auc:0.86249 validation_1-auc:0.83552 [20] validation_0-auc:0.86298 validation_1-auc:0.83452 [21] validation_0-auc:0.86375 validation_1-auc:0.83437 [22] validation_0-auc:0.86440 validation_1-auc:0.83516 [23] validation_0-auc:0.86554 validation_1-auc:0.83470 [24] validation_0-auc:0.86601 validation_1-auc:0.83492 [25] validation_0-auc:0.86700 validation_1-auc:0.83510 [26] validation_0-auc:0.86770 validation_1-auc:0.83412 [27] validation_0-auc:0.86852 validation_1-auc:0.83394 [28] validation_0-auc:0.86898 validation_1-auc:0.83441 [29] validation_0-auc:0.86914 validation_1-auc:0.83440 [30] validation_0-auc:0.86953 validation_1-auc:0.83380 [31] validation_0-auc:0.87051 validation_1-auc:0.83346 [32] validation_0-auc:0.87085 validation_1-auc:0.83334 [33] validation_0-auc:0.87112 validation_1-auc:0.83313 [34] validation_0-auc:0.87161 validation_1-auc:0.83383 [35] validation_0-auc:0.87173 validation_1-auc:0.83376 [36] validation_0-auc:0.87260 validation_1-auc:0.83340 [37] validation_0-auc:0.87310 validation_1-auc:0.83344 [38] validation_0-auc:0.87322 validation_1-auc:0.83343 [39] validation_0-auc:0.87339 validation_1-auc:0.83370 [40] validation_0-auc:0.87351 validation_1-auc:0.83373 [41] validation_0-auc:0.87411 validation_1-auc:0.83358 [42] validation_0-auc:0.87433 validation_1-auc:0.83325 [43] validation_0-auc:0.87432 validation_1-auc:0.83319 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80013 validation_1-auc:0.79685 [1] validation_0-auc:0.82084 validation_1-auc:0.81574 [2] validation_0-auc:0.82744 validation_1-auc:0.82189 [3] validation_0-auc:0.83029 validation_1-auc:0.82317 [4] validation_0-auc:0.83578 validation_1-auc:0.82564 [5] validation_0-auc:0.83777 validation_1-auc:0.83385 [6] validation_0-auc:0.83742 validation_1-auc:0.83162 [7] validation_0-auc:0.84373 validation_1-auc:0.83436 [8] validation_0-auc:0.84836 validation_1-auc:0.83664 [9] validation_0-auc:0.84790 validation_1-auc:0.83583 [10] validation_0-auc:0.84717 validation_1-auc:0.83268 [11] validation_0-auc:0.84654 validation_1-auc:0.83066 [12] validation_0-auc:0.85377 validation_1-auc:0.83579 [13] validation_0-auc:0.85800 validation_1-auc:0.83859 [14] validation_0-auc:0.85962 validation_1-auc:0.83984 [15] validation_0-auc:0.86143 validation_1-auc:0.84003 [16] validation_0-auc:0.86269 validation_1-auc:0.84049 [17] validation_0-auc:0.86399 validation_1-auc:0.84009 [18] validation_0-auc:0.86474 validation_1-auc:0.84034 [19] validation_0-auc:0.86662 validation_1-auc:0.84138 [20] validation_0-auc:0.86730 validation_1-auc:0.84100 [21] validation_0-auc:0.86821 validation_1-auc:0.84058 [22] validation_0-auc:0.86942 validation_1-auc:0.84128 [23] validation_0-auc:0.86992 validation_1-auc:0.84122 [24] validation_0-auc:0.87035 validation_1-auc:0.84116 [25] validation_0-auc:0.87091 validation_1-auc:0.84045 [26] validation_0-auc:0.87139 validation_1-auc:0.83974 [27] validation_0-auc:0.87296 validation_1-auc:0.83926 [28] validation_0-auc:0.87307 validation_1-auc:0.83943 [29] validation_0-auc:0.87330 validation_1-auc:0.84017 [30] validation_0-auc:0.87443 validation_1-auc:0.83949 [31] validation_0-auc:0.87467 validation_1-auc:0.83936 [32] validation_0-auc:0.87513 validation_1-auc:0.83943 [33] validation_0-auc:0.87519 validation_1-auc:0.83951 [34] validation_0-auc:0.87542 validation_1-auc:0.83953 [35] validation_0-auc:0.87552 validation_1-auc:0.83946 [36] validation_0-auc:0.87582 validation_1-auc:0.83936 [37] validation_0-auc:0.87604 validation_1-auc:0.83919 [38] validation_0-auc:0.87622 validation_1-auc:0.83874 [39] validation_0-auc:0.87670 validation_1-auc:0.83844 [40] validation_0-auc:0.87678 validation_1-auc:0.83859 [41] validation_0-auc:0.87711 validation_1-auc:0.83830 [42] validation_0-auc:0.87738 validation_1-auc:0.83823 [43] validation_0-auc:0.87752 validation_1-auc:0.83796 [44] validation_0-auc:0.87777 validation_1-auc:0.83765 [45] validation_0-auc:0.87785 validation_1-auc:0.83786 [46] validation_0-auc:0.87802 validation_1-auc:0.83761 [47] validation_0-auc:0.87840 validation_1-auc:0.83698 [48] validation_0-auc:0.87868 validation_1-auc:0.83699 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80039 validation_1-auc:0.80013 [1] validation_0-auc:0.82111 validation_1-auc:0.82026 [2] validation_0-auc:0.82749 validation_1-auc:0.82627 [3] validation_0-auc:0.83124 validation_1-auc:0.82830 [4] validation_0-auc:0.83475 validation_1-auc:0.82881 [5] validation_0-auc:0.83676 validation_1-auc:0.83385 [6] validation_0-auc:0.83648 validation_1-auc:0.83085 [7] validation_0-auc:0.84336 validation_1-auc:0.83472 [8] validation_0-auc:0.84624 validation_1-auc:0.83404 [9] validation_0-auc:0.84541 validation_1-auc:0.83287 [10] validation_0-auc:0.84554 validation_1-auc:0.83039 [11] validation_0-auc:0.84525 validation_1-auc:0.82995 [12] validation_0-auc:0.85144 validation_1-auc:0.83489 [13] validation_0-auc:0.85525 validation_1-auc:0.83803 [14] validation_0-auc:0.85745 validation_1-auc:0.84145 [15] validation_0-auc:0.85817 validation_1-auc:0.84082 [16] validation_0-auc:0.86006 validation_1-auc:0.84076 [17] validation_0-auc:0.86127 validation_1-auc:0.84139 [18] validation_0-auc:0.86194 validation_1-auc:0.84041 [19] validation_0-auc:0.86337 validation_1-auc:0.84100 [20] validation_0-auc:0.86386 validation_1-auc:0.84145 [21] validation_0-auc:0.86550 validation_1-auc:0.84030 [22] validation_0-auc:0.86690 validation_1-auc:0.84072 [23] validation_0-auc:0.86765 validation_1-auc:0.84077 [24] validation_0-auc:0.86827 validation_1-auc:0.84136 [25] validation_0-auc:0.86939 validation_1-auc:0.84120 [26] validation_0-auc:0.87045 validation_1-auc:0.84098 [27] validation_0-auc:0.87062 validation_1-auc:0.84148 [28] validation_0-auc:0.87072 validation_1-auc:0.84120 [29] validation_0-auc:0.87113 validation_1-auc:0.84147 [30] validation_0-auc:0.87115 validation_1-auc:0.84181 [31] validation_0-auc:0.87145 validation_1-auc:0.84172 [32] validation_0-auc:0.87226 validation_1-auc:0.84100 [33] validation_0-auc:0.87242 validation_1-auc:0.84149 [34] validation_0-auc:0.87255 validation_1-auc:0.84120 [35] validation_0-auc:0.87297 validation_1-auc:0.84095 [36] validation_0-auc:0.87348 validation_1-auc:0.84051 [37] validation_0-auc:0.87395 validation_1-auc:0.84084 [38] validation_0-auc:0.87433 validation_1-auc:0.84055 [39] validation_0-auc:0.87448 validation_1-auc:0.84048 [40] validation_0-auc:0.87465 validation_1-auc:0.84042 [41] validation_0-auc:0.87486 validation_1-auc:0.84034 [42] validation_0-auc:0.87518 validation_1-auc:0.84021 [43] validation_0-auc:0.87525 validation_1-auc:0.84022 [44] validation_0-auc:0.87595 validation_1-auc:0.83967 [45] validation_0-auc:0.87629 validation_1-auc:0.84004 [46] validation_0-auc:0.87704 validation_1-auc:0.83966 [47] validation_0-auc:0.87746 validation_1-auc:0.83963 [48] validation_0-auc:0.87774 validation_1-auc:0.83931 [49] validation_0-auc:0.87784 validation_1-auc:0.83925 [50] validation_0-auc:0.87826 validation_1-auc:0.83935 [51] validation_0-auc:0.87861 validation_1-auc:0.83920 [52] validation_0-auc:0.87950 validation_1-auc:0.83895 [53] validation_0-auc:0.88024 validation_1-auc:0.83876 [54] validation_0-auc:0.88117 validation_1-auc:0.83840 [55] validation_0-auc:0.88126 validation_1-auc:0.83834 [56] validation_0-auc:0.88145 validation_1-auc:0.83873 [57] validation_0-auc:0.88157 validation_1-auc:0.83860 [58] validation_0-auc:0.88178 validation_1-auc:0.83810 [59] validation_0-auc:0.88186 validation_1-auc:0.83774 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79210 validation_1-auc:0.79292 [1] validation_0-auc:0.81759 validation_1-auc:0.81404 [2] validation_0-auc:0.82567 validation_1-auc:0.81864 [3] validation_0-auc:0.82819 validation_1-auc:0.82244 [4] validation_0-auc:0.83233 validation_1-auc:0.82618 [5] validation_0-auc:0.83480 validation_1-auc:0.83163 [6] validation_0-auc:0.83342 validation_1-auc:0.82840 [7] validation_0-auc:0.84265 validation_1-auc:0.83512 [8] validation_0-auc:0.84614 validation_1-auc:0.83742 [9] validation_0-auc:0.84573 validation_1-auc:0.83475 [10] validation_0-auc:0.84426 validation_1-auc:0.83066 [11] validation_0-auc:0.84358 validation_1-auc:0.82937 [12] validation_0-auc:0.85089 validation_1-auc:0.83491 [13] validation_0-auc:0.85457 validation_1-auc:0.83785 [14] validation_0-auc:0.85645 validation_1-auc:0.83894 [15] validation_0-auc:0.85744 validation_1-auc:0.83784 [16] validation_0-auc:0.85870 validation_1-auc:0.83899 [17] validation_0-auc:0.86002 validation_1-auc:0.83854 [18] validation_0-auc:0.86091 validation_1-auc:0.83860 [19] validation_0-auc:0.86154 validation_1-auc:0.83818 [20] validation_0-auc:0.86189 validation_1-auc:0.83772 [21] validation_0-auc:0.86295 validation_1-auc:0.83703 [22] validation_0-auc:0.86334 validation_1-auc:0.83721 [23] validation_0-auc:0.86402 validation_1-auc:0.83581 [24] validation_0-auc:0.86456 validation_1-auc:0.83557 [25] validation_0-auc:0.86494 validation_1-auc:0.83534 [26] validation_0-auc:0.86516 validation_1-auc:0.83481 [27] validation_0-auc:0.86660 validation_1-auc:0.83557 [28] validation_0-auc:0.86784 validation_1-auc:0.83546 [29] validation_0-auc:0.86793 validation_1-auc:0.83545 [30] validation_0-auc:0.86840 validation_1-auc:0.83496 [31] validation_0-auc:0.86867 validation_1-auc:0.83481 [32] validation_0-auc:0.86884 validation_1-auc:0.83472 [33] validation_0-auc:0.86900 validation_1-auc:0.83482 [34] validation_0-auc:0.86907 validation_1-auc:0.83423 [35] validation_0-auc:0.86981 validation_1-auc:0.83350 [36] validation_0-auc:0.86996 validation_1-auc:0.83334 [37] validation_0-auc:0.87004 validation_1-auc:0.83365 [38] validation_0-auc:0.87022 validation_1-auc:0.83384 [39] validation_0-auc:0.87078 validation_1-auc:0.83373 [40] validation_0-auc:0.87094 validation_1-auc:0.83373 [41] validation_0-auc:0.87109 validation_1-auc:0.83359 [42] validation_0-auc:0.87173 validation_1-auc:0.83365 [43] validation_0-auc:0.87264 validation_1-auc:0.83386 [44] validation_0-auc:0.87336 validation_1-auc:0.83319 [45] validation_0-auc:0.87361 validation_1-auc:0.83318 [46] validation_0-auc:0.87406 validation_1-auc:0.83227 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79931 validation_1-auc:0.79594 [1] validation_0-auc:0.81987 validation_1-auc:0.81503 [2] validation_0-auc:0.82734 validation_1-auc:0.82126 [3] validation_0-auc:0.83110 validation_1-auc:0.82302 [4] validation_0-auc:0.83608 validation_1-auc:0.82494 [5] validation_0-auc:0.83914 validation_1-auc:0.83100 [6] validation_0-auc:0.83828 validation_1-auc:0.82999 [7] validation_0-auc:0.84425 validation_1-auc:0.83439 [8] validation_0-auc:0.84749 validation_1-auc:0.83609 [9] validation_0-auc:0.84727 validation_1-auc:0.83597 [10] validation_0-auc:0.84703 validation_1-auc:0.83250 [11] validation_0-auc:0.84664 validation_1-auc:0.83237 [12] validation_0-auc:0.85343 validation_1-auc:0.83713 [13] validation_0-auc:0.85671 validation_1-auc:0.83887 [14] validation_0-auc:0.85824 validation_1-auc:0.83919 [15] validation_0-auc:0.85962 validation_1-auc:0.83905 [16] validation_0-auc:0.86089 validation_1-auc:0.84031 [17] validation_0-auc:0.86216 validation_1-auc:0.84051 [18] validation_0-auc:0.86264 validation_1-auc:0.84051 [19] validation_0-auc:0.86341 validation_1-auc:0.84030 [20] validation_0-auc:0.86379 validation_1-auc:0.83988 [21] validation_0-auc:0.86413 validation_1-auc:0.84020 [22] validation_0-auc:0.86513 validation_1-auc:0.84033 [23] validation_0-auc:0.86584 validation_1-auc:0.84016 [24] validation_0-auc:0.86638 validation_1-auc:0.84016 [25] validation_0-auc:0.86691 validation_1-auc:0.83991 [26] validation_0-auc:0.86798 validation_1-auc:0.83979 [27] validation_0-auc:0.86869 validation_1-auc:0.83952 [28] validation_0-auc:0.86881 validation_1-auc:0.83942 [29] validation_0-auc:0.86908 validation_1-auc:0.83912 [30] validation_0-auc:0.86934 validation_1-auc:0.83907 [31] validation_0-auc:0.86942 validation_1-auc:0.83896 [32] validation_0-auc:0.87000 validation_1-auc:0.83860 [33] validation_0-auc:0.87016 validation_1-auc:0.83878 [34] validation_0-auc:0.87050 validation_1-auc:0.83830 [35] validation_0-auc:0.87069 validation_1-auc:0.83825 [36] validation_0-auc:0.87118 validation_1-auc:0.83880 [37] validation_0-auc:0.87126 validation_1-auc:0.83883 [38] validation_0-auc:0.87138 validation_1-auc:0.83882 [39] validation_0-auc:0.87243 validation_1-auc:0.83833 [40] validation_0-auc:0.87267 validation_1-auc:0.83813 [41] validation_0-auc:0.87282 validation_1-auc:0.83811 [42] validation_0-auc:0.87356 validation_1-auc:0.83806 [43] validation_0-auc:0.87372 validation_1-auc:0.83815 [44] validation_0-auc:0.87384 validation_1-auc:0.83807 [45] validation_0-auc:0.87395 validation_1-auc:0.83813 [46] validation_0-auc:0.87450 validation_1-auc:0.83757 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80248 validation_1-auc:0.80001 [1] validation_0-auc:0.82249 validation_1-auc:0.81765 [2] validation_0-auc:0.82833 validation_1-auc:0.82524 [3] validation_0-auc:0.83371 validation_1-auc:0.82814 [4] validation_0-auc:0.83653 validation_1-auc:0.82856 [5] validation_0-auc:0.83838 validation_1-auc:0.83345 [6] validation_0-auc:0.83823 validation_1-auc:0.83165 [7] validation_0-auc:0.84386 validation_1-auc:0.83505 [8] validation_0-auc:0.84688 validation_1-auc:0.83507 [9] validation_0-auc:0.84634 validation_1-auc:0.83483 [10] validation_0-auc:0.84564 validation_1-auc:0.83324 [11] validation_0-auc:0.84501 validation_1-auc:0.83283 [12] validation_0-auc:0.85011 validation_1-auc:0.83693 [13] validation_0-auc:0.85299 validation_1-auc:0.83995 [14] validation_0-auc:0.85523 validation_1-auc:0.84250 [15] validation_0-auc:0.85608 validation_1-auc:0.84183 [16] validation_0-auc:0.85748 validation_1-auc:0.84319 [17] validation_0-auc:0.85895 validation_1-auc:0.84363 [18] validation_0-auc:0.85944 validation_1-auc:0.84311 [19] validation_0-auc:0.86102 validation_1-auc:0.84368 [20] validation_0-auc:0.86122 validation_1-auc:0.84367 [21] validation_0-auc:0.86196 validation_1-auc:0.84403 [22] validation_0-auc:0.86291 validation_1-auc:0.84498 [23] validation_0-auc:0.86385 validation_1-auc:0.84460 [24] validation_0-auc:0.86452 validation_1-auc:0.84460 [25] validation_0-auc:0.86534 validation_1-auc:0.84480 [26] validation_0-auc:0.86584 validation_1-auc:0.84441 [27] validation_0-auc:0.86653 validation_1-auc:0.84401 [28] validation_0-auc:0.86697 validation_1-auc:0.84422 [29] validation_0-auc:0.86770 validation_1-auc:0.84385 [30] validation_0-auc:0.86777 validation_1-auc:0.84407 [31] validation_0-auc:0.86803 validation_1-auc:0.84395 [32] validation_0-auc:0.86826 validation_1-auc:0.84381 [33] validation_0-auc:0.86862 validation_1-auc:0.84417 [34] validation_0-auc:0.86902 validation_1-auc:0.84385 [35] validation_0-auc:0.86959 validation_1-auc:0.84369 [36] validation_0-auc:0.87020 validation_1-auc:0.84297 [37] validation_0-auc:0.87047 validation_1-auc:0.84278 [38] validation_0-auc:0.87175 validation_1-auc:0.84286 [39] validation_0-auc:0.87269 validation_1-auc:0.84224 [40] validation_0-auc:0.87289 validation_1-auc:0.84197 [41] validation_0-auc:0.87294 validation_1-auc:0.84175 [42] validation_0-auc:0.87418 validation_1-auc:0.84148 [43] validation_0-auc:0.87431 validation_1-auc:0.84121 [44] validation_0-auc:0.87441 validation_1-auc:0.84127 [45] validation_0-auc:0.87458 validation_1-auc:0.84103 [46] validation_0-auc:0.87475 validation_1-auc:0.84119 [47] validation_0-auc:0.87529 validation_1-auc:0.84128 [48] validation_0-auc:0.87554 validation_1-auc:0.84050 [49] validation_0-auc:0.87572 validation_1-auc:0.84039 [50] validation_0-auc:0.87575 validation_1-auc:0.84062 [51] validation_0-auc:0.87605 validation_1-auc:0.84105 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80843 validation_1-auc:0.80885 [1] validation_0-auc:0.82920 validation_1-auc:0.82211 [2] validation_0-auc:0.83320 validation_1-auc:0.82400 [3] validation_0-auc:0.83625 validation_1-auc:0.82577 [4] validation_0-auc:0.84188 validation_1-auc:0.82897 [5] validation_0-auc:0.84455 validation_1-auc:0.83377 [6] validation_0-auc:0.84503 validation_1-auc:0.82916 [7] validation_0-auc:0.85319 validation_1-auc:0.83364 [8] validation_0-auc:0.85976 validation_1-auc:0.83390 [9] validation_0-auc:0.85952 validation_1-auc:0.82834 [10] validation_0-auc:0.85919 validation_1-auc:0.82378 [11] validation_0-auc:0.85956 validation_1-auc:0.82400 [12] validation_0-auc:0.86574 validation_1-auc:0.82888 [13] validation_0-auc:0.87027 validation_1-auc:0.83251 [14] validation_0-auc:0.87240 validation_1-auc:0.83311 [15] validation_0-auc:0.87365 validation_1-auc:0.83080 [16] validation_0-auc:0.87567 validation_1-auc:0.83134 [17] validation_0-auc:0.87777 validation_1-auc:0.83255 [18] validation_0-auc:0.87904 validation_1-auc:0.83149 [19] validation_0-auc:0.88037 validation_1-auc:0.83083 [20] validation_0-auc:0.88104 validation_1-auc:0.82964 [21] validation_0-auc:0.88159 validation_1-auc:0.82802 [22] validation_0-auc:0.88227 validation_1-auc:0.82806 [23] validation_0-auc:0.88255 validation_1-auc:0.82806 [24] validation_0-auc:0.88328 validation_1-auc:0.82840 [25] validation_0-auc:0.88353 validation_1-auc:0.82851 [26] validation_0-auc:0.88384 validation_1-auc:0.82899 [27] validation_0-auc:0.88509 validation_1-auc:0.82988 [28] validation_0-auc:0.88544 validation_1-auc:0.82886 [29] validation_0-auc:0.88569 validation_1-auc:0.82922 [30] validation_0-auc:0.88588 validation_1-auc:0.82962 [31] validation_0-auc:0.88682 validation_1-auc:0.82951 [32] validation_0-auc:0.88752 validation_1-auc:0.82858 [33] validation_0-auc:0.88762 validation_1-auc:0.82843 [34] validation_0-auc:0.88792 validation_1-auc:0.82804 [35] validation_0-auc:0.88865 validation_1-auc:0.82692 [36] validation_0-auc:0.88868 validation_1-auc:0.82609 [37] validation_0-auc:0.88901 validation_1-auc:0.82607 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81304 validation_1-auc:0.81746 [1] validation_0-auc:0.82882 validation_1-auc:0.82026 [2] validation_0-auc:0.83609 validation_1-auc:0.82474 [3] validation_0-auc:0.84041 validation_1-auc:0.82824 [4] validation_0-auc:0.84760 validation_1-auc:0.83130 [5] validation_0-auc:0.84938 validation_1-auc:0.83590 [6] validation_0-auc:0.85116 validation_1-auc:0.83167 [7] validation_0-auc:0.85828 validation_1-auc:0.83471 [8] validation_0-auc:0.86371 validation_1-auc:0.83640 [9] validation_0-auc:0.86365 validation_1-auc:0.83549 [10] validation_0-auc:0.86395 validation_1-auc:0.83127 [11] validation_0-auc:0.86437 validation_1-auc:0.82983 [12] validation_0-auc:0.87068 validation_1-auc:0.83421 [13] validation_0-auc:0.87545 validation_1-auc:0.83773 [14] validation_0-auc:0.87779 validation_1-auc:0.83843 [15] validation_0-auc:0.87893 validation_1-auc:0.83628 [16] validation_0-auc:0.88035 validation_1-auc:0.83878 [17] validation_0-auc:0.88227 validation_1-auc:0.83749 [18] validation_0-auc:0.88364 validation_1-auc:0.83710 [19] validation_0-auc:0.88528 validation_1-auc:0.83727 [20] validation_0-auc:0.88606 validation_1-auc:0.83670 [21] validation_0-auc:0.88672 validation_1-auc:0.83629 [22] validation_0-auc:0.88793 validation_1-auc:0.83586 [23] validation_0-auc:0.88875 validation_1-auc:0.83562 [24] validation_0-auc:0.88913 validation_1-auc:0.83589 [25] validation_0-auc:0.88932 validation_1-auc:0.83575 [26] validation_0-auc:0.89053 validation_1-auc:0.83424 [27] validation_0-auc:0.89116 validation_1-auc:0.83427 [28] validation_0-auc:0.89172 validation_1-auc:0.83384 [29] validation_0-auc:0.89244 validation_1-auc:0.83318 [30] validation_0-auc:0.89260 validation_1-auc:0.83224 [31] validation_0-auc:0.89294 validation_1-auc:0.83214 [32] validation_0-auc:0.89361 validation_1-auc:0.83111 [33] validation_0-auc:0.89396 validation_1-auc:0.83114 [34] validation_0-auc:0.89481 validation_1-auc:0.83121 [35] validation_0-auc:0.89548 validation_1-auc:0.83133 [36] validation_0-auc:0.89589 validation_1-auc:0.83039 [37] validation_0-auc:0.89614 validation_1-auc:0.83024 [38] validation_0-auc:0.89743 validation_1-auc:0.82952 [39] validation_0-auc:0.89749 validation_1-auc:0.82950 [40] validation_0-auc:0.89754 validation_1-auc:0.82932 [41] validation_0-auc:0.89813 validation_1-auc:0.82838 [42] validation_0-auc:0.89831 validation_1-auc:0.82849 [43] validation_0-auc:0.89841 validation_1-auc:0.82827 [44] validation_0-auc:0.89908 validation_1-auc:0.82824 [45] validation_0-auc:0.89919 validation_1-auc:0.82788 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81393 validation_1-auc:0.81377 [1] validation_0-auc:0.82962 validation_1-auc:0.82668 [2] validation_0-auc:0.83724 validation_1-auc:0.83017 [3] validation_0-auc:0.84075 validation_1-auc:0.83079 [4] validation_0-auc:0.84691 validation_1-auc:0.83337 [5] validation_0-auc:0.84896 validation_1-auc:0.83502 [6] validation_0-auc:0.84980 validation_1-auc:0.82858 [7] validation_0-auc:0.85918 validation_1-auc:0.83358 [8] validation_0-auc:0.86284 validation_1-auc:0.83470 [9] validation_0-auc:0.86365 validation_1-auc:0.83427 [10] validation_0-auc:0.86243 validation_1-auc:0.83264 [11] validation_0-auc:0.86248 validation_1-auc:0.83255 [12] validation_0-auc:0.86969 validation_1-auc:0.83531 [13] validation_0-auc:0.87452 validation_1-auc:0.83774 [14] validation_0-auc:0.87630 validation_1-auc:0.83936 [15] validation_0-auc:0.87826 validation_1-auc:0.83676 [16] validation_0-auc:0.87988 validation_1-auc:0.83852 [17] validation_0-auc:0.88289 validation_1-auc:0.83811 [18] validation_0-auc:0.88333 validation_1-auc:0.83735 [19] validation_0-auc:0.88506 validation_1-auc:0.83720 [20] validation_0-auc:0.88528 validation_1-auc:0.83718 [21] validation_0-auc:0.88547 validation_1-auc:0.83646 [22] validation_0-auc:0.88632 validation_1-auc:0.83706 [23] validation_0-auc:0.88770 validation_1-auc:0.83714 [24] validation_0-auc:0.88867 validation_1-auc:0.83742 [25] validation_0-auc:0.88905 validation_1-auc:0.83753 [26] validation_0-auc:0.89065 validation_1-auc:0.83634 [27] validation_0-auc:0.89158 validation_1-auc:0.83565 [28] validation_0-auc:0.89214 validation_1-auc:0.83460 [29] validation_0-auc:0.89345 validation_1-auc:0.83413 [30] validation_0-auc:0.89377 validation_1-auc:0.83373 [31] validation_0-auc:0.89392 validation_1-auc:0.83396 [32] validation_0-auc:0.89410 validation_1-auc:0.83435 [33] validation_0-auc:0.89416 validation_1-auc:0.83412 [34] validation_0-auc:0.89437 validation_1-auc:0.83386 [35] validation_0-auc:0.89513 validation_1-auc:0.83338 [36] validation_0-auc:0.89553 validation_1-auc:0.83232 [37] validation_0-auc:0.89589 validation_1-auc:0.83223 [38] validation_0-auc:0.89609 validation_1-auc:0.83222 [39] validation_0-auc:0.89636 validation_1-auc:0.83187 [40] validation_0-auc:0.89652 validation_1-auc:0.83146 [41] validation_0-auc:0.89655 validation_1-auc:0.83131 [42] validation_0-auc:0.89789 validation_1-auc:0.83068 [43] validation_0-auc:0.89792 validation_1-auc:0.83069 [44] validation_0-auc:0.89889 validation_1-auc:0.83038 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80901 validation_1-auc:0.80653 [1] validation_0-auc:0.82713 validation_1-auc:0.82150 [2] validation_0-auc:0.83227 validation_1-auc:0.82513 [3] validation_0-auc:0.83319 validation_1-auc:0.82525 [4] validation_0-auc:0.83786 validation_1-auc:0.82805 [5] validation_0-auc:0.84104 validation_1-auc:0.82979 [6] validation_0-auc:0.84432 validation_1-auc:0.82639 [7] validation_0-auc:0.85301 validation_1-auc:0.83411 [8] validation_0-auc:0.85882 validation_1-auc:0.83754 [9] validation_0-auc:0.85838 validation_1-auc:0.83437 [10] validation_0-auc:0.85606 validation_1-auc:0.83252 [11] validation_0-auc:0.85677 validation_1-auc:0.83031 [12] validation_0-auc:0.86256 validation_1-auc:0.83311 [13] validation_0-auc:0.86712 validation_1-auc:0.83500 [14] validation_0-auc:0.86926 validation_1-auc:0.83593 [15] validation_0-auc:0.87031 validation_1-auc:0.83404 [16] validation_0-auc:0.87119 validation_1-auc:0.83472 [17] validation_0-auc:0.87276 validation_1-auc:0.83454 [18] validation_0-auc:0.87365 validation_1-auc:0.83418 [19] validation_0-auc:0.87495 validation_1-auc:0.83324 [20] validation_0-auc:0.87498 validation_1-auc:0.83267 [21] validation_0-auc:0.87527 validation_1-auc:0.83259 [22] validation_0-auc:0.87572 validation_1-auc:0.83274 [23] validation_0-auc:0.87659 validation_1-auc:0.83362 [24] validation_0-auc:0.87704 validation_1-auc:0.83315 [25] validation_0-auc:0.87743 validation_1-auc:0.83338 [26] validation_0-auc:0.87762 validation_1-auc:0.83358 [27] validation_0-auc:0.87818 validation_1-auc:0.83337 [28] validation_0-auc:0.87822 validation_1-auc:0.83346 [29] validation_0-auc:0.87890 validation_1-auc:0.83331 [30] validation_0-auc:0.87903 validation_1-auc:0.83315 [31] validation_0-auc:0.87993 validation_1-auc:0.83277 [32] validation_0-auc:0.88063 validation_1-auc:0.83284 [33] validation_0-auc:0.88096 validation_1-auc:0.83339 [34] validation_0-auc:0.88210 validation_1-auc:0.83309 [35] validation_0-auc:0.88207 validation_1-auc:0.83317 [36] validation_0-auc:0.88224 validation_1-auc:0.83314 [37] validation_0-auc:0.88240 validation_1-auc:0.83292 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81176 validation_1-auc:0.80947 [1] validation_0-auc:0.82651 validation_1-auc:0.82286 [2] validation_0-auc:0.83551 validation_1-auc:0.82712 [3] validation_0-auc:0.83820 validation_1-auc:0.82810 [4] validation_0-auc:0.84733 validation_1-auc:0.82952 [5] validation_0-auc:0.84903 validation_1-auc:0.83409 [6] validation_0-auc:0.84836 validation_1-auc:0.83191 [7] validation_0-auc:0.85387 validation_1-auc:0.83486 [8] validation_0-auc:0.85876 validation_1-auc:0.83709 [9] validation_0-auc:0.85840 validation_1-auc:0.83730 [10] validation_0-auc:0.85787 validation_1-auc:0.83417 [11] validation_0-auc:0.85814 validation_1-auc:0.83328 [12] validation_0-auc:0.86431 validation_1-auc:0.83684 [13] validation_0-auc:0.86878 validation_1-auc:0.83901 [14] validation_0-auc:0.87119 validation_1-auc:0.83987 [15] validation_0-auc:0.87268 validation_1-auc:0.83789 [16] validation_0-auc:0.87455 validation_1-auc:0.83903 [17] validation_0-auc:0.87645 validation_1-auc:0.83873 [18] validation_0-auc:0.87724 validation_1-auc:0.83908 [19] validation_0-auc:0.87799 validation_1-auc:0.83966 [20] validation_0-auc:0.87882 validation_1-auc:0.83958 [21] validation_0-auc:0.87902 validation_1-auc:0.83960 [22] validation_0-auc:0.87951 validation_1-auc:0.83985 [23] validation_0-auc:0.88042 validation_1-auc:0.83903 [24] validation_0-auc:0.88118 validation_1-auc:0.83938 [25] validation_0-auc:0.88183 validation_1-auc:0.83941 [26] validation_0-auc:0.88279 validation_1-auc:0.83943 [27] validation_0-auc:0.88430 validation_1-auc:0.83947 [28] validation_0-auc:0.88447 validation_1-auc:0.83972 [29] validation_0-auc:0.88487 validation_1-auc:0.83903 [30] validation_0-auc:0.88567 validation_1-auc:0.83956 [31] validation_0-auc:0.88560 validation_1-auc:0.83942 [32] validation_0-auc:0.88572 validation_1-auc:0.83903 [33] validation_0-auc:0.88598 validation_1-auc:0.83902 [34] validation_0-auc:0.88633 validation_1-auc:0.83882 [35] validation_0-auc:0.88642 validation_1-auc:0.83890 [36] validation_0-auc:0.88707 validation_1-auc:0.83877 [37] validation_0-auc:0.88742 validation_1-auc:0.83862 [38] validation_0-auc:0.88755 validation_1-auc:0.83835 [39] validation_0-auc:0.88788 validation_1-auc:0.83760 [40] validation_0-auc:0.88777 validation_1-auc:0.83781 [41] validation_0-auc:0.88796 validation_1-auc:0.83789 [42] validation_0-auc:0.88804 validation_1-auc:0.83796 [43] validation_0-auc:0.88868 validation_1-auc:0.83769 [44] validation_0-auc:0.88942 validation_1-auc:0.83764 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81519 validation_1-auc:0.81115 [1] validation_0-auc:0.83201 validation_1-auc:0.82366 [2] validation_0-auc:0.83718 validation_1-auc:0.83029 [3] validation_0-auc:0.84145 validation_1-auc:0.83163 [4] validation_0-auc:0.84628 validation_1-auc:0.83410 [5] validation_0-auc:0.84792 validation_1-auc:0.83694 [6] validation_0-auc:0.84780 validation_1-auc:0.83116 [7] validation_0-auc:0.85599 validation_1-auc:0.83759 [8] validation_0-auc:0.85905 validation_1-auc:0.83700 [9] validation_0-auc:0.85860 validation_1-auc:0.83638 [10] validation_0-auc:0.85875 validation_1-auc:0.83594 [11] validation_0-auc:0.85921 validation_1-auc:0.83691 [12] validation_0-auc:0.86560 validation_1-auc:0.84075 [13] validation_0-auc:0.86941 validation_1-auc:0.84350 [14] validation_0-auc:0.87102 validation_1-auc:0.84520 [15] validation_0-auc:0.87174 validation_1-auc:0.84423 [16] validation_0-auc:0.87350 validation_1-auc:0.84460 [17] validation_0-auc:0.87528 validation_1-auc:0.84395 [18] validation_0-auc:0.87593 validation_1-auc:0.84331 [19] validation_0-auc:0.87733 validation_1-auc:0.84275 [20] validation_0-auc:0.87769 validation_1-auc:0.84252 [21] validation_0-auc:0.87822 validation_1-auc:0.84160 [22] validation_0-auc:0.87989 validation_1-auc:0.84207 [23] validation_0-auc:0.88086 validation_1-auc:0.84223 [24] validation_0-auc:0.88139 validation_1-auc:0.84238 [25] validation_0-auc:0.88186 validation_1-auc:0.84258 [26] validation_0-auc:0.88258 validation_1-auc:0.84240 [27] validation_0-auc:0.88359 validation_1-auc:0.84183 [28] validation_0-auc:0.88402 validation_1-auc:0.84147 [29] validation_0-auc:0.88415 validation_1-auc:0.84140 [30] validation_0-auc:0.88455 validation_1-auc:0.84080 [31] validation_0-auc:0.88538 validation_1-auc:0.84070 [32] validation_0-auc:0.88563 validation_1-auc:0.84055 [33] validation_0-auc:0.88610 validation_1-auc:0.84024 [34] validation_0-auc:0.88631 validation_1-auc:0.83977 [35] validation_0-auc:0.88637 validation_1-auc:0.83959 [36] validation_0-auc:0.88644 validation_1-auc:0.83935 [37] validation_0-auc:0.88728 validation_1-auc:0.83898 [38] validation_0-auc:0.88802 validation_1-auc:0.83814 [39] validation_0-auc:0.88815 validation_1-auc:0.83806 [40] validation_0-auc:0.88815 validation_1-auc:0.83811 [41] validation_0-auc:0.88838 validation_1-auc:0.83807 [42] validation_0-auc:0.88883 validation_1-auc:0.83753 [43] validation_0-auc:0.88902 validation_1-auc:0.83781 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81007 validation_1-auc:0.80693 [1] validation_0-auc:0.82137 validation_1-auc:0.81877 [2] validation_0-auc:0.82976 validation_1-auc:0.82498 [3] validation_0-auc:0.83120 validation_1-auc:0.82212 [4] validation_0-auc:0.83382 validation_1-auc:0.82481 [5] validation_0-auc:0.83696 validation_1-auc:0.82672 [6] validation_0-auc:0.83976 validation_1-auc:0.83016 [7] validation_0-auc:0.84177 validation_1-auc:0.83330 [8] validation_0-auc:0.84585 validation_1-auc:0.83282 [9] validation_0-auc:0.84984 validation_1-auc:0.83519 [10] validation_0-auc:0.85146 validation_1-auc:0.83530 [11] validation_0-auc:0.85113 validation_1-auc:0.83380 [12] validation_0-auc:0.85502 validation_1-auc:0.83622 [13] validation_0-auc:0.85797 validation_1-auc:0.83644 [14] validation_0-auc:0.85990 validation_1-auc:0.83686 [15] validation_0-auc:0.86114 validation_1-auc:0.83639 [16] validation_0-auc:0.86158 validation_1-auc:0.83602 [17] validation_0-auc:0.86285 validation_1-auc:0.83501 [18] validation_0-auc:0.86405 validation_1-auc:0.83454 [19] validation_0-auc:0.86498 validation_1-auc:0.83497 [20] validation_0-auc:0.86595 validation_1-auc:0.83417 [21] validation_0-auc:0.86757 validation_1-auc:0.83454 [22] validation_0-auc:0.86810 validation_1-auc:0.83466 [23] validation_0-auc:0.86830 validation_1-auc:0.83461 [24] validation_0-auc:0.86859 validation_1-auc:0.83422 [25] validation_0-auc:0.86941 validation_1-auc:0.83371 [26] validation_0-auc:0.86986 validation_1-auc:0.83392 [27] validation_0-auc:0.87053 validation_1-auc:0.83330 [28] validation_0-auc:0.87105 validation_1-auc:0.83367 [29] validation_0-auc:0.87111 validation_1-auc:0.83371 [30] validation_0-auc:0.87152 validation_1-auc:0.83435 [31] validation_0-auc:0.87181 validation_1-auc:0.83437 [32] validation_0-auc:0.87286 validation_1-auc:0.83459 [33] validation_0-auc:0.87304 validation_1-auc:0.83470 [34] validation_0-auc:0.87347 validation_1-auc:0.83407 [35] validation_0-auc:0.87393 validation_1-auc:0.83319 [36] validation_0-auc:0.87464 validation_1-auc:0.83300 [37] validation_0-auc:0.87469 validation_1-auc:0.83311 [38] validation_0-auc:0.87502 validation_1-auc:0.83281 [39] validation_0-auc:0.87594 validation_1-auc:0.83273 [40] validation_0-auc:0.87620 validation_1-auc:0.83299 [41] validation_0-auc:0.87747 validation_1-auc:0.83274 [42] validation_0-auc:0.87754 validation_1-auc:0.83254 [43] validation_0-auc:0.87846 validation_1-auc:0.83286 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80863 validation_1-auc:0.80010 [1] validation_0-auc:0.82349 validation_1-auc:0.81717 [2] validation_0-auc:0.82654 validation_1-auc:0.81737 [3] validation_0-auc:0.82988 validation_1-auc:0.82281 [4] validation_0-auc:0.83570 validation_1-auc:0.82554 [5] validation_0-auc:0.83917 validation_1-auc:0.82930 [6] validation_0-auc:0.84492 validation_1-auc:0.83396 [7] validation_0-auc:0.84657 validation_1-auc:0.83569 [8] validation_0-auc:0.84837 validation_1-auc:0.83476 [9] validation_0-auc:0.85010 validation_1-auc:0.83841 [10] validation_0-auc:0.85017 validation_1-auc:0.83887 [11] validation_0-auc:0.85091 validation_1-auc:0.83723 [12] validation_0-auc:0.85584 validation_1-auc:0.83976 [13] validation_0-auc:0.85900 validation_1-auc:0.84063 [14] validation_0-auc:0.86059 validation_1-auc:0.84054 [15] validation_0-auc:0.86167 validation_1-auc:0.84086 [16] validation_0-auc:0.86303 validation_1-auc:0.84085 [17] validation_0-auc:0.86383 validation_1-auc:0.83947 [18] validation_0-auc:0.86462 validation_1-auc:0.83971 [19] validation_0-auc:0.86559 validation_1-auc:0.84059 [20] validation_0-auc:0.86650 validation_1-auc:0.83981 [21] validation_0-auc:0.86762 validation_1-auc:0.84030 [22] validation_0-auc:0.86865 validation_1-auc:0.84050 [23] validation_0-auc:0.86916 validation_1-auc:0.83978 [24] validation_0-auc:0.86953 validation_1-auc:0.84033 [25] validation_0-auc:0.86992 validation_1-auc:0.84000 [26] validation_0-auc:0.87005 validation_1-auc:0.83998 [27] validation_0-auc:0.87115 validation_1-auc:0.83964 [28] validation_0-auc:0.87205 validation_1-auc:0.83972 [29] validation_0-auc:0.87328 validation_1-auc:0.83984 [30] validation_0-auc:0.87360 validation_1-auc:0.83929 [31] validation_0-auc:0.87367 validation_1-auc:0.83938 [32] validation_0-auc:0.87441 validation_1-auc:0.83918 [33] validation_0-auc:0.87490 validation_1-auc:0.83990 [34] validation_0-auc:0.87594 validation_1-auc:0.84011 [35] validation_0-auc:0.87618 validation_1-auc:0.83988 [36] validation_0-auc:0.87648 validation_1-auc:0.83991 [37] validation_0-auc:0.87657 validation_1-auc:0.83991 [38] validation_0-auc:0.87676 validation_1-auc:0.83987 [39] validation_0-auc:0.87696 validation_1-auc:0.83973 [40] validation_0-auc:0.87705 validation_1-auc:0.83990 [41] validation_0-auc:0.87724 validation_1-auc:0.83941 [42] validation_0-auc:0.87781 validation_1-auc:0.83934 [43] validation_0-auc:0.87810 validation_1-auc:0.83924 [44] validation_0-auc:0.87848 validation_1-auc:0.83882 [45] validation_0-auc:0.87863 validation_1-auc:0.83888 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.82005 validation_1-auc:0.81815 [1] validation_0-auc:0.82547 validation_1-auc:0.82159 [2] validation_0-auc:0.83019 validation_1-auc:0.82631 [3] validation_0-auc:0.83230 validation_1-auc:0.82660 [4] validation_0-auc:0.83488 validation_1-auc:0.82988 [5] validation_0-auc:0.83888 validation_1-auc:0.83262 [6] validation_0-auc:0.84242 validation_1-auc:0.83408 [7] validation_0-auc:0.84581 validation_1-auc:0.83560 [8] validation_0-auc:0.84775 validation_1-auc:0.83617 [9] validation_0-auc:0.84989 validation_1-auc:0.83746 [10] validation_0-auc:0.85052 validation_1-auc:0.83816 [11] validation_0-auc:0.84982 validation_1-auc:0.83603 [12] validation_0-auc:0.85408 validation_1-auc:0.83825 [13] validation_0-auc:0.85547 validation_1-auc:0.83955 [14] validation_0-auc:0.85818 validation_1-auc:0.84292 [15] validation_0-auc:0.85990 validation_1-auc:0.84361 [16] validation_0-auc:0.86142 validation_1-auc:0.84287 [17] validation_0-auc:0.86247 validation_1-auc:0.84280 [18] validation_0-auc:0.86276 validation_1-auc:0.84297 [19] validation_0-auc:0.86368 validation_1-auc:0.84290 [20] validation_0-auc:0.86488 validation_1-auc:0.84279 [21] validation_0-auc:0.86540 validation_1-auc:0.84307 [22] validation_0-auc:0.86631 validation_1-auc:0.84285 [23] validation_0-auc:0.86687 validation_1-auc:0.84289 [24] validation_0-auc:0.86777 validation_1-auc:0.84289 [25] validation_0-auc:0.86830 validation_1-auc:0.84279 [26] validation_0-auc:0.86862 validation_1-auc:0.84237 [27] validation_0-auc:0.87011 validation_1-auc:0.84232 [28] validation_0-auc:0.87063 validation_1-auc:0.84224 [29] validation_0-auc:0.87063 validation_1-auc:0.84199 [30] validation_0-auc:0.87108 validation_1-auc:0.84246 [31] validation_0-auc:0.87190 validation_1-auc:0.84252 [32] validation_0-auc:0.87275 validation_1-auc:0.84147 [33] validation_0-auc:0.87302 validation_1-auc:0.84149 [34] validation_0-auc:0.87350 validation_1-auc:0.84118 [35] validation_0-auc:0.87371 validation_1-auc:0.84115 [36] validation_0-auc:0.87407 validation_1-auc:0.84113 [37] validation_0-auc:0.87475 validation_1-auc:0.84038 [38] validation_0-auc:0.87529 validation_1-auc:0.84009 [39] validation_0-auc:0.87540 validation_1-auc:0.83988 [40] validation_0-auc:0.87555 validation_1-auc:0.83984 [41] validation_0-auc:0.87579 validation_1-auc:0.83991 [42] validation_0-auc:0.87630 validation_1-auc:0.83942 [43] validation_0-auc:0.87664 validation_1-auc:0.83926 [44] validation_0-auc:0.87713 validation_1-auc:0.83916 [45] validation_0-auc:0.87763 validation_1-auc:0.83868 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81105 validation_1-auc:0.80637 [1] validation_0-auc:0.82008 validation_1-auc:0.81881 [2] validation_0-auc:0.82922 validation_1-auc:0.82532 [3] validation_0-auc:0.83159 validation_1-auc:0.82594 [4] validation_0-auc:0.83378 validation_1-auc:0.82618 [5] validation_0-auc:0.83671 validation_1-auc:0.82887 [6] validation_0-auc:0.84111 validation_1-auc:0.83302 [7] validation_0-auc:0.84227 validation_1-auc:0.83380 [8] validation_0-auc:0.84422 validation_1-auc:0.83346 [9] validation_0-auc:0.84742 validation_1-auc:0.83581 [10] validation_0-auc:0.84984 validation_1-auc:0.83563 [11] validation_0-auc:0.84933 validation_1-auc:0.83344 [12] validation_0-auc:0.85285 validation_1-auc:0.83653 [13] validation_0-auc:0.85494 validation_1-auc:0.83796 [14] validation_0-auc:0.85653 validation_1-auc:0.83880 [15] validation_0-auc:0.85803 validation_1-auc:0.83841 [16] validation_0-auc:0.85922 validation_1-auc:0.83773 [17] validation_0-auc:0.85983 validation_1-auc:0.83709 [18] validation_0-auc:0.86162 validation_1-auc:0.83622 [19] validation_0-auc:0.86232 validation_1-auc:0.83513 [20] validation_0-auc:0.86287 validation_1-auc:0.83518 [21] validation_0-auc:0.86374 validation_1-auc:0.83543 [22] validation_0-auc:0.86416 validation_1-auc:0.83540 [23] validation_0-auc:0.86459 validation_1-auc:0.83510 [24] validation_0-auc:0.86482 validation_1-auc:0.83477 [25] validation_0-auc:0.86526 validation_1-auc:0.83484 [26] validation_0-auc:0.86545 validation_1-auc:0.83473 [27] validation_0-auc:0.86568 validation_1-auc:0.83481 [28] validation_0-auc:0.86578 validation_1-auc:0.83485 [29] validation_0-auc:0.86654 validation_1-auc:0.83501 [30] validation_0-auc:0.86666 validation_1-auc:0.83465 [31] validation_0-auc:0.86790 validation_1-auc:0.83486 [32] validation_0-auc:0.86802 validation_1-auc:0.83488 [33] validation_0-auc:0.86809 validation_1-auc:0.83473 [34] validation_0-auc:0.86821 validation_1-auc:0.83483 [35] validation_0-auc:0.86828 validation_1-auc:0.83508 [36] validation_0-auc:0.86861 validation_1-auc:0.83435 [37] validation_0-auc:0.86866 validation_1-auc:0.83425 [38] validation_0-auc:0.86892 validation_1-auc:0.83451 [39] validation_0-auc:0.86913 validation_1-auc:0.83425 [40] validation_0-auc:0.86939 validation_1-auc:0.83430 [41] validation_0-auc:0.86940 validation_1-auc:0.83443 [42] validation_0-auc:0.86949 validation_1-auc:0.83436 [43] validation_0-auc:0.87013 validation_1-auc:0.83441 [44] validation_0-auc:0.87059 validation_1-auc:0.83365 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81067 validation_1-auc:0.81109 [1] validation_0-auc:0.82045 validation_1-auc:0.81627 [2] validation_0-auc:0.82760 validation_1-auc:0.82116 [3] validation_0-auc:0.82925 validation_1-auc:0.81730 [4] validation_0-auc:0.83628 validation_1-auc:0.82554 [5] validation_0-auc:0.83889 validation_1-auc:0.82992 [6] validation_0-auc:0.84258 validation_1-auc:0.83304 [7] validation_0-auc:0.84515 validation_1-auc:0.83327 [8] validation_0-auc:0.84797 validation_1-auc:0.83479 [9] validation_0-auc:0.84982 validation_1-auc:0.83737 [10] validation_0-auc:0.84996 validation_1-auc:0.83746 [11] validation_0-auc:0.84929 validation_1-auc:0.83715 [12] validation_0-auc:0.85506 validation_1-auc:0.83957 [13] validation_0-auc:0.85817 validation_1-auc:0.84131 [14] validation_0-auc:0.85945 validation_1-auc:0.84041 [15] validation_0-auc:0.86040 validation_1-auc:0.83984 [16] validation_0-auc:0.86127 validation_1-auc:0.83954 [17] validation_0-auc:0.86170 validation_1-auc:0.83947 [18] validation_0-auc:0.86276 validation_1-auc:0.83945 [19] validation_0-auc:0.86327 validation_1-auc:0.84019 [20] validation_0-auc:0.86381 validation_1-auc:0.84075 [21] validation_0-auc:0.86454 validation_1-auc:0.84078 [22] validation_0-auc:0.86530 validation_1-auc:0.84164 [23] validation_0-auc:0.86598 validation_1-auc:0.84128 [24] validation_0-auc:0.86656 validation_1-auc:0.84078 [25] validation_0-auc:0.86721 validation_1-auc:0.84069 [26] validation_0-auc:0.86745 validation_1-auc:0.84066 [27] validation_0-auc:0.86808 validation_1-auc:0.84017 [28] validation_0-auc:0.86914 validation_1-auc:0.84027 [29] validation_0-auc:0.86951 validation_1-auc:0.84014 [30] validation_0-auc:0.86972 validation_1-auc:0.84016 [31] validation_0-auc:0.86996 validation_1-auc:0.83992 [32] validation_0-auc:0.87072 validation_1-auc:0.84001 [33] validation_0-auc:0.87090 validation_1-auc:0.83997 [34] validation_0-auc:0.87111 validation_1-auc:0.83969 [35] validation_0-auc:0.87145 validation_1-auc:0.83964 [36] validation_0-auc:0.87215 validation_1-auc:0.84006 [37] validation_0-auc:0.87242 validation_1-auc:0.83987 [38] validation_0-auc:0.87262 validation_1-auc:0.83995 [39] validation_0-auc:0.87270 validation_1-auc:0.84021 [40] validation_0-auc:0.87275 validation_1-auc:0.84066 [41] validation_0-auc:0.87323 validation_1-auc:0.84095 [42] validation_0-auc:0.87372 validation_1-auc:0.84074 [43] validation_0-auc:0.87433 validation_1-auc:0.84057 [44] validation_0-auc:0.87440 validation_1-auc:0.84028 [45] validation_0-auc:0.87511 validation_1-auc:0.84011 [46] validation_0-auc:0.87553 validation_1-auc:0.83972 [47] validation_0-auc:0.87606 validation_1-auc:0.83880 [48] validation_0-auc:0.87630 validation_1-auc:0.83876 [49] validation_0-auc:0.87629 validation_1-auc:0.83900 [50] validation_0-auc:0.87637 validation_1-auc:0.83902 [51] validation_0-auc:0.87649 validation_1-auc:0.83930 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81835 validation_1-auc:0.81691 [1] validation_0-auc:0.82862 validation_1-auc:0.82346 [2] validation_0-auc:0.83280 validation_1-auc:0.82893 [3] validation_0-auc:0.83563 validation_1-auc:0.82931 [4] validation_0-auc:0.83780 validation_1-auc:0.83200 [5] validation_0-auc:0.83975 validation_1-auc:0.83280 [6] validation_0-auc:0.84205 validation_1-auc:0.83374 [7] validation_0-auc:0.84453 validation_1-auc:0.83256 [8] validation_0-auc:0.84638 validation_1-auc:0.83384 [9] validation_0-auc:0.84986 validation_1-auc:0.83670 [10] validation_0-auc:0.85058 validation_1-auc:0.83825 [11] validation_0-auc:0.84986 validation_1-auc:0.83646 [12] validation_0-auc:0.85321 validation_1-auc:0.83744 [13] validation_0-auc:0.85478 validation_1-auc:0.83942 [14] validation_0-auc:0.85613 validation_1-auc:0.84091 [15] validation_0-auc:0.85709 validation_1-auc:0.84170 [16] validation_0-auc:0.85891 validation_1-auc:0.84239 [17] validation_0-auc:0.86023 validation_1-auc:0.84215 [18] validation_0-auc:0.86146 validation_1-auc:0.84247 [19] validation_0-auc:0.86202 validation_1-auc:0.84237 [20] validation_0-auc:0.86268 validation_1-auc:0.84152 [21] validation_0-auc:0.86342 validation_1-auc:0.84132 [22] validation_0-auc:0.86492 validation_1-auc:0.84044 [23] validation_0-auc:0.86602 validation_1-auc:0.84073 [24] validation_0-auc:0.86688 validation_1-auc:0.84082 [25] validation_0-auc:0.86779 validation_1-auc:0.84074 [26] validation_0-auc:0.86849 validation_1-auc:0.84076 [27] validation_0-auc:0.86910 validation_1-auc:0.84096 [28] validation_0-auc:0.86931 validation_1-auc:0.84113 [29] validation_0-auc:0.86974 validation_1-auc:0.84187 [30] validation_0-auc:0.87070 validation_1-auc:0.84167 [31] validation_0-auc:0.87108 validation_1-auc:0.84174 [32] validation_0-auc:0.87123 validation_1-auc:0.84166 [33] validation_0-auc:0.87153 validation_1-auc:0.84142 [34] validation_0-auc:0.87214 validation_1-auc:0.84153 [35] validation_0-auc:0.87289 validation_1-auc:0.84147 [36] validation_0-auc:0.87329 validation_1-auc:0.84136 [37] validation_0-auc:0.87345 validation_1-auc:0.84116 [38] validation_0-auc:0.87355 validation_1-auc:0.84114 [39] validation_0-auc:0.87411 validation_1-auc:0.84087 [40] validation_0-auc:0.87419 validation_1-auc:0.84088 [41] validation_0-auc:0.87540 validation_1-auc:0.84065 [42] validation_0-auc:0.87576 validation_1-auc:0.84078 [43] validation_0-auc:0.87598 validation_1-auc:0.84097 [44] validation_0-auc:0.87646 validation_1-auc:0.84047 [45] validation_0-auc:0.87666 validation_1-auc:0.84048 [46] validation_0-auc:0.87670 validation_1-auc:0.84016 [47] validation_0-auc:0.87719 validation_1-auc:0.84000 [48] validation_0-auc:0.87796 validation_1-auc:0.83922 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81685 validation_1-auc:0.81075 [1] validation_0-auc:0.82791 validation_1-auc:0.82283 [2] validation_0-auc:0.83537 validation_1-auc:0.82615 [3] validation_0-auc:0.83996 validation_1-auc:0.82712 [4] validation_0-auc:0.84558 validation_1-auc:0.82791 [5] validation_0-auc:0.84781 validation_1-auc:0.82977 [6] validation_0-auc:0.85151 validation_1-auc:0.83373 [7] validation_0-auc:0.85510 validation_1-auc:0.83444 [8] validation_0-auc:0.85998 validation_1-auc:0.83601 [9] validation_0-auc:0.86238 validation_1-auc:0.83804 [10] validation_0-auc:0.86435 validation_1-auc:0.83584 [11] validation_0-auc:0.86583 validation_1-auc:0.83093 [12] validation_0-auc:0.87079 validation_1-auc:0.83235 [13] validation_0-auc:0.87454 validation_1-auc:0.83253 [14] validation_0-auc:0.87642 validation_1-auc:0.83254 [15] validation_0-auc:0.87856 validation_1-auc:0.83218 [16] validation_0-auc:0.87973 validation_1-auc:0.83171 [17] validation_0-auc:0.88122 validation_1-auc:0.83115 [18] validation_0-auc:0.88256 validation_1-auc:0.83119 [19] validation_0-auc:0.88330 validation_1-auc:0.83139 [20] validation_0-auc:0.88408 validation_1-auc:0.83082 [21] validation_0-auc:0.88505 validation_1-auc:0.83044 [22] validation_0-auc:0.88631 validation_1-auc:0.83025 [23] validation_0-auc:0.88670 validation_1-auc:0.83047 [24] validation_0-auc:0.88740 validation_1-auc:0.82903 [25] validation_0-auc:0.88770 validation_1-auc:0.82895 [26] validation_0-auc:0.88793 validation_1-auc:0.82913 [27] validation_0-auc:0.88808 validation_1-auc:0.82881 [28] validation_0-auc:0.88830 validation_1-auc:0.82901 [29] validation_0-auc:0.88834 validation_1-auc:0.82910 [30] validation_0-auc:0.88894 validation_1-auc:0.82854 [31] validation_0-auc:0.88898 validation_1-auc:0.82859 [32] validation_0-auc:0.88914 validation_1-auc:0.82837 [33] validation_0-auc:0.88935 validation_1-auc:0.82847 [34] validation_0-auc:0.89037 validation_1-auc:0.82891 [35] validation_0-auc:0.89097 validation_1-auc:0.82869 [36] validation_0-auc:0.89158 validation_1-auc:0.82814 [37] validation_0-auc:0.89167 validation_1-auc:0.82822 [38] validation_0-auc:0.89184 validation_1-auc:0.82764 [39] validation_0-auc:0.89187 validation_1-auc:0.82734 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/05/10/santander.html",
            "relUrl": "/kaggle/2022/05/10/santander.html",
            "date": " • May 10, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "EDA HIGHLIGHT",
            "content": "library(tidyverse) . Warning message: &#34;패키지 &#39;tidyverse&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 Warning message: &#34;패키지 &#39;ggplot2&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tibble&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tidyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;readr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;purrr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;dplyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;stringr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;forcats&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() . options(repr.plot.width=16, repr.plot.height=8) . Ch.1 R Language . 생략 | . Ch.2 Introduction to EDA . 자료분석은 대체로 두 가지 단계로 나뉜다. . 탐색적 자료분석(데이터의 구조와 특징 파악) . | 확증적 자료분석(모형, 재현성 평가) . | . EDA(Exploratory Data Analysis) . EDA는 데이터 특징과 내재하는 구조적 관계를 알아내기 위한 기법들을 통칭한다. . | 데이터를 특정한 모형에 적합시키기 보다는 데이터를 있는 그대로 보려는 데에 중점을 둔다. . | . 4 Themes . EDA에서는 네 가지 주제가 때로는 홀로, 때로는 얽혀서 나타난다. . 저항성(resistance)의 강조(예를 들어 평균보다는 일부자료의 파손에 저항적인 중위수가 바람직한 대표값 측도로서 선호된다.) . | 잔차(residual) 계산 . | 자료변수의 재표현(re-expression)을 통한 다각적 시도 . | 그래프를 통한 현시성(revelation) . | . Summary (2) . 자료분석은 탐색적 자료분석과 확증적 자료분석의 두 단계로 나눌 수 있다. . | EDA는 자료의 구조 및 특징의 파악을 목적으로 한다. 이를 위하여 효과적이고 신뢰성 있는 데이터의 요약과 그래프적 기법이 사용된다. . | EDA의 네 개 주제는 저항성, 잔차, 재표현, 현시성이다. . | 통계적 모형은 &#39;진실&#39;로서가 아니라 &#39;주요 사례&#39;로서 의미가 있다. 또한 모형과 데이터는 일방통행이 아닌 쌍방통행으로 이해되어야 한다. 데이터와 분석도 사이클을 이룬다. . | . Ch.3 Stem and Leaf . Stem and Leaf . 데이터의 값을 십 단위인 줄기(stem)와 일 단위인 잎(leaf)으로 분리한다. . - 장점 . 사분위수나 중앙값을 찾기 쉽다. . | 분포의 전체적인 모양(봉우리 개수, 대칭분포, 치우친 방향)을 쉽게 알 수 있다. . | 이상값 유무를 파악할 수 있다. . | . - 단점 . 자료가 많은 경우에는 부적합하고 자료의 수가 50개 이하일 때 적합하다. | . exam1 = read.table(&#39;dataset/EDA/exam1.txt&#39;, header = TRUE) head(exam1) . A data.frame: 6 × 2 hwscore . &lt;int&gt;&lt;int&gt; . 10 | 54 | . 20 | 51 | . 31 | 52 | . 40 | 82 | . 51 | 37 | . 61 | 41 | . stem(exam1$score) . The decimal point is 1 digit(s) to the right of the | 0 | 00 1 | 058 2 | 1333458889 3 | 0355789 4 | 11133456678 5 | 11122233344456688 6 | 147779 7 | 33478 8 | 29 9 | 09 . 이 줄기그림의 주요 특징은 이봉분포의 모습을 보인다는 점이다. 이것은 자료가 2개의 군집으로 되어 있음을 말한다. | . exam1[exam1$hw == 0, ]$score %&gt;% stem # 과제 미제출자 exam1[exam1$hw == 1, ]$score %&gt;% stem # 과제 제출자 . The decimal point is 1 digit(s) to the right of the | 0 | 00 1 | 05 2 | 13334589 3 | 0355 4 | 13378 5 | 122333446 6 | 4 7 | 3 8 | 29 The decimal point is 1 digit(s) to the right of the | 1 | 8 2 | 88 3 | 789 4 | 114566 5 | 11245688 6 | 17779 7 | 3478 8 | 9 | 09 . 과제 제출 여부에 따라서 줄기-잎 그림을 두 개 그려본 결과 흥미로운 점을 확인했다. . 과제 미제출자에 경우 여전히 이봉분포를 띄고 있으나 과제 제출자의 경우 단봉분포를 띄고 있다. . 이것은 과제물 미제출 그룹이 서로 이질적인 어떤 두 집단(50점대를 중심으로 하는 집단과 20점대를 중심으로 하는 집단)의 혼합임을 말하여 준다. . 아마 전자는 학업습관이 불성실하나 학업능력은 우수한 집단이고 후자는 학업습관과 학업능력 모두 좋지 않은 학생들의 집단이 아닐까 생각해본다. . Tip :R에서 줄기 수를 줄이거나 늘이려면 stem() 함수 내 scale 파라미터를 조정하면 된다. . stem(exam1$score, scale = 0.5) # 줄기 수를 절반으로 줄인다. . The decimal point is 1 digit(s) to the right of the | 0 | 00058 2 | 13334588890355789 4 | 1113345667811122233344456688 6 | 14777933478 8 | 2909 . 이 줄기 그림은 단봉분포의 형태를 취한다. 이것은 너무 단순하여 이 자료의 주요 특성을 잃은 것으로 볼 수 있다. . 즉 2개의 봉우리를 구분하지 못하고 1개만 본 것이다. . stem(exam1$score, scale = 2) # 줄기 수를 두 배로 늘인다. . The decimal point is 1 digit(s) to the right of the | 0 | 00 0 | 1 | 0 1 | 58 2 | 13334 2 | 58889 3 | 03 3 | 55789 4 | 111334 4 | 56678 5 | 111222333444 5 | 56688 6 | 14 6 | 7779 7 | 334 7 | 78 8 | 2 8 | 9 9 | 0 9 | 9 . 줄기 수를 늘였더니 더 많은 봉우리를 볼 수 있다. . 일반적으로 줄기 수를 늘이면 늘일수록 많은 수의 봉우리를 보게 되고 그 반대로 줄기 수를 줄이면 줄일수록 적은 수의 봉우리를 보게 된다. . Compared with Histogram . hist(exam1$score) . 줄기 - 잎 히스토그램 . 정보 손실 | 손실되지 않음 | 손실됨 | . 줄기 수 변환 | 쉬움 | 어려움 | . 구간의 폭 | 조정 불가능 | 조정 가능 | . Summary (3) . 줄기 그림은 히스토그램과 마찬가지로 자료 분포의 특성을 그래프화한 것이다. 줄기 그림은 히스토그램에 비하여 정보의 보전 면에서 우수하며 쉽게 구간(줄기) 수를 늘이거나 줄일 수 있다. 그러나 구간(줄기)의 선정시 제약이 따른다. . | 적절한 줄기 그림을 그리기 위하여 여러 개의 그림을 그려보고 비교해 보아야 한다. 계획된 시행착오가 필요하다. . | 줄기 그림에서는 다음과 같은 자료의 특성을 관찰할 수 있다. . 군집의 수 | 집중도가 높은 구간 | 대칭성 여부 | 자료의 범위 및 산포 | 특이점의 존재여부 | . | . Ch.4 Numerical Summary and Box Plot . Mean and Median . . 한 쪽 꼬리가 긴 분포에서 평균값은 쉽게 휘둘리지만 중앙값은 쉽게 휘둘리지 않다. 따라서 중앙값이 대표값으로서 적합하다. | . $ begin{cases} X_{ frac{N+1}{2}} qquad if N = odd dfrac{X_{ frac{N}{2}} + X_{ frac{N}{2} + 1}}{2} qquad if N = even end{cases}$ . 중간값의 깊이 $d(M)$은 . $d(M) = dfrac{(N+1)}{2}$ . Five Number Summary . 아래 4분위수 : $H_L$ 중간값 : $M$ 위 4분위수 : $H_U$ . 다섯 숫자 요약 = (min, $H_L, M, H_U$, max) = (최솟값, 제 1사분위수, 중앙값, 제 3사분위수, 최댓값) . summary(exam1$score) . Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00 33.00 48.00 47.23 58.00 99.00 . Skewness . 왜도 = $SKEW = dfrac{(H_U - M) - (M - H_L)}{(H_U - M) + (M - H_L)}$ . 왜도 &lt; 0 이면 왼쪽으로 기울어진 분포 . 왜도 &gt; 0 이면 오른쪽으로 기울어진 분포 . Quantiles . low = quantile(exam1$score, c(1/2, 1/4, 1/8, 1/16), type = 8) %&gt;% as.numeric %&gt;% round(2)# 중간값, 아래 4분위수, 아래 8분위수, 아래 16분위수 high = quantile(exam1$score, c(1/2, 3/4, 7/8, 15/16), type = 8) %&gt;% as.numeric %&gt;% round(2) # 중간값, 위 4분위수, 위 8분위수, 위 16분위수 values = cbind(low, high, (low+high)/2, high-low) colnames(values) = c(&#39;low&#39;, &#39;high&#39;, &#39;mid&#39;, &#39;spr&#39;) rownames(values) = c(&#39;M&#39;, &#39;H&#39;, &#39;E&#39;, &#39;D&#39;) values . A matrix: 4 × 4 of type dbl lowhighmidspr . M48.00 | 48.00 | 48.00 | 0.00 | . H32.00 | 58.00 | 45.00 | 26.00 | . E23.00 | 73.00 | 48.00 | 50.00 | . D16.25 | 80.33 | 48.29 | 64.08 | . KURTO . Ch.5 Data Re-Expression . &#47729;&#49849;, &#47196;&#44536;, &#51648;&#49688; &#48320;&#54872;&#50640; &#51032;&#54620; &#51116;&#54364;&#54788; . Standardization . 표준화 변환이란 통상적으로 한 자료묶음의 평균이 0, 표준편차가 1이 되도록 하는 선형변환을 말한다. | . $x_1, x_2, dots, x_n$을 자료 값이라고 할 때 이것의 표준화변환 $z_1, z_2, dots, z_n$은 다음과 같이 정한다. . $z_i = dfrac{x_i - bar{x}}{s_x}, , i = 1,2, dots,n. qquad(1)$ . 그런데 (1)은 로버스트하지 않은 $ bar{x}$와 $s_x$에 의존하므로 EDA의 관점에서는 믿고 사용하기 어렵다. [^1] . 왜냐하면 표본평균과 표본표준편차는 극단적인 이상점에 의해 크게 변동될 수 있기 때문이다. | 그러나 중앙값 또는 사분위수범위(IQR)은 비교적 로버스트하다. | . 즉, 평균 $ bar{x}$ 대신에 중앙값 $med_x$를, 표준편차 $s_x$ 대신에 사분위수범위 $IQR$을 보정한 $ tilde{ sigma_x} = dfrac{IQR}{1.35}$을 쓰는 것이 좋을 것이다. . 따라서 로버스트 표준화 변환은 다음과 같다. . $ bar{z_i} = dfrac{x_i - med_x}{ tilde{ sigma_x}} , i = 1,2, dots,n. qquad(1)$ . 표준화 변환을 사용하는 예시 상황은 다음과 같다. . [1] &#34;A 그룹 학생 100명의 시험 X 점수는 N(40,10)으로부터 생성되었다.&#34; [1] &#34;B 그룹 학생 90명의 시험 Y 점수는 N(40,10)으로부터, 나머지 10명의 시험 Y 점수 N(80,5)으로부터 생성되었다.&#34; . par(mfrow = c(1,2)) X_group &lt;- rnorm(100, 40, 10) Y_group &lt;- c(rnorm(90,40,10), rnorm(10,80,5)) z_X &lt;- (X_group-mean(X_group))/sd(X_group) z_Y &lt;- (Y_group-mean(Y_group))/sd(Y_group) hist(z_X, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Standardized X&#39;) hist(z_Y, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Standardized Y&#39;) . par(mfrow = c(1,2)) robust_z_X &lt;- (X_group-median(X_group))/(IQR(X_group)/1.35) robust_z_Y &lt;- (Y_group-median(Y_group))/(IQR(Y_group)/1.35) hist(robust_z_X, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Robust Standardized X&#39;) hist(robust_z_Y, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Robust Standardized Y&#39;) . Summary (5) . 선형변환 $ax+b , (a &gt; 0)$은 분포의 형태를 바꾸지 않는다. 그러나 비선형변환은 분포의 형태를 바꾼다. | . 변환의 사다리는 $x^p$ 꼴의 파워(power, 멱승)형 변환을 일컫는데 변환의 사다리를 내려가면 $(p &lt; 1)$ 오른쪽 꼬리가 짧아진다. $p=0$에 해당하는 변환은 로그변환이다. | . 자료의 재표현은 분포의 대칭화를 위하여, 또는 자료묶음들의 산포를 균일화하기 위한 목적으로 실행된다. | . 자료의 재표현은 자료 해석을 풍부하게 한다. | . Ch.6 QQ-Plot . Various Patterns . Normal Distribution . 정규분포로부터의 모의생성 자료에 대한 정규확률 플롯 대체로 직선적인 경향선을 확인할 수 있다. | 직선의 절편과 기울기가 각각 100과 15 근처임을 확인할 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- rnorm(40, 100, 15) qqnorm(x1) qqline(x1) x2 &lt;- rnorm(4000, 100, 15) qqnorm(x2) qqline(x2) . Mixture Normal Distribution . 혼합 정규분포로부터의 모의생성 자료에 대한 정규확률 플롯 중앙에서 밀도가 낮다. | 우상과 좌하 부분에서 강한 곡선성을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- c(rnorm(20, 70, 15), rnorm(20, 130, 15)) qqnorm(x1) x2 &lt;- c(rnorm(2000, 70, 15), rnorm(2000, 130, 15)) qqnorm(x2) . Data with outliers . 특이값이 내재하는 모의생성 자료에 대한 정규확률 플롯 25, 175가 이상점으로 존재한다. | 25는 주경향선보다 아래에 있다. | 175는 주경향선보다 위에 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- rnorm(38, 100, 15) outliers &lt;- c(25, 175) qqnorm(c(x1, outliers)) x2 &lt;- rnorm(3800, 100, 15) qqnorm(c(x2, outliers)) . Short Tail . 꼬리가 짧은 분포로부터의 모의생성 자료에 대한 정규확률 플롯 플롯의 전체적 모양이 비스듬한 S자 성장곡선의 형태를 취하고 있음을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- runif(40, 80, 120) qqnorm(x1) x2 &lt;- runif(4000, 80, 120) qqnorm(x2) . Long Tail . 꼬리가 긴 분포로부터의 모의생성 자료에 대한 정규확률 플롯 플롯의 전체적인 모양이 비스듬한 역 S자 성장곡선의 형태를 취하고 있음을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- c(rexp(20,1), -rexp(20,1)) qqnorm(x1) x2 &lt;- c(rexp(2000,1), -rexp(2000,1)) qqnorm(x2) . Right Skewed . 큰 값 쪽으로 긴 꼬리를 뻗은 기울어진 분포의 경우 비스듬히 기울어진 J자 곡선의 형태임을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- exp(rnorm(40, 5, 1)) qqnorm(x1) x2 &lt;- exp(rnorm(4000, 5, 1)) qqnorm(x2) . Left Skewed . 작은 값 쪽으로 긴 꼬리를 뻗은 기울어진 분포의 경우 비스듬히 기울어진 역 J자 곡선의 형태임을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- 1500 - exp(rnorm(40,5,1)) qqnorm(x1) x2 &lt;- 1500 - exp(rnorm(40,5,1)) qqnorm(x2) . Example . 백혈병 환자 21명의 생존시간에 관한 다음의 자료를 지수분포에 적합하여 보자. | . leukemia &lt;- c(1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23) leukemia_quant = ((1:length(leukemia)) - 1/3) / (length(leukemia) + 1/3) x &lt;- -log(1-leukemia_quant) y &lt;- sort(leukemia) plot(y ~ x) . library(lattice) qqmath(leukemia) . Ch.7 2&#50896; &#51088;&#47308;, &#48712;&#46020; &#54364;&#51032; &#53456;&#49353; . 2&#50896; &#51088;&#47308; &#54364;&#51032; &#48516;&#54644; . 평균과 같은 추정치는 특이값에 의해 쉽게 영향을 받으므로 EDA적 관점에서 보면 바람직하지 않다. . 따라서 평균을 중간값으로 대체하고 다듬을 필요가 있다. . R 함수 . {r} medpolish(world_temp) . 7장 이원자료빈도표, . 7.4절 . 8장 시계자료탐색 표 데이터주고 3할 구하기 등, 스필트해보기 등, . 149페이지 표3에 대한거 스필트해보기 등, . 147페이지 . 19.5 봉우리인지 골자기인지, 봉우리를 스필트해라, . 8.5 자기 상관과 가중치? 여기서 그림이 의미하는 바, hf? 자기상관 교차상관, . 9장 . 165 lms lts , . 167 lts, . 10장 그림을 그려서 알수잇는게 뭐냐 . 182 밀감 추정? 데이터 주고 커널 밀도 추정해봐라 계산해서 그려봐라 계산기 필요! 도자기 플룻 한번 보기 . 11장 가변량 자료 여러가지 데이터를 주고 평행자료 같은거 주면 해석하기 그림을 그리고 해석하라 알수잇는게 뭐냐 정도 . world_temp &lt;- read.table(&#39;dataset/EDA/WorldTemperature_Mean.txt&#39;, header = TRUE) world_temp . A data.frame: 21 × 12 JanFebMarchAprilMayJuneJulyAugSepOctNovDec . &lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; . BAHRAIN16.0 | 16.5 | 20.0 | 24.0 | 28.5 | 31.0 | 32.5 | 33.0 | 31.0 | 27.5 | 23.0 | 18.0 | . CALCUTTA20.0 | 22.5 | 27.5 | 30.0 | 31.0 | 29.5 | 29.0 | 29.0 | 29.0 | 28.0 | 23.5 | 19.5 | . COLOMBO26.0 | 26.0 | 27.0 | 27.5 | 28.0 | 27.5 | 27.0 | 27.0 | 27.5 | 26.5 | 31.0 | 26.0 | . CHENNAI25.0 | 25.5 | 27.5 | 30.5 | 33.5 | 32.5 | 31.0 | 30.5 | 29.5 | 28.0 | 26.0 | 25.0 | . DUBAI18.0 | 19.0 | 22.5 | 25.0 | 28.0 | 30.5 | 33.5 | 33.5 | 31.0 | 27.5 | 23.5 | 20.0 | . FRAKFURT-0.5 | 1.0 | 6.5 | 12.0 | 15.0 | 18.5 | 20.0 | 19.5 | 16.0 | 11.0 | 5.5 | 1.0 | . GENEVA 1.0 | 0.0 | 6.0 | 10.0 | 13.5 | 16.0 | 20.0 | 19.5 | 15.0 | 9.5 | 5.5 | 2.0 | . LONDON 4.0 | 4.5 | 6.5 | 9.0 | 12.5 | 15.5 | 17.5 | 17.0 | 15.0 | 11.0 | 7.5 | 5.0 | . MADRID 5.0 | 6.5 | 9.5 | 12.5 | 16.5 | 21.0 | 25.0 | 24.5 | 20.5 | 14.0 | 9.0 | 5.0 | . MANILA24.0 | 24.5 | 26.5 | 27.5 | 28.0 | 28.0 | 26.0 | 26.0 | 26.5 | 26.5 | 25.0 | 24.5 | . MOSCOW-9.5 | -8.5 | -4.0 | 4.0 | 12.5 | 17.0 | 18.5 | 16.5 | 11.0 | 4.5 | -2.5 | -8.0 | . MEXICO_CITY14.0 | 19.0 | 21.0 | 22.5 | 22.5 | 22.5 | 22.5 | 22.0 | 22.0 | 20.5 | 19.5 | 15.5 | . NEW_DELHI13.5 | 17.0 | 21.5 | 28.0 | 33.0 | 33.5 | 31.0 | 30.0 | 29.0 | 26.0 | 19.5 | 15.0 | . NEW_YORK 1.0 | 0.5 | 5.0 | 10.5 | 17.0 | 22.0 | 25.0 | 24.0 | 20.0 | 15.0 | 8.5 | 2.0 | . PARIS 4.0 | 4.0 | 7.0 | 10.5 | 13.5 | 17.0 | 19.0 | 18.5 | 16.0 | 11.5 | 7.0 | 3.5 | . ROME 7.5 | 9.0 | 11.5 | 14.5 | 18.0 | 22.5 | 25.5 | 25.5 | 22.0 | 17.0 | 12.5 | 8.5 | . SAN_FRANCISCO 9.5 | 10.0 | 12.0 | 13.0 | 14.5 | 16.0 | 17.0 | 17.0 | 17.5 | 16.5 | 13.0 | 10.0 | . SINGAPORE26.5 | 26.5 | 27.5 | 27.5 | 27.0 | 27.5 | 27.5 | 27.5 | 27.0 | 27.0 | 27.0 | 26.5 | . SYDNEY25.0 | 25.0 | 22.0 | 19.5 | 16.0 | 11.0 | 12.0 | 13.0 | 15.5 | 17.5 | 21.0 | 25.0 | . TOKYO 4.5 | 5.0 | 8.0 | 13.5 | 18.5 | 21.5 | 25.5 | 27.0 | 23.5 | 17.0 | 12.0 | 7.0 | . TORONTO-4.5 | -4.5 | 0.0 | 4.5 | 5.0 | 18.5 | 21.5 | 21.0 | 17.0 | 10.0 | 8.0 | -2.0 | . polish &lt;- medpolish(world_temp) polish . 1: 703.75 2: 686.375 Final: 685.75 . Median Polish Results (Dataset: &#34;world_temp&#34;) Overall: 16.125 Row Effects: BAHRAIN CALCUTTA COLOMBO CHENNAI DUBAI 9.5000 11.2500 10.8750 14.0625 10.2500 FRAKFURT GENEVA LONDON MADRID MANILA -5.1875 -6.0625 -5.7500 -2.5000 10.8750 MOSCOW MEXICO_CITY NEW_DELHI NEW_YORK PARIS -11.8750 5.1875 8.0625 -2.8125 -4.9375 ROME SAN_FRANCISCO SINGAPORE SYDNEY TOKYO 0.0000 -1.5000 11.1250 2.3750 -0.3125 TORONTO -8.5000 Column Effects: Jan Feb March April May June July Aug Sep Oct -8.3750 -7.1250 -3.8750 -0.2500 2.3125 5.3750 7.1250 7.1250 4.8125 0.3750 Nov Dec -3.6250 -7.6250 Residuals: Jan Feb March April May June July BAHRAIN -1.2500 -2.0000 -1.7500 -1.3750 0.5625 0.0000 -0.2500 CALCUTTA 1.0000 2.2500 4.0000 2.8750 1.3125 -3.2500 -5.5000 COLOMBO 7.3750 6.1250 3.8750 0.7500 -1.3125 -4.8750 -7.1250 CHENNAI 3.1875 2.4375 1.1875 0.5625 1.0000 -3.0625 -6.3125 DUBAI 0.0000 -0.2500 0.0000 -1.1250 -0.6875 -1.2500 0.0000 FRAKFURT -3.0625 -2.8125 -0.5625 1.3125 1.7500 2.1875 1.9375 GENEVA -0.6875 -2.9375 -0.1875 0.1875 1.1250 0.5625 2.8125 LONDON 2.0000 1.2500 0.0000 -1.1250 -0.1875 -0.2500 0.0000 MADRID -0.2500 0.0000 -0.2500 -0.8750 0.5625 2.0000 4.2500 MANILA 5.3750 4.6250 3.3750 0.7500 -1.3125 -4.3750 -8.1250 MOSCOW -5.3750 -5.6250 -4.3750 0.0000 5.9375 7.3750 7.1250 MEXICO_CITY 1.0625 4.8125 3.5625 1.4375 -1.1250 -4.1875 -5.9375 NEW_DELHI -2.3125 -0.0625 1.1875 4.0625 6.5000 3.9375 -0.3125 NEW_YORK -3.9375 -5.6875 -4.4375 -2.5625 1.3750 3.3125 4.5625 PARIS 1.1875 -0.0625 -0.3125 -0.4375 0.0000 0.4375 0.6875 ROME -0.2500 0.0000 -0.7500 -1.3750 -0.4375 1.0000 2.2500 SAN_FRANCISCO 3.2500 2.5000 1.2500 -1.3750 -2.4375 -4.0000 -4.7500 SINGAPORE 7.6250 6.3750 4.1250 0.5000 -2.5625 -5.1250 -6.8750 SYDNEY 14.8750 13.6250 7.3750 1.2500 -4.8125 -12.8750 -13.6250 TOKYO -2.9375 -3.6875 -3.9375 -2.0625 0.3750 0.3125 2.5625 TORONTO -3.7500 -5.0000 -3.7500 -2.8750 -4.9375 5.5000 6.7500 Aug Sep Oct Nov Dec BAHRAIN 0.2500 0.5625 1.5000 1.0000 0.0000 CALCUTTA -5.5000 -3.1875 0.2500 -0.2500 -0.2500 COLOMBO -7.1250 -4.3125 -0.8750 7.6250 6.6250 CHENNAI -6.8125 -5.5000 -2.5625 -0.5625 2.4375 DUBAI 0.0000 -0.1875 0.7500 0.7500 1.2500 FRAKFURT 1.4375 0.2500 -0.3125 -1.8125 -2.3125 GENEVA 2.3125 0.1250 -0.9375 -0.9375 -0.4375 LONDON -0.5000 -0.1875 0.2500 0.7500 2.2500 MADRID 3.7500 2.0625 0.0000 -1.0000 -1.0000 MANILA -8.1250 -5.3125 -0.8750 1.6250 5.1250 MOSCOW 5.1250 1.9375 -0.1250 -3.1250 -4.6250 MEXICO_CITY -6.4375 -4.1250 -1.1875 1.8125 1.8125 NEW_DELHI -1.3125 0.0000 1.4375 -1.0625 -1.5625 NEW_YORK 3.5625 1.8750 1.3125 -1.1875 -3.6875 PARIS 0.1875 0.0000 -0.0625 -0.5625 -0.0625 ROME 2.2500 1.0625 0.5000 0.0000 0.0000 SAN_FRANCISCO -4.7500 -1.9375 1.5000 2.0000 3.0000 SINGAPORE -6.8750 -5.0625 -0.6250 3.3750 6.8750 SYDNEY -12.6250 -7.8125 -1.3750 6.1250 14.1250 TOKYO 4.0625 2.8750 0.8125 -0.1875 -1.1875 TORONTO 6.2500 4.5625 2.0000 4.0000 -2.0000 . &#51201;&#54633; &#47784;&#54805;&#51032; &#44160;&#53664; . comparison &lt;- (matrix(polish$row, ncol = 1) %*% matrix(polish$col, nrow = 1))/polish$overall comparison . A matrix: 21 × 12 of type dbl -4.9341085 | -4.1976744 | -2.2829457 | -0.147286822 | 1.36240310 | 3.1666667 | 4.1976744 | 4.1976744 | 2.8352713 | 0.220930233 | -2.13565891 | -4.4922481 | . -5.8430233 | -4.9709302 | -2.7034884 | -0.174418605 | 1.61337209 | 3.7500000 | 4.9709302 | 4.9709302 | 3.3575581 | 0.261627907 | -2.52906977 | -5.3197674 | . -5.6482558 | -4.8052326 | -2.6133721 | -0.168604651 | 1.55959302 | 3.6250000 | 4.8052326 | 4.8052326 | 3.2456395 | 0.252906977 | -2.44476744 | -5.1424419 | . -7.3037791 | -6.2136628 | -3.3793605 | -0.218023256 | 2.01671512 | 4.6875000 | 6.2136628 | 6.2136628 | 4.1969477 | 0.327034884 | -3.16133721 | -6.6497093 | . -5.3236434 | -4.5290698 | -2.4631783 | -0.158914729 | 1.46996124 | 3.4166667 | 4.5290698 | 4.5290698 | 3.0591085 | 0.238372093 | -2.30426357 | -4.8468992 | . 2.6942829 | 2.2921512 | 1.2466085 | 0.080426357 | -0.74394380 | -1.7291667 | -2.2921512 | -2.2921512 | -1.5482074 | -0.120639535 | 1.16618217 | 2.4530039 | . 3.1487403 | 2.6787791 | 1.4568798 | 0.093992248 | -0.86942829 | -2.0208333 | -2.6787791 | -2.6787791 | -1.8093508 | -0.140988372 | 1.36288760 | 2.8667636 | . 2.9864341 | 2.5406977 | 1.3817829 | 0.089147287 | -0.82461240 | -1.9166667 | -2.5406977 | -2.5406977 | -1.7160853 | -0.133720930 | 1.29263566 | 2.7189922 | . 1.2984496 | 1.1046512 | 0.6007752 | 0.038759690 | -0.35852713 | -0.8333333 | -1.1046512 | -1.1046512 | -0.7461240 | -0.058139535 | 0.56201550 | 1.1821705 | . -5.6482558 | -4.8052326 | -2.6133721 | -0.168604651 | 1.55959302 | 3.6250000 | 4.8052326 | 4.8052326 | 3.2456395 | 0.252906977 | -2.44476744 | -5.1424419 | . 6.1676357 | 5.2470930 | 2.8536822 | 0.184108527 | -1.70300388 | -3.9583333 | -5.2470930 | -5.2470930 | -3.5440891 | -0.276162791 | 2.66957364 | 5.6153101 | . -2.6942829 | -2.2921512 | -1.2466085 | -0.080426357 | 0.74394380 | 1.7291667 | 2.2921512 | 2.2921512 | 1.5482074 | 0.120639535 | -1.16618217 | -2.4530039 | . -4.1875000 | -3.5625000 | -1.9375000 | -0.125000000 | 1.15625000 | 2.6875000 | 3.5625000 | 3.5625000 | 2.4062500 | 0.187500000 | -1.81250000 | -3.8125000 | . 1.4607558 | 1.2427326 | 0.6758721 | 0.043604651 | -0.40334302 | -0.9375000 | -1.2427326 | -1.2427326 | -0.8393895 | -0.065406977 | 0.63226744 | 1.3299419 | . 2.5644380 | 2.1816860 | 1.1865310 | 0.076550388 | -0.70809109 | -1.6458333 | -2.1816860 | -2.1816860 | -1.4735950 | -0.114825581 | 1.10998062 | 2.3347868 | . 0.0000000 | 0.0000000 | 0.0000000 | 0.000000000 | 0.00000000 | 0.0000000 | 0.0000000 | 0.0000000 | 0.0000000 | 0.000000000 | 0.00000000 | 0.0000000 | . 0.7790698 | 0.6627907 | 0.3604651 | 0.023255814 | -0.21511628 | -0.5000000 | -0.6627907 | -0.6627907 | -0.4476744 | -0.034883721 | 0.33720930 | 0.7093023 | . -5.7781008 | -4.9156977 | -2.6734496 | -0.172480620 | 1.59544574 | 3.7083333 | 4.9156977 | 4.9156977 | 3.3202519 | 0.258720930 | -2.50096899 | -5.2606589 | . -1.2335271 | -1.0494186 | -0.5707364 | -0.036821705 | 0.34060078 | 0.7916667 | 1.0494186 | 1.0494186 | 0.7088178 | 0.055232558 | -0.53391473 | -1.1230620 | . 0.1623062 | 0.1380814 | 0.0750969 | 0.004844961 | -0.04481589 | -0.1041667 | -0.1380814 | -0.1380814 | -0.0932655 | -0.007267442 | 0.07025194 | 0.1477713 | . 4.4147287 | 3.7558140 | 2.0426357 | 0.131782946 | -1.21899225 | -2.8333333 | -3.7558140 | -3.7558140 | -2.5368217 | -0.197674419 | 1.91085271 | 4.0193798 | . plot(polish$residual ~ comparison, xlim = c(-15, 15), ylim = c(-15, 15)) . round(polish$residuals[order(polish$row),],1) . A matrix: 21 × 12 of type dbl JanFebMarchAprilMayJuneJulyAugSepOctNovDec . MOSCOW-5.4 | -5.6 | -4.4 | 0.0 | 5.9 | 7.4 | 7.1 | 5.1 | 1.9 | -0.1 | -3.1 | -4.6 | . TORONTO-3.8 | -5.0 | -3.8 | -2.9 | -4.9 | 5.5 | 6.8 | 6.2 | 4.6 | 2.0 | 4.0 | -2.0 | . GENEVA-0.7 | -2.9 | -0.2 | 0.2 | 1.1 | 0.6 | 2.8 | 2.3 | 0.1 | -0.9 | -0.9 | -0.4 | . LONDON 2.0 | 1.2 | 0.0 | -1.1 | -0.2 | -0.2 | 0.0 | -0.5 | -0.2 | 0.2 | 0.8 | 2.2 | . FRAKFURT-3.1 | -2.8 | -0.6 | 1.3 | 1.8 | 2.2 | 1.9 | 1.4 | 0.2 | -0.3 | -1.8 | -2.3 | . PARIS 1.2 | -0.1 | -0.3 | -0.4 | 0.0 | 0.4 | 0.7 | 0.2 | 0.0 | -0.1 | -0.6 | -0.1 | . NEW_YORK-3.9 | -5.7 | -4.4 | -2.6 | 1.4 | 3.3 | 4.6 | 3.6 | 1.9 | 1.3 | -1.2 | -3.7 | . MADRID-0.2 | 0.0 | -0.2 | -0.9 | 0.6 | 2.0 | 4.2 | 3.8 | 2.1 | 0.0 | -1.0 | -1.0 | . SAN_FRANCISCO 3.2 | 2.5 | 1.2 | -1.4 | -2.4 | -4.0 | -4.8 | -4.8 | -1.9 | 1.5 | 2.0 | 3.0 | . TOKYO-2.9 | -3.7 | -3.9 | -2.1 | 0.4 | 0.3 | 2.6 | 4.1 | 2.9 | 0.8 | -0.2 | -1.2 | . ROME-0.2 | 0.0 | -0.8 | -1.4 | -0.4 | 1.0 | 2.2 | 2.2 | 1.1 | 0.5 | 0.0 | 0.0 | . SYDNEY14.9 | 13.6 | 7.4 | 1.2 | -4.8 | -12.9 | -13.6 | -12.6 | -7.8 | -1.4 | 6.1 | 14.1 | . MEXICO_CITY 1.1 | 4.8 | 3.6 | 1.4 | -1.1 | -4.2 | -5.9 | -6.4 | -4.1 | -1.2 | 1.8 | 1.8 | . NEW_DELHI-2.3 | -0.1 | 1.2 | 4.1 | 6.5 | 3.9 | -0.3 | -1.3 | 0.0 | 1.4 | -1.1 | -1.6 | . BAHRAIN-1.2 | -2.0 | -1.8 | -1.4 | 0.6 | 0.0 | -0.2 | 0.2 | 0.6 | 1.5 | 1.0 | 0.0 | . DUBAI 0.0 | -0.2 | 0.0 | -1.1 | -0.7 | -1.2 | 0.0 | 0.0 | -0.2 | 0.8 | 0.8 | 1.2 | . COLOMBO 7.4 | 6.1 | 3.9 | 0.8 | -1.3 | -4.9 | -7.1 | -7.1 | -4.3 | -0.9 | 7.6 | 6.6 | . MANILA 5.4 | 4.6 | 3.4 | 0.8 | -1.3 | -4.4 | -8.1 | -8.1 | -5.3 | -0.9 | 1.6 | 5.1 | . SINGAPORE 7.6 | 6.4 | 4.1 | 0.5 | -2.6 | -5.1 | -6.9 | -6.9 | -5.1 | -0.6 | 3.4 | 6.9 | . CALCUTTA 1.0 | 2.2 | 4.0 | 2.9 | 1.3 | -3.2 | -5.5 | -5.5 | -3.2 | 0.2 | -0.2 | -0.2 | . CHENNAI 3.2 | 2.4 | 1.2 | 0.6 | 1.0 | -3.1 | -6.3 | -6.8 | -5.5 | -2.6 | -0.6 | 2.4 | . &#44032;&#44396; &#49548;&#48708;&#51648;&#52636; &#49324;&#47168; . house &lt;- read.table(&#39;dataset/EDA/household.txt&#39;, header = TRUE) house . A data.frame: 12 × 10 식료품주거비광열수도가구가사피복신발보건의료교육교양오락교통통신기타소비 . &lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt; . 1993301675 | 45588 | 44341 | 51479 | 75672 | 50533 | 95654 | 51904 | 103564 | 200544 | . 1994341574 | 47389 | 45995 | 55698 | 85239 | 54408 | 106110 | 58551 | 129972 | 215496 | . 1995367080 | 48301 | 52271 | 59117 | 97474 | 59373 | 127027 | 66848 | 142972 | 245429 | . 1996409502 | 54037 | 60177 | 62996 | 104896 | 65592 | 149334 | 75746 | 167487 | 277086 | . 1997427458 | 54038 | 67972 | 63376 | 97824 | 66640 | 162403 | 77057 | 188192 | 284584 | . 1998365859 | 49785 | 74272 | 53147 | 69750 | 58652 | 151439 | 58589 | 183911 | 250820 | . 1999412056 | 54190 | 76172 | 55533 | 80223 | 67098 | 164242 | 71850 | 225085 | 272430 | . 2000447018 | 57026 | 86762 | 59539 | 91785 | 70606 | 182370 | 84481 | 261442 | 291272 | . 2001463582 | 68246 | 92527 | 70408 | 98140 | 74300 | 199407 | 86650 | 286626 | 322237 | . 2002481049 | 62394 | 93238 | 71424 | 101980 | 81057 | 206033 | 86705 | 309288 | 341645 | . 2003509649 | 65051 | 94395 | 72480 | 106861 | 89657 | 224005 | 93911 | 329453 | 337389 | . 2004544775 | 66631 | 97937 | 79384 | 105079 | 94494 | 235813 | 97981 | 346974 | 349143 | . house_mp &lt;- medpolish(house) house_mp . 1: 2361220 2: 2199938 Final: 2191052 . Median Polish Results (Dataset: &#34;house&#34;) Overall: 79769.58 Row Effects: 1993 1994 1995 1996 1997 1998 1999 -24674.047 -20523.547 -13960.984 -2377.453 1800.141 -14832.141 -1800.141 2000 2001 2002 2003 2004 11146.484 17513.016 18713.297 22243.016 26432.516 Column Effects: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 교육 339987.422 -25655.578 -7941.344 -20315.078 4830.484 -11754.109 86387.062 교양오락 교통통신 기타소비 -6233.938 133044.562 200024.906 Residuals: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 교육 교양오락 1993 -93408.0 16148.0 -2813.19 16698.5 15745.984 7191.578 -45828.6 3042.41 1994 -57659.5 13798.5 -5309.69 16767.0 21162.484 6916.078 -39523.1 5538.91 1995 -38716.0 8148.0 -5596.25 13623.5 26834.922 5318.516 -25168.7 7273.34 1996 -7877.5 2300.5 -9273.78 5919.0 22673.391 -46.016 -14445.2 4587.81 1997 5900.9 -1876.1 -5656.38 2121.4 11423.797 -3175.609 -5553.8 1721.22 1998 -39065.9 10503.1 17275.91 8524.6 -17.922 5468.672 114.5 -114.50 1999 -5900.9 1876.1 6143.91 -2121.4 -2576.922 882.672 -114.5 114.50 2000 16114.5 -8234.5 3787.28 -11062.0 -3961.547 -8555.953 5066.9 -201.12 2001 26312.0 -3381.0 3185.75 -6559.5 -3973.078 -11228.484 15737.3 -4398.66 2002 42578.7 -10433.3 2696.47 -6743.8 -1333.359 -5671.766 21163.1 -5543.94 2003 67649.0 -11306.0 323.75 -9217.5 17.922 -601.484 35605.3 -1867.66 2004 98585.5 -13915.5 -323.75 -6503.0 -5953.578 46.016 43223.8 -1987.16 교통통신 기타소비 1993 -84576 -54576.44 1994 -62319 -43774.94 1995 -55881 -20404.50 1996 -42950 -331.03 1997 -26422 2989.38 1998 -14071 -14142.34 1999 14071 -5564.34 2000 37481 331.03 2001 56299 24929.50 2002 77761 43137.22 2003 94396 35351.50 2004 107727 42916.00 . house_mp$residual . A matrix: 12 × 10 of type dbl 식료품주거비광열수도가구가사피복신발보건의료교육교양오락교통통신기타소비 . 1993-93407.953 | 16148.047 | -2813.188 | 16698.547 | 15745.98438 | 7191.57812 | -45828.594 | 3042.406 | -84576.09 | -54576.4375 | . 1994-57659.453 | 13798.547 | -5309.688 | 16767.047 | 21162.48438 | 6916.07812 | -39523.094 | 5538.906 | -62318.59 | -43774.9375 | . 1995-38716.016 | 8147.984 | -5596.250 | 13623.484 | 26834.92188 | 5318.51562 | -25168.656 | 7273.344 | -55881.16 | -20404.5000 | . 1996 -7877.547 | 2300.453 | -9273.781 | 5918.953 | 22673.39062 | -46.01562 | -14445.188 | 4587.812 | -42949.69 | -331.0312 | . 1997 5900.859 | -1876.141 | -5656.375 | 2121.359 | 11423.79688 | -3175.60938 | -5553.781 | 1721.219 | -26422.28 | 2989.3750 | . 1998-39065.859 | 10503.141 | 17275.906 | 8524.641 | -17.92188 | 5468.67188 | 114.500 | -114.500 | -14071.00 | -14142.3438 | . 1999 -5900.859 | 1876.141 | 6143.906 | -2121.359 | -2576.92188 | 882.67188 | -114.500 | 114.500 | 14071.00 | -5564.3438 | . 2000 16114.516 | -8234.484 | 3787.281 | -11061.984 | -3961.54688 | -8555.95312 | 5066.875 | -201.125 | 37481.38 | 331.0312 | . 2001 26311.984 | -3381.016 | 3185.750 | -6559.516 | -3973.07812 | -11228.48438 | 15737.344 | -4398.656 | 56298.84 | 24929.5000 | . 2002 42578.703 | -10433.297 | 2696.469 | -6743.797 | -1333.35938 | -5671.76562 | 21163.062 | -5543.938 | 77760.56 | 43137.2188 | . 2003 67648.984 | -11306.016 | 323.750 | -9217.516 | 17.92188 | -601.48438 | 35605.344 | -1867.656 | 94395.84 | 35351.5000 | . 2004 98585.484 | -13915.516 | -323.750 | -6503.016 | -5953.57812 | 46.01562 | 43223.844 | -1987.156 | 107727.34 | 42916.0000 | . matrix(house_mp$row, ncol = 1) %*% matrix(house_mp$col, nrow = 1) / house_mp$overall . A matrix: 12 × 10 of type dbl -105163.720 | 7935.6937 | 2456.3887 | 6283.7889 | -1494.1485 | 3635.7400 | -26720.944 | 1928.2597 | -41152.879 | -61871.004 | . -87473.796 | 6600.8054 | 2043.1917 | 5226.7728 | -1242.8130 | 3024.1606 | -22226.129 | 1603.9010 | -34230.422 | -51463.486 | . -59503.375 | 4490.1469 | 1389.8654 | 3555.4718 | -845.4140 | 2057.1619 | -15119.153 | 1091.0413 | -23284.980 | -35007.639 | . -10132.988 | 764.6390 | 236.6839 | 605.4707 | -143.9678 | 350.3196 | -2574.681 | 185.7963 | -3965.261 | -5961.544 | . 7672.413 | -578.9632 | -179.2104 | -458.4454 | 109.0084 | -265.2521 | 1949.476 | -140.6797 | 3002.384 | 4513.913 | . -63216.346 | 4770.3291 | 1476.5921 | 3777.3309 | -898.1673 | 2185.5275 | -16062.578 | 1159.1216 | -24737.948 | -37192.093 | . -7672.413 | 578.9632 | 179.2104 | 458.4454 | -109.0084 | 265.2521 | -1949.476 | 140.6797 | -3002.384 | -4513.913 | . 47507.641 | -3584.9444 | -1109.6720 | -2838.6975 | 674.9806 | -1642.4431 | 12071.169 | -871.0901 | 18590.786 | 27950.185 | . 74642.554 | -5632.5551 | -1743.4827 | -4460.0747 | 1060.5089 | -2580.5565 | 18965.852 | -1368.6301 | 29209.274 | 43914.477 | . 79758.295 | -6018.5908 | -1862.9749 | -4765.7528 | 1133.1925 | -2757.4188 | 20265.705 | -1462.4312 | 31211.177 | 46924.223 | . 94802.376 | -7153.8228 | -2214.3709 | -5664.6733 | 1346.9363 | -3277.5257 | 24088.240 | -1738.2763 | 37098.257 | 55775.112 | . 112658.523 | -8501.2543 | -2631.4505 | -6731.6217 | 1600.6334 | -3894.8517 | 28625.291 | -2065.6829 | 44085.760 | 66280.424 | . comparison &lt;- matrix(house_mp$row, ncol = 1) %*% matrix(house_mp$col, nrow = 1) / house_mp$overall . plot(house_mp$residuals ~ comparison) . 가법적이지 않다. (그렇다고 승법적인 것은 아니다.) . 이제 어떻게 할 것인가? 로그 변환을 취한 뒤 다시 살펴보자 . house_log_mp &lt;- medpolish(log(house)) house_log_mp . 1: 10.10955 2: 8.252357 Final: 8.19259 . Median Polish Results (Dataset: &#34;log(house)&#34;) Overall: 11.30465 Row Effects: 1993 1994 1995 1996 1997 1998 -0.32175936 -0.22877213 -0.12265758 -0.01029705 0.01029705 -0.13095072 1999 2000 2001 2002 2003 2004 -0.01268085 0.06165227 0.14900604 0.17342852 0.21261832 0.26830487 Column Effects: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 1.63469862 -0.39685225 -0.07144553 -0.29124378 0.06146341 -0.18422407 교육 교양오락 교통통신 기타소비 0.73231337 -0.08222035 0.99039062 1.22733896 Residuals: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 1993 -0.00047918 1.4137e-01 -0.2117756 0.15728690 0.18981396 0.031720 1994 0.03074766 8.7124e-02 -0.2681398 0.14306996 0.21587760 0.012617 1995 -0.00335143 7.1902e-05 -0.2463454 0.09652970 0.24388965 -0.006169 1996 -0.00634999 -7.1902e-05 -0.2178574 0.04772186 0.20491282 -0.018915 1997 0.01597017 -2.0647e-02 -0.1166459 0.03314178 0.11451929 -0.023658 1998 0.00160992 3.8627e-02 0.1132401 -0.00163403 -0.08248547 -0.010094 1999 0.00225130 5.1393e-03 0.0202301 -0.07598812 -0.06086250 0.006169 2000 0.00935777 -1.8183e-02 0.0760718 -0.08066713 -0.00055699 -0.017203 2001 -0.04161157 7.4075e-02 0.0530498 -0.00034556 -0.02096462 -0.053561 2002 -0.02904820 -3.9997e-02 0.0362822 -0.01044099 -0.00700541 0.009058 2003 -0.01048489 -3.7485e-02 0.0094251 -0.03495407 0.00055699 0.070707 2004 0.00047918 -6.9173e-02 -0.0094251 0.00034556 -0.07194603 0.067565 교육 교양오락 교통통신 기타소비 1993 -0.246707 -0.0435146 -0.425332 -0.0014361 1994 -0.235955 -0.0159995 -0.291190 -0.0225147 1995 -0.162146 0.0104091 -0.301974 0.0014361 1996 -0.112721 0.0230128 -0.256078 0.0103958 1997 -0.049420 0.0195785 -0.160115 0.0165023 1998 0.021930 -0.1131722 -0.041878 0.0314571 1999 -0.015182 -0.0274084 0.041878 -0.0041666 2000 0.015182 0.0602045 0.117279 -0.0116239 2001 0.017138 -0.0017989 0.121891 0.0020521 2002 0.025404 -0.0255868 0.173564 0.0361145 2003 0.069846 0.0150594 0.197535 -0.0156109 2004 0.065531 0.0017989 0.193664 -0.0370524 . comparison &lt;- matrix(house_log_mp$row, ncol = 1) %*% matrix(house_log_mp$col, nrow = 1) / house_log_mp$overall . plot(house_log_mp$residuals ~ comparison) . 이제는 경향선을 찾기 어렵다. . 로그 변환 자료에 대한 중간값 다듬기 결과를 살펴보자. . round(house_log_mp$residuals, 2) . A matrix: 12 × 10 of type dbl 식료품주거비광열수도가구가사피복신발보건의료교육교양오락교통통신기타소비 . 1993 0.00 | 0.14 | -0.21 | 0.16 | 0.19 | 0.03 | -0.25 | -0.04 | -0.43 | 0.00 | . 1994 0.03 | 0.09 | -0.27 | 0.14 | 0.22 | 0.01 | -0.24 | -0.02 | -0.29 | -0.02 | . 1995 0.00 | 0.00 | -0.25 | 0.10 | 0.24 | -0.01 | -0.16 | 0.01 | -0.30 | 0.00 | . 1996-0.01 | 0.00 | -0.22 | 0.05 | 0.20 | -0.02 | -0.11 | 0.02 | -0.26 | 0.01 | . 1997 0.02 | -0.02 | -0.12 | 0.03 | 0.11 | -0.02 | -0.05 | 0.02 | -0.16 | 0.02 | . 1998 0.00 | 0.04 | 0.11 | 0.00 | -0.08 | -0.01 | 0.02 | -0.11 | -0.04 | 0.03 | . 1999 0.00 | 0.01 | 0.02 | -0.08 | -0.06 | 0.01 | -0.02 | -0.03 | 0.04 | 0.00 | . 2000 0.01 | -0.02 | 0.08 | -0.08 | 0.00 | -0.02 | 0.02 | 0.06 | 0.12 | -0.01 | . 2001-0.04 | 0.07 | 0.05 | 0.00 | -0.02 | -0.05 | 0.02 | 0.00 | 0.12 | 0.00 | . 2002-0.03 | -0.04 | 0.04 | -0.01 | -0.01 | 0.01 | 0.03 | -0.03 | 0.17 | 0.04 | . 2003-0.01 | -0.04 | 0.01 | -0.03 | 0.00 | 0.07 | 0.07 | 0.02 | 0.20 | -0.02 | . 2004 0.00 | -0.07 | -0.01 | 0.00 | -0.07 | 0.07 | 0.07 | 0.00 | 0.19 | -0.04 | . house_log_mp . Median Polish Results (Dataset: &#34;log(house)&#34;) Overall: 11.30465 Row Effects: 1993 1994 1995 1996 1997 1998 -0.32175936 -0.22877213 -0.12265758 -0.01029705 0.01029705 -0.13095072 1999 2000 2001 2002 2003 2004 -0.01268085 0.06165227 0.14900604 0.17342852 0.21261832 0.26830487 Column Effects: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 1.63469862 -0.39685225 -0.07144553 -0.29124378 0.06146341 -0.18422407 교육 교양오락 교통통신 기타소비 0.73231337 -0.08222035 0.99039062 1.22733896 Residuals: 식료품 주거비 광열수도 가구가사 피복신발 보건의료 1993 -0.00047918 1.4137e-01 -0.2117756 0.15728690 0.18981396 0.031720 1994 0.03074766 8.7124e-02 -0.2681398 0.14306996 0.21587760 0.012617 1995 -0.00335143 7.1902e-05 -0.2463454 0.09652970 0.24388965 -0.006169 1996 -0.00634999 -7.1902e-05 -0.2178574 0.04772186 0.20491282 -0.018915 1997 0.01597017 -2.0647e-02 -0.1166459 0.03314178 0.11451929 -0.023658 1998 0.00160992 3.8627e-02 0.1132401 -0.00163403 -0.08248547 -0.010094 1999 0.00225130 5.1393e-03 0.0202301 -0.07598812 -0.06086250 0.006169 2000 0.00935777 -1.8183e-02 0.0760718 -0.08066713 -0.00055699 -0.017203 2001 -0.04161157 7.4075e-02 0.0530498 -0.00034556 -0.02096462 -0.053561 2002 -0.02904820 -3.9997e-02 0.0362822 -0.01044099 -0.00700541 0.009058 2003 -0.01048489 -3.7485e-02 0.0094251 -0.03495407 0.00055699 0.070707 2004 0.00047918 -6.9173e-02 -0.0094251 0.00034556 -0.07194603 0.067565 교육 교양오락 교통통신 기타소비 1993 -0.246707 -0.0435146 -0.425332 -0.0014361 1994 -0.235955 -0.0159995 -0.291190 -0.0225147 1995 -0.162146 0.0104091 -0.301974 0.0014361 1996 -0.112721 0.0230128 -0.256078 0.0103958 1997 -0.049420 0.0195785 -0.160115 0.0165023 1998 0.021930 -0.1131722 -0.041878 0.0314571 1999 -0.015182 -0.0274084 0.041878 -0.0041666 2000 0.015182 0.0602045 0.117279 -0.0116239 2001 0.017138 -0.0017989 0.121891 0.0020521 2002 0.025404 -0.0255868 0.173564 0.0361145 2003 0.069846 0.0150594 0.197535 -0.0156109 2004 0.065531 0.0017989 0.193664 -0.0370524 . 2&#50896; &#48712;&#46020; &#54364; &#48516;&#49437; . UCBAdmissions . , , Dept = A Gender Admit Male Female Admitted 512 89 Rejected 313 19 , , Dept = B Gender Admit Male Female Admitted 353 17 Rejected 207 8 , , Dept = C Gender Admit Male Female Admitted 120 202 Rejected 205 391 , , Dept = D Gender Admit Male Female Admitted 138 131 Rejected 279 244 , , Dept = E Gender Admit Male Female Admitted 53 94 Rejected 138 299 , , Dept = F Gender Admit Male Female Admitted 22 24 Rejected 351 317 . Tab1 &lt;- UCBAdmissions[1,,] Tab1 . Dept Gender A B C D E F Male 512 353 120 138 53 22 Female 89 17 202 131 94 24 . Tab2 &lt;- UCBAdmissions[2,,] Tab2 . Dept Gender A B C D E F Male 313 207 205 279 138 351 Female 19 8 391 244 299 317 . Tab &lt;- Tab1 + Tab2 Tab . Dept Gender A B C D E F Male 825 560 325 417 191 373 Female 108 25 593 375 393 341 . barplot(Tab, legend = rownames(Tab)) . Tab_col &lt;- apply(Tab, 2, sum) Tab_col . &lt;dl class=dl-inline&gt;A933B585C918D792E584F714&lt;/dl&gt; Tab_c &lt;- Tab %*% diag(1/Tab_col) * 100 Tab_c . A matrix: 2 × 6 of type dbl Male88.42444 | 95.726496 | 35.40305 | 52.65152 | 32.70548 | 52.2409 | . Female11.57556 | 4.273504 | 64.59695 | 47.34848 | 67.29452 | 47.7591 | . colnames(Tab_c) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;) rownames(Tab_c) &lt;- c(&#39;Male&#39;, &#39;Female&#39;) Tab_c . A matrix: 2 × 6 of type dbl ABCDEF . Male88.42444 | 95.726496 | 35.40305 | 52.65152 | 32.70548 | 52.2409 | . Female11.57556 | 4.273504 | 64.59695 | 47.34848 | 67.29452 | 47.7591 | . barplot(Tab_c, legend = rownames(Tab_c)) . par(mfrow = c(1,2)) barplot(t(Tab), beside = T, legend = colnames(Tab)) Tab_row &lt;- apply(Tab,1,sum) Tab_r &lt;- diag(1/Tab_row) %*% Tab * 100 colnames(Tab_r) &lt;- colnames(Tab_c) rownames(Tab_r) &lt;- rownames(Tab_c) barplot(t(Tab_r), beside = T, legend = colnames(Tab)) . 남학생은 A, B에 50% 이상 집중되어 있는 것과 달리 여학생은 C, D, E, F에 90% 이상이 집중되어 있다. | . Tab_M &lt;- UCBAdmissions[,1,] addmargins(Tab_M) . A table: 3 × 7 of type dbl ABCDEFSum . Admitted512 | 353 | 120 | 138 | 53 | 22 | 1198 | . Rejected313 | 207 | 205 | 279 | 138 | 351 | 1493 | . Sum825 | 560 | 325 | 417 | 191 | 373 | 2691 | . Tab_F &lt;- UCBAdmissions[,2,] addmargins(Tab_F) . A table: 3 × 7 of type dbl ABCDEFSum . Admitted 89 | 17 | 202 | 131 | 94 | 24 | 557 | . Rejected 19 | 8 | 391 | 244 | 299 | 317 | 1278 | . Sum108 | 25 | 593 | 375 | 393 | 341 | 1835 | . 이제 남학생의 합격률과 여성의 합격률을 비교해보자. . par(mfrow = c(1,2)) Tab_M_col &lt;- apply(Tab_M, 2, sum) Tab_M_C &lt;- Tab_M %*% diag(1/Tab_M_col) * 100 colnames(Tab_M_C) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;) barplot(Tab_M_C, legend = rownames(Tab_M_C), main = &#39;Male&#39;) Tab_F_col &lt;- apply(Tab_F, 2, sum) Tab_F_C &lt;- Tab_F %*% diag(1/Tab_F_col) * 100 colnames(Tab_F_C) &lt;- colnames(Tab_M_C) barplot(Tab_F_C, legend = rownames(Tab_F_C), main = &#39;Female&#39;) . C, D, E, F 학과에서 남학생과 여학생의 합격률은 비슷하고 . A, B 학과에서는 여학생의 합격률이 더 높은 것으로 보인다. . par(mfrow = c(1,2)) mosaicplot(~Dept+Gender, data = UCBAdmissions, color = T) mosaicplot(~Gender+Dept, data = UCBAdmissions, color = T) . Ch.8 &#49884;&#44228;&#50676; &#51088;&#47308;&#51032; &#53456;&#49353; . &#49884;&#44228;&#50676; &#54217;&#54876;&#51060;&#46976;? . par(mfrow = c(1,2)) t &lt;- seq(1:60) s &lt;- sin(2*pi*t/12) + sin(2*pi*t/30) a &lt;- rnorm(60, 0, 1) x &lt;- s+a plot(t, x, type = &#39;l&#39;, lty = &#39;dotted&#39;) lines(t, s, col = &#39;red&#39;) plot(t, smooth(x), type = &#39;l&#39;, lty = &#39;dotted&#39;) lines(t, s, col = &#39;red&#39;) . &#54217;&#54876;&#48169;&#48277; . 이동평균(이동중앙값)에서 $K$가 작으면 $smooth_t$ 계열이 평탄하게 된다. 반대로 $K$를 작게하면 $smooth_t$ 계열에 잦은 변동이 생긴다. . 이동평균은 특이점에 취약하므로 평균을 중간값으로 대치한 이동중간값이 선호된다. . t &lt;- 1:12 x &lt;- c(2.1, 9.8, 19.5, 22.5, 16.6, 16.1, 18.5, -3.4, 8.9, -25.2, -14.0, -0.4) . runmed(x, k=3) . &lt;ol class=list-inline&gt;2.1 | 9.8 | 19.5 | 19.5 | 16.6 | 16.6 | 16.1 | 8.9 | -3.4 | -14 | -14 | -14 | &lt;/ol&gt; par(mfrow = c(1,2)) plot(t, x, type = &#39;l&#39;) plot(t, runmed(x, k = 3), type = &#39;l&#39;) . smooth(x, kind = c(&quot;3RSS&quot;), twiceit = T) . 3RSS Tukey smoother resulting from smooth(x = x, kind = c(&#34;3RSS&#34;), twiceit = T) __twiced__ used 5 iterations [1] 2.1 9.8 19.5 16.6 16.6 16.6 16.1 8.9 -3.4 -14.0 -14.0 -14.0 . &#49324;&#47168;&#48516;&#49437; . Telemetric &lt;- read.table(&quot;dataset/EDA/Telemetric.txt&quot;, header = T) head(Telemetric) . A data.frame: 6 × 2 daytemperature . &lt;int&gt;&lt;int&gt; . 11 | 60 | . 22 | 70 | . 33 | 54 | . 44 | 56 | . 55 | 70 | . 66 | 66 | . par(mfrow = c(2,2)) plot(Telemetric$temperature, type = &#39;l&#39;, lty = &#39;dotted&#39;) plot(smooth(Telemetric$temperature), type = &#39;l&#39;, lty = &#39;dotted&#39;) plot(smooth(Telemetric$temperature, kind = c(&#39;3RS3R&#39;), twiceit = TRUE), type = &#39;l&#39;, lty = &#39;dotted&#39;) plot(smooth(Telemetric$temperature, twiceit = TRUE), type = &#39;l&#39;, lty = &#39;dotted&#39;) . &#44228;&#51208;&#54805; &#49884;&#44228;&#50676;&#51032; &#48516;&#54644; . export &lt;- read.table(&#39;dataset/EDA/Export_1988.txt&#39;, header = TRUE) series &lt;- ts(export$Series/1000, start = c(1988,1), frequency = 12) . par(mfrow = c(1,2)) plot(series) plot(log(series)) . decomp_out &lt;- decompose(log(series)) . decomp_out$season . A Time Series: 18 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1988-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1989-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1990-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1991-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1992-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1993-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1994-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1995-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1996-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1997-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1998-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 1999-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2000-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2001-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2002-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2003-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2004-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | 0.110627315 | . 2005-0.138387108 | -0.114503286 | 0.034690776 | -0.001900557 | 0.012587202 | 0.054760266 | -0.024138058 | -0.039345242 | 0.005798168 | 0.045167687 | 0.054642837 | | . round(decomp_out$season[1:12], 4) . &lt;ol class=list-inline&gt;-0.1384 | -0.1145 | 0.0347 | -0.0019 | 0.0126 | 0.0548 | -0.0241 | -0.0393 | 0.0058 | 0.0452 | 0.0546 | 0.1106 | &lt;/ol&gt; plot(decomp_out$trend) . decomp_out$trend . A Time Series: 18 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1988 NA | NA | NA | NA | NA | NA | 8.524909 | 8.531449 | 8.538009 | 8.543836 | 8.546743 | 8.551241 | . 1989 8.554463 | 8.554375 | 8.554675 | 8.555680 | 8.554926 | 8.552561 | 8.546942 | 8.545727 | 8.548441 | 8.549405 | 8.552998 | 8.557128 | . 1990 8.561084 | 8.563496 | 8.568499 | 8.572934 | 8.575389 | 8.583094 | 8.594780 | 8.602987 | 8.608054 | 8.618382 | 8.632110 | 8.645528 | . 1991 8.652480 | 8.656182 | 8.656451 | 8.662890 | 8.676699 | 8.684838 | 8.694924 | 8.703511 | 8.710656 | 8.719050 | 8.723894 | 8.727557 | . 1992 8.735643 | 8.744232 | 8.753537 | 8.762913 | 8.765570 | 8.761348 | 8.756939 | 8.762407 | 8.771404 | 8.776919 | 8.781985 | 8.785710 | . 1993 8.788833 | 8.793225 | 8.799012 | 8.804420 | 8.810720 | 8.821024 | 8.832465 | 8.839711 | 8.845555 | 8.854876 | 8.866559 | 8.878594 | . 1994 8.889805 | 8.901593 | 8.913848 | 8.926946 | 8.943529 | 8.964617 | 8.986476 | 9.009531 | 9.033671 | 9.056742 | 9.081111 | 9.106401 | . 1995 9.132387 | 9.159646 | 9.185185 | 9.207737 | 9.227706 | 9.240700 | 9.254871 | 9.271723 | 9.284862 | 9.293293 | 9.297460 | 9.299917 | . 1996 9.297720 | 9.291511 | 9.283803 | 9.281053 | 9.281978 | 9.283356 | 9.281056 | 9.274890 | 9.271320 | 9.272877 | 9.277592 | 9.283272 | . 1997 9.294459 | 9.307260 | 9.317959 | 9.325326 | 9.328999 | 9.331261 | 9.331807 | 9.339199 | 9.349185 | 9.353986 | 9.354772 | 9.350116 | . 1998 9.340215 | 9.328039 | 9.320473 | 9.311639 | 9.304434 | 9.303823 | 9.304976 | 9.298497 | 9.289597 | 9.286373 | 9.284808 | 9.289721 | . 1999 9.300839 | 9.314044 | 9.325097 | 9.339359 | 9.357316 | 9.373227 | 9.392332 | 9.416432 | 9.438060 | 9.453700 | 9.470780 | 9.488347 | . 2000 9.504214 | 9.523792 | 9.544524 | 9.559536 | 9.567028 | 9.569349 | 9.571030 | 9.574822 | 9.576103 | 9.570657 | 9.562131 | 9.551290 | . 2001 9.534494 | 9.515050 | 9.497387 | 9.479562 | 9.461806 | 9.444202 | 9.430287 | 9.417909 | 9.407256 | 9.408100 | 9.414232 | 9.416737 | . 2002 9.423363 | 9.437233 | 9.448955 | 9.462791 | 9.480672 | 9.498986 | 9.518222 | 9.535730 | 9.549874 | 9.563409 | 9.572192 | 9.581732 | . 2003 9.595717 | 9.605641 | 9.618105 | 9.636003 | 9.653074 | 9.672030 | 9.695130 | 9.721929 | 9.750299 | 9.776638 | 9.804250 | 9.832368 | . 2004 9.858732 | 9.882113 | 9.901066 | 9.916983 | 9.934279 | 9.950823 | 9.964561 | 9.974221 | 9.982031 | 9.989783 | 9.996741 | 10.004872 | . 200510.012865 | 10.023942 | 10.037616 | 10.049119 | 10.058616 | NA | NA | NA | NA | NA | NA | | . x_tilde &lt;- log(series) - decomp_out$seasonal # x_tilde = x - S_5 adjusted &lt;- exp(x_tilde) # 변환 . adj_series &lt;- ts(adjusted, start = c(1988,1), frequency = 12) adj_series . A Time Series: 18 × 12 JanFebMarAprMayJunJulAugSepOctNovDec . 1988 4546.032 | 4621.058 | 4628.182 | 4635.268 | 4713.702 | 4783.146 | 5329.913 | 5522.953 | 5361.729 | 5024.112 | 5501.507 | 5730.333 | . 1989 5054.007 | 4862.924 | 5147.936 | 4792.822 | 4888.130 | 5138.230 | 5360.522 | 5479.749 | 5443.044 | 5069.931 | 5354.060 | 5563.241 | . 1990 4548.954 | 5247.636 | 5091.585 | 4959.266 | 5149.581 | 5385.584 | 5623.639 | 5534.706 | 6076.445 | 5051.467 | 5699.796 | 6287.357 | . 1991 5328.024 | 5455.744 | 5530.650 | 5849.841 | 6069.310 | 6305.439 | 5675.393 | 5993.823 | 5647.321 | 6343.622 | 6322.349 | 6891.019 | . 1992 6192.634 | 5768.236 | 6209.581 | 6373.047 | 6257.822 | 6677.496 | 6507.071 | 6424.417 | 6587.160 | 6811.010 | 6276.172 | 6272.825 | . 1993 6119.844 | 6655.342 | 6679.025 | 6763.548 | 6658.934 | 6862.103 | 6824.736 | 6806.393 | 7143.941 | 7150.435 | 6954.065 | 7249.763 | . 1994 6968.359 | 6955.015 | 7353.724 | 7682.968 | 7759.298 | 7861.148 | 7796.697 | 7906.040 | 8253.233 | 8475.501 | 8734.753 | 9574.575 | . 1995 8916.070 | 9452.755 | 9657.319 | 10177.701 | 10512.510 | 10646.272 | 10741.260 | 11039.106 | 10910.525 | 11015.478 | 10853.046 | 10525.803 | . 199611395.750 | 11082.590 | 11290.855 | 10657.439 | 11095.214 | 10699.891 | 10138.507 | 10076.117 | 9934.509 | 11325.111 | 10793.137 | 10940.250 | . 199710375.331 | 10498.131 | 10940.664 | 11417.296 | 11597.668 | 11731.271 | 12095.300 | 11483.532 | 11268.881 | 11914.799 | 11204.563 | 11126.363 | . 199810336.304 | 12583.285 | 11599.816 | 12083.593 | 11166.884 | 10895.630 | 10268.683 | 10098.754 | 10686.143 | 10164.054 | 11049.034 | 11118.538 | . 199910633.808 | 10469.968 | 11259.727 | 11521.657 | 11279.921 | 12136.241 | 12038.471 | 11826.267 | 11897.204 | 12855.674 | 13442.003 | 13389.032 | . 200013967.572 | 14213.853 | 13937.709 | 13547.764 | 14453.674 | 14438.249 | 14808.833 | 15380.270 | 15045.887 | 14574.670 | 14191.998 | 13408.095 | . 200114521.690 | 14974.141 | 13643.289 | 12144.169 | 13140.511 | 12242.825 | 11670.590 | 12238.364 | 12375.361 | 11552.275 | 11692.289 | 10666.697 | . 200213071.178 | 12360.311 | 12799.507 | 13209.616 | 13995.894 | 12206.989 | 13722.447 | 14519.282 | 13820.397 | 14418.478 | 14388.457 | 13452.556 | . 200316445.296 | 14955.158 | 14854.494 | 15750.498 | 14492.519 | 14822.034 | 15808.738 | 15992.270 | 16922.676 | 18093.678 | 17272.033 | 17662.549 | . 200421805.189 | 21458.595 | 20452.878 | 21524.226 | 20573.658 | 20503.049 | 21516.953 | 20593.492 | 20710.700 | 21662.414 | 21849.473 | 20767.817 | . 200525788.107 | 22878.681 | 23137.922 | 22916.943 | 22835.364 | 22452.638 | 23803.653 | 24284.127 | 24385.299 | 24247.685 | 24517.219 | | . plot(adj_series) . &#51088;&#44592;&#49345;&#44288;&#44284; &#45796;&#51473; &#49884;&#44228;&#50676; . geyser$waiting . &lt;ol class=list-inline&gt;80 | 71 | 57 | 80 | 75 | 77 | 60 | 86 | 77 | 56 | 81 | 50 | 89 | 54 | 90 | 73 | 60 | 83 | 65 | 82 | 84 | 54 | 85 | 58 | 79 | 57 | 88 | 68 | 76 | 78 | 74 | 85 | 75 | 65 | 76 | 58 | 91 | 50 | 87 | 48 | 93 | 54 | 86 | 53 | 78 | 52 | 83 | 60 | 87 | 49 | 80 | 60 | 92 | 43 | 89 | 60 | 84 | 69 | 74 | 71 | 108 | 50 | 77 | 57 | 80 | 61 | 82 | 48 | 81 | 73 | 62 | 79 | 54 | 80 | 73 | 81 | 62 | 81 | 71 | 79 | 81 | 74 | 59 | 81 | 66 | 87 | 53 | 80 | 50 | 87 | 51 | 82 | 58 | 81 | 49 | 92 | 50 | 88 | 62 | 93 | 56 | 89 | 51 | 79 | 58 | 82 | 52 | 88 | 52 | 78 | 69 | 75 | 77 | 53 | 80 | 55 | 87 | 53 | 85 | 61 | 93 | 54 | 76 | 80 | 81 | 59 | 86 | 78 | 71 | 77 | 76 | 94 | 75 | 50 | 83 | 82 | 72 | 77 | 75 | 65 | 79 | 72 | 78 | 77 | 79 | 75 | 78 | 64 | 80 | 49 | 88 | 54 | 85 | 51 | 96 | 50 | 80 | 78 | 81 | 72 | 75 | 78 | 87 | 69 | 55 | 83 | 49 | 82 | 57 | 84 | 57 | 84 | 73 | 78 | 57 | 79 | 57 | 90 | 62 | 87 | 78 | 52 | 98 | 48 | 78 | 79 | 65 | 84 | 50 | 83 | 60 | 80 | 50 | 88 | 50 | 84 | 74 | 76 | 65 | 89 | 49 | 88 | 51 | 78 | 85 | 65 | 75 | 77 | 69 | 92 | 68 | 87 | 61 | 81 | 55 | 93 | 53 | 84 | 70 | 73 | 93 | 50 | 87 | 77 | 74 | 72 | 82 | 74 | 80 | 49 | 91 | 53 | 86 | 49 | 79 | 89 | 87 | 76 | 59 | 80 | 89 | 45 | 93 | 72 | 71 | 54 | 79 | 74 | 65 | 78 | 57 | 87 | 72 | 84 | 47 | 84 | 57 | 87 | 68 | 86 | 75 | 73 | 53 | 82 | 93 | 77 | 54 | 96 | 48 | 89 | 63 | 84 | 76 | 62 | 83 | 50 | 85 | 78 | 78 | 81 | 78 | 76 | 74 | 81 | 66 | 84 | 48 | 93 | 47 | 87 | 51 | 78 | 54 | 87 | 52 | 85 | 58 | 88 | 79 | &lt;/ol&gt; par(mfrow = c(2,2)) library(MASS) plot(geyser$waiting) acf(geyser$waiting) plot(geyser$waiting) acf(geyser$duration) . round(acf(geyser$waiting)$acf[1:5],3) round(acf(geyser$duration)$acf[1:5],3) . &lt;ol class=list-inline&gt;1 | -0.702 | 0.547 | -0.445 | 0.386 | &lt;/ol&gt; &lt;ol class=list-inline&gt;1 | -0.657 | 0.553 | -0.398 | 0.346 | &lt;/ol&gt; waiting과 duration 사이의 관계는 어떨까? 이것은 다음과 같이 정의되는 교차상관함수(CCF)로 알 수 있다. . ccf(geyser$waiting, geyser$duration) . round(cbind(ccf(geyser$waiting, geyser$duration)$lag, ccf(geyser$waiting, geyser$duration)$acf), 3) . A matrix: 43 × 2 of type dbl -21 | 0.038 | . -20 | -0.023 | . -19 | -0.005 | . -18 | 0.009 | . -17 | 0.000 | . -16 | 0.009 | . -15 | 0.046 | . -14 | -0.079 | . -13 | 0.146 | . -12 | -0.166 | . -11 | 0.190 | . -10 | -0.188 | . -9 | 0.179 | . -8 | -0.192 | . -7 | 0.189 | . -6 | -0.196 | . -5 | 0.213 | . -4 | -0.270 | . -3 | 0.365 | . -2 | -0.425 | . -1 | 0.557 | . 0 | -0.645 | . 1 | 0.886 | . 2 | -0.664 | . 3 | 0.562 | . 4 | -0.431 | . 5 | 0.368 | . 6 | -0.308 | . 7 | 0.249 | . 8 | -0.213 | . 9 | 0.201 | . 10 | -0.188 | . 11 | 0.153 | . 12 | -0.203 | . 13 | 0.227 | . 14 | -0.194 | . 15 | 0.139 | . 16 | -0.146 | . 17 | 0.115 | . 18 | -0.045 | . 19 | 0.041 | . 20 | -0.008 | . 21 | 0.038 | . par(mfrow = c(1,2)) plot(geyser$duration, geyser$waiting) waiting_1 &lt;- c(geyser$waiting[2:299], NA) plot(waiting_1 ~ geyser$duration) . &#51452;&#44032; &#49324;&#47168;: &#51076;&#51032;&#48372;&#54665; . library(foreign) . ee &lt;- read.spss(&#39;dataset/EDA/EEstock2000.sav&#39;) x &lt;- ee$change . par(mfrow = c(1,2)) plot(x, type = &#39;l&#39;, ylim = c(-15, 15)) acf(x, ylim = c(-1, 1)) . par(mfrow = c(1,2)) m &lt;- mean(x) sd &lt;- sd(x) hist(x, freq=F, nclass = 20, main = &#39;Daily Stock Change&#39;) curve(dnorm(x, m, sd), add = T) ms &lt;- read.spss(&#39;dataset/EDA/MSstock2000.sav&#39;) x &lt;- ms$change m &lt;- mean(x) sd &lt;- sd(x) hist(x, freq = F, nclass = 20, xlim = c(-15, 15), main = &#39;Daily Stock Change&#39;) curve(dnorm(x, m, sd), add = T) . Ch.9 &#47196;&#48260;&#49828;&#53944; &#49440;&#54805;&#54924;&#44480; . set.seed(1234567) x &lt;- seq(1,10) y &lt;- 2.5 + 0.5*x + rnorm(10,0,1) y[10] &lt;- -10 par(mfrow=c(1,2)) plot(y~x) library(MASS) m0 &lt;- lm(y ~ x) m1 &lt;- rlm(y ~ x) m2 &lt;- lqs(y ~ x) plot(y ~ x) abline(m0$coef) abline(m1$coef, lty = &#39;dotted&#39;, col = &#39;blue&#39;) abline(m2$coef, lty = &#39;dotted&#39;, col = &#39;red&#39;) . Ch.10 &#51060;&#48320;&#47049; &#51088;&#47308; &#53456;&#49353; . par(mfrow = c(1,2)) library(MASS) data(geyser) hist(geyser$duration) . density(geyser$duration) . Call: density.default(x = geyser$duration) Data: geyser$duration (299 obs.); Bandwidth &#39;bw&#39; = 0.3304 x y Min. :-0.1578 Min. :0.0000454 1st Qu.: 1.4919 1st Qu.:0.0070880 Median : 3.1417 Median :0.0815875 Mean : 3.1417 Mean :0.1513899 3rd Qu.: 4.7914 3rd Qu.:0.2768205 Max. : 6.4411 Max. :0.5154276 . hist(geyser$duration, freq = F) lines(density(geyser$duration), lty = 2) . ker &lt;- function(n . geyser$duration %&gt;% length() . 299 1/299 * sum( . d &lt;- density(rnorm(10000)) . plot(d) . approx(d$x, d$y, xout = c(-2, 0, 2)) . $x &lt;ol class=list-inline&gt;-2 | 0 | 2 | &lt;/ol&gt; $y &lt;ol class=list-inline&gt;0.050900899026136 | 0.402491511167809 | 0.0592883888354947 | &lt;/ol&gt; d$x . &lt;ol class=list-inline&gt;-4.34657395650395 | -4.32943776420608 | -4.31230157190821 | -4.29516537961034 | -4.27802918731248 | -4.26089299501461 | -4.24375680271674 | -4.22662061041888 | -4.20948441812101 | -4.19234822582314 | -4.17521203352528 | -4.15807584122741 | -4.14093964892954 | -4.12380345663167 | -4.10666726433381 | -4.08953107203594 | -4.07239487973807 | -4.05525868744021 | -4.03812249514234 | -4.02098630284447 | -4.0038501105466 | -3.98671391824874 | -3.96957772595087 | -3.952441533653 | -3.93530534135514 | -3.91816914905727 | -3.9010329567594 | -3.88389676446153 | -3.86676057216367 | -3.8496243798658 | -3.83248818756793 | -3.81535199527007 | -3.7982158029722 | -3.78107961067433 | -3.76394341837646 | -3.7468072260786 | -3.72967103378073 | -3.71253484148286 | -3.695398649185 | -3.67826245688713 | -3.66112626458926 | -3.64399007229139 | -3.62685387999353 | -3.60971768769566 | -3.59258149539779 | -3.57544530309993 | -3.55830911080206 | -3.54117291850419 | -3.52403672620632 | -3.50690053390846 | -3.48976434161059 | -3.47262814931272 | -3.45549195701486 | -3.43835576471699 | -3.42121957241912 | -3.40408338012125 | -3.38694718782339 | -3.36981099552552 | -3.35267480322765 | -3.33553861092979 | -3.31840241863192 | -3.30126622633405 | -3.28413003403618 | -3.26699384173832 | -3.24985764944045 | -3.23272145714258 | -3.21558526484472 | -3.19844907254685 | -3.18131288024898 | -3.16417668795111 | -3.14704049565325 | -3.12990430335538 | -3.11276811105751 | -3.09563191875965 | -3.07849572646178 | -3.06135953416391 | -3.04422334186604 | -3.02708714956818 | -3.00995095727031 | -2.99281476497244 | -2.97567857267458 | -2.95854238037671 | -2.94140618807884 | -2.92426999578097 | -2.90713380348311 | -2.88999761118524 | -2.87286141888737 | -2.85572522658951 | -2.83858903429164 | -2.82145284199377 | -2.8043166496959 | -2.78718045739804 | -2.77004426510017 | -2.7529080728023 | -2.73577188050444 | -2.71863568820657 | -2.7014994959087 | -2.68436330361083 | -2.66722711131297 | -2.6500909190151 | -2.63295472671723 | -2.61581853441937 | -2.5986823421215 | -2.58154614982363 | -2.56440995752576 | -2.5472737652279 | -2.53013757293003 | -2.51300138063216 | -2.4958651883343 | -2.47872899603643 | -2.46159280373856 | -2.44445661144069 | -2.42732041914283 | -2.41018422684496 | -2.39304803454709 | -2.37591184224923 | -2.35877564995136 | -2.34163945765349 | -2.32450326535562 | -2.30736707305776 | -2.29023088075989 | -2.27309468846202 | -2.25595849616416 | -2.23882230386629 | -2.22168611156842 | -2.20454991927055 | -2.18741372697269 | -2.17027753467482 | -2.15314134237695 | -2.13600515007909 | -2.11886895778122 | -2.10173276548335 | -2.08459657318548 | -2.06746038088762 | -2.05032418858975 | -2.03318799629188 | -2.01605180399402 | -1.99891561169615 | -1.98177941939828 | -1.96464322710041 | -1.94750703480255 | -1.93037084250468 | -1.91323465020681 | -1.89609845790895 | -1.87896226561108 | -1.86182607331321 | -1.84468988101534 | -1.82755368871748 | -1.81041749641961 | -1.79328130412174 | -1.77614511182388 | -1.75900891952601 | -1.74187272722814 | -1.72473653493027 | -1.70760034263241 | -1.69046415033454 | -1.67332795803667 | -1.65619176573881 | -1.63905557344094 | -1.62191938114307 | -1.6047831888452 | -1.58764699654734 | -1.57051080424947 | -1.5533746119516 | -1.53623841965374 | -1.51910222735587 | -1.501966035058 | -1.48482984276013 | -1.46769365046227 | -1.4505574581644 | -1.43342126586653 | -1.41628507356867 | -1.3991488812708 | -1.38201268897293 | -1.36487649667506 | -1.3477403043772 | -1.33060411207933 | -1.31346791978146 | -1.2963317274836 | -1.27919553518573 | -1.26205934288786 | -1.24492315058999 | -1.22778695829213 | -1.21065076599426 | -1.19351457369639 | -1.17637838139853 | -1.15924218910066 | -1.14210599680279 | -1.12496980450492 | -1.10783361220706 | -1.09069741990919 | -1.07356122761132 | -1.05642503531346 | -1.03928884301559 | -1.02215265071772 | -1.00501645841985 | -0.987880266121987 | -0.97074407382412 | -0.953607881526253 | -0.936471689228386 | ⋯ | 0.999918040430601 | 1.01705423272847 | 1.03419042502633 | 1.0513266173242 | 1.06846280962207 | 1.08559900191994 | 1.1027351942178 | 1.11987138651567 | 1.13700757881354 | 1.15414377111141 | 1.17127996340927 | 1.18841615570714 | 1.20555234800501 | 1.22268854030287 | 1.23982473260074 | 1.25696092489861 | 1.27409711719648 | 1.29123330949434 | 1.30836950179221 | 1.32550569409008 | 1.34264188638794 | 1.35977807868581 | 1.37691427098368 | 1.39405046328154 | 1.41118665557941 | 1.42832284787728 | 1.44545904017515 | 1.46259523247301 | 1.47973142477088 | 1.49686761706875 | 1.51400380936662 | 1.53114000166448 | 1.54827619396235 | 1.56541238626022 | 1.58254857855808 | 1.59968477085595 | 1.61682096315382 | 1.63395715545168 | 1.65109334774955 | 1.66822954004742 | 1.68536573234529 | 1.70250192464315 | 1.71963811694102 | 1.73677430923889 | 1.75391050153675 | 1.77104669383462 | 1.78818288613249 | 1.80531907843036 | 1.82245527072822 | 1.83959146302609 | 1.85672765532396 | 1.87386384762183 | 1.89100003991969 | 1.90813623221756 | 1.92527242451543 | 1.94240861681329 | 1.95954480911116 | 1.97668100140903 | 1.99381719370689 | 2.01095338600476 | 2.02808957830263 | 2.0452257706005 | 2.06236196289836 | 2.07949815519623 | 2.0966343474941 | 2.11377053979196 | 2.13090673208983 | 2.1480429243877 | 2.16517911668557 | 2.18231530898343 | 2.1994515012813 | 2.21658769357917 | 2.23372388587703 | 2.2508600781749 | 2.26799627047277 | 2.28513246277064 | 2.3022686550685 | 2.31940484736637 | 2.33654103966424 | 2.3536772319621 | 2.37081342425997 | 2.38794961655784 | 2.40508580885571 | 2.42222200115357 | 2.43935819345144 | 2.45649438574931 | 2.47363057804717 | 2.49076677034504 | 2.50790296264291 | 2.52503915494078 | 2.54217534723864 | 2.55931153953651 | 2.57644773183438 | 2.59358392413224 | 2.61072011643011 | 2.62785630872798 | 2.64499250102585 | 2.66212869332371 | 2.67926488562158 | 2.69640107791945 | 2.71353727021731 | 2.73067346251518 | 2.74780965481305 | 2.76494584711092 | 2.78208203940878 | 2.79921823170665 | 2.81635442400452 | 2.83349061630238 | 2.85062680860025 | 2.86776300089812 | 2.88489919319599 | 2.90203538549385 | 2.91917157779172 | 2.93630777008959 | 2.95344396238745 | 2.97058015468532 | 2.98771634698319 | 3.00485253928106 | 3.02198873157892 | 3.03912492387679 | 3.05626111617466 | 3.07339730847252 | 3.09053350077039 | 3.10766969306826 | 3.12480588536613 | 3.14194207766399 | 3.15907826996186 | 3.17621446225973 | 3.19335065455759 | 3.21048684685546 | 3.22762303915333 | 3.2447592314512 | 3.26189542374906 | 3.27903161604693 | 3.2961678083448 | 3.31330400064266 | 3.33044019294053 | 3.3475763852384 | 3.36471257753627 | 3.38184876983413 | 3.398984962132 | 3.41612115442987 | 3.43325734672773 | 3.4503935390256 | 3.46752973132347 | 3.48466592362134 | 3.5018021159192 | 3.51893830821707 | 3.53607450051494 | 3.5532106928128 | 3.57034688511067 | 3.58748307740854 | 3.60461926970641 | 3.62175546200427 | 3.63889165430214 | 3.65602784660001 | 3.67316403889787 | 3.69030023119574 | 3.70743642349361 | 3.72457261579148 | 3.74170880808934 | 3.75884500038721 | 3.77598119268508 | 3.79311738498294 | 3.81025357728081 | 3.82738976957868 | 3.84452596187655 | 3.86166215417441 | 3.87879834647228 | 3.89593453877015 | 3.91307073106801 | 3.93020692336588 | 3.94734311566375 | 3.96447930796162 | 3.98161550025948 | 3.99875169255735 | 4.01588788485522 | 4.03302407715308 | 4.05016026945095 | 4.06729646174882 | 4.08443265404669 | 4.10156884634455 | 4.11870503864242 | 4.13584123094029 | 4.15297742323815 | 4.17011361553602 | 4.18724980783389 | 4.20438600013176 | 4.22152219242962 | 4.23865838472749 | 4.25579457702536 | 4.27293076932322 | 4.29006696162109 | 4.30720315391896 | 4.32433934621683 | 4.34147553851469 | 4.35861173081256 | 4.37574792311043 | 4.39288411540829 | 4.41002030770616 | &lt;/ol&gt; [^1] 로버스트(robust) 한 통계량은 이상치/에러값으로 부터 영향을 크게 받지 않는 (건장한) 통계량 .",
            "url": "https://stahangryum.github.io/Woo/eda/r/highlight/2022/04/12/eda_highlight.html",
            "relUrl": "/eda/r/highlight/2022/04/12/eda_highlight.html",
            "date": " • Apr 12, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "numpy Highlight",
            "content": "Reference . ref. https://numpy.org/doc/stable/user/index.html# . ref. https://github.com/guebin/IP2022 . NumPy . Numerical Python | . Import . import numpy as np . Array Creation(ndarray) . N Dimensional Array | . A = np.array([1,2,3]) A . array([1, 2, 3]) . mylist = [3, 4, 5] array_from_list = np.array(mylist) array_from_list . array([3, 4, 5]) . mytuple = (4,5,6) array_from_tuple = np.array(mytuple) array_from_tuple . array([4, 5, 6]) . np.array(range(10)) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.linspace(0,1,12) # 0 ~ 1을 12등분하여 만듬 (끝점을 포함) . array([0. , 0.09090909, 0.18181818, 0.27272727, 0.36363636, 0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182, 0.90909091, 1. ]) . len(np.linspace(0,1,12)) . 12 . np.arange(5) . array([0, 1, 2, 3, 4]) . np.arange(1,6) . array([1, 2, 3, 4, 5]) . np.zeros(3) # 0을 3개 . array([0., 0., 0.]) . np.zeros((3,3)) . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . np.ones(3) # 1을 3개 . array([1., 1., 1.]) . np.ones((3,3)) . array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) . np.eye(3) # 단위 행렬 . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.diag([1,2,3]) # 대각선이 1,2,3인 행렬 . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . Broadcasting . A+1 # 덧셈 . array([2, 3, 4]) . A-4 # 뺄셈 . array([-3, -2, -1]) . A*2 # 곱셈 . array([2, 4, 6]) . A/2 # 나눗셈 . array([0.5, 1. , 1.5]) . A**2 # 제곱 . array([1, 4, 9]) . A%2 # 나머지 . array([1, 0, 1], dtype=int32) . np.sqrt(A) . array([1. , 1.41421356, 1.73205081]) . np.log(A) . array([0. , 0.69314718, 1.09861229]) . np.exp(A) . array([ 2.71828183, 7.3890561 , 20.08553692]) . np.sin(A) . array([0.84147098, 0.90929743, 0.14112001]) . A = np.array([11,22,33,44,55,66]) . Indexing . A[2] . 33 . A[5] . 66 . A[1:4] . array([22, 33, 44]) . A[[0,2,4]] . array([11, 33, 55]) . A[[True, False, True, False, False, True]] . array([11, 33, 66]) . A &lt; 33 . array([ True, True, False, False, False, False]) . A[A&lt;33] . array([11, 22]) . Matrix Indexing . A2 = np.array([[1,2,3,4],[-1,-2,-3,-4],[5,6,7,8],[-5,-6,-7,-8]]) A2 . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[1][3] . -4 . A2[1,3] . -4 . A2[0, 0:2] . array([1, 2]) . A2[0] . array([1, 2, 3, 4]) . A2[0, 2:] . array([3, 4]) . A2[:, :] . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[[0,2], :] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . A2[[0,2]] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . Useful Functions . np.random . {python} np.random. . np.random.rand . {python} np.random.rand(N) . np.random.rand(10) # 0~1 사이에서 10개의 난수 생성 . array([0.74265347, 0.12865142, 0.9512475 , 0.24358922, 0.03553823, 0.2295192 , 0.12775839, 0.0919773 , 0.50947895, 0.74584544]) . np.random.rand(10)*2 # (0~1)*2 = 0~2 사이에서 10개의 난수 생성 . array([0.70426213, 1.54703597, 0.73553232, 1.35319966, 1.7224698 , 1.43787541, 1.62631319, 1.54416732, 0.3511027 , 1.63909984]) . np.random.rand(10)+1 # 1~2 사이에서 10개의 난수 생성 . array([1.8882947 , 1.21487037, 1.87248472, 1.8506064 , 1.42443757, 1.795348 , 1.37847909, 1.24870616, 1.66128972, 1.7608405 ]) . np.random.rand(10)*2+1 # 1~3 사이에서 10개의 난수 생성 . array([2.16303879, 1.27818637, 1.32514334, 1.6161346 , 2.03926784, 1.50032755, 1.16183896, 1.51923212, 2.67152899, 1.2616228 ]) . np.random.randn . {python} np.random.randn(N) . np.random.randn(10) # 표준정규분포에서 10개 추출 . array([ 0.16284596, -1.41505923, -0.87931282, -1.96742692, -0.17715718, -0.18035526, 1.31177136, -1.02100905, -0.3559429 , 0.40319735]) . np.random.randn(10)*2 # 평균이 0이고 표준편차가 2인 정규분포 . array([ 0.29806358, 2.00020956, -0.5111455 , -3.0789904 , 2.98176489, 3.77815177, -1.25610359, -1.54689973, -2.11675118, 0.5415075 ]) . np.random.randn(10)*2 + 3 # 평균이 0이고 표준편차가 3인 정규분포 . array([ 0.19911518, 2.68233421, 3.80413328, 2.60169535, 2.48309103, 5.28444139, 5.77762188, 5.52430879, -0.17405269, 3.34573411]) . np.random.randint . 중복을 허용하지 않고 정수를 생성한다. | . np.random.randint(7) # [0,7)의 범위에서 정수 한 개 생성 . 1 . np.random.randint(7, size = (20,)) . array([1, 0, 2, 1, 4, 4, 0, 2, 3, 1, 3, 5, 2, 5, 6, 5, 6, 1, 1, 2]) . np.random.randint(7, size = (2,2)) # [0,7)의 범위 무작위 정수가 원소인 2x2 행렬을 생성한다. . array([[2, 1], [4, 6]]) . np.random.randint(low=10, high=20, size=(2,5)) # [10, 20)의 범위에서 정수 한 개 생성 . array([[16, 14, 13, 13, 11], [12, 11, 18, 14, 15]]) . Warning :np.random.randint(high = N)은 사용할 수 없다. . np.random.choice . 복원추출이 default | . np.random.choice(5,20) . array([3, 2, 4, 2, 0, 1, 1, 2, 1, 4, 3, 3, 3, 1, 0, 0, 0, 0, 2, 2]) . np.random.choice([&#39;apple&#39;, &#39;orange&#39;, &#39;banana&#39;], 20) . array([&#39;banana&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;banana&#39;], dtype=&#39;&lt;U6&#39;) . np.random.choice(5, 2, replace = False) # Replace = False로 하면 비복원추출을 시행한다. . array([2, 3]) . np.random.binomial . np.random.binomial(n=10, p=0.2, size = (5,)) . array([2, 3, 3, 2, 2]) . np.random.normal . {python} np.random.normal(loc = mean, scale = stdev, size) default : np.random.normal(loc = 0, scale = 1, size = None) . np.random.normal(2, 3, 10) # 평균이 2이고 표준편차가 3인 정규분포 . array([-2.2383093 , 4.8091005 , 0.15352256, 4.9793976 , 0.33729707, 6.12970338, 5.18818041, -1.62767898, -1.60791145, 0.97458435]) . np.random.uniform . np.random.uniform(low=2, high=4, size = (5,)) # 균등분포 . array([2.51568342, 2.3858703 , 2.42784566, 3.65082018, 2.5756164 ]) . np.random.poisson(lam=5, size=(5,)) . array([2, 5, 4, 3, 3]) . np.random.poisson . np.random.poisson(lam=5, size=(5,)) . array([3, 7, 3, 4, 4]) . .corrcoef . np.random.seed(43052) x= np.random.randn(10000) y= np.random.randn(10000)*2 z= np.random.randn(10000)*0.5 np.corrcoef([x,y,z]).round(2) . array([[ 1. , -0.01, 0.01], [-0.01, 1. , 0. ], [ 0.01, 0. , 1. ]]) . .cov . np.cov([x,y,z]).round(2) . array([[ 0.99, -0.02, 0. ], [-0.02, 4.06, 0. ], [ 0. , 0. , 0.25]]) . .reshape . . Tip: R의 dim 함수와 유사하다. . A = np.array([11,22,33,44,55,66]) A . array([11, 22, 33, 44, 55, 66]) . A.reshape(2,3) . array([[11, 22, 33], [44, 55, 66]]) . A . array([11, 22, 33, 44, 55, 66]) . A = A.reshape(2,3) A . array([[11, 22, 33], [44, 55, 66]]) . note :reshape with -1 . A = np.arange(24) A . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . A.reshape(2,-1) # 행의 수가 2인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . A.reshape(4, -1) # 행의 수가 4인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . A.reshape(-1, 4) # 열의 수가 4인 행렬, 행은 알아서 맞춰 . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . A.reshape(-1) # 다시 길이가 24인 벡터로 . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . .shape . A = np.array(3.14) # Scalar, 0d array A.shape . () . A = np.array([3.14]) # Vector, 1d array A.shape . (1,) . A = np.array([[3.14]]) # Matrix, 2d array A.shape . (1, 1) . A = np.array([[[3.14]]]) # Tensor, 3d array A.shape . (1, 1, 1) . A = np.array([[1,2,3],[2,5,6],[4,4,2]]) A.shape . (3, 3) . .T . A = np.arange(4).reshape(2,2) A . array([[0, 1], [2, 3]]) . A.T # 전치행렬 . array([[0, 2], [1, 3]]) . np.linalg.inv . np.linalg.inv(A) # 역행렬 . array([[-1.5, 0.5], [ 1. , 0. ]]) . @ . A @ np.linalg.inv(A) . array([[1., 0.], [0., 1.]]) . np.concatenate . A = np.array([1,2]) B = np.array([4,5]) np.concatenate([A, B]) . array([1, 2, 4, 5]) . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B]) # axis = 0이 생략되어 있다. . array([[ 0, 1], [ 2, 3], [10, 11], [12, 13]]) . A=np.array(range(4)).reshape(2,2) B=np.array(range(2)).reshape(2,1) np.concatenate([a,b],axis=1) # 꼭 같은 차원일 필요는 없고, 붙여지는 부분의 길이만 같으면 됨 . NameError Traceback (most recent call last) Input In [82], in &lt;cell line: 3&gt;() 1 A=np.array(range(4)).reshape(2,2) 2 B=np.array(range(2)).reshape(2,1) -&gt; 3 np.concatenate([a,b],axis=1) NameError: name &#39;a&#39; is not defined . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B], axis=1) . A = np.array(range(2*3*4)).reshape(2,3,4) B = - A A, B . np.concatenate([A,B], axis=0) . np.concatenate([A,B], axis=1) . np.concatenate([A,B], axis=2) . np.stack . Warning : . A = np.array([1,2,3]) B = np.array([2,3,4]) np.concatenate([A,B], axis = 1) . AxisError Traceback (most recent call last) Input In [83], in &lt;cell line: 3&gt;() 1 A = np.array([1,2,3]) 2 B = np.array([2,3,4]) -&gt; 3 np.concatenate([A,B], axis = 1) File &lt;__array_function__ internals&gt;:180, in concatenate(*args, **kwargs) AxisError: axis 1 is out of bounds for array of dimension 1 . A = np.array([1,2,3]) B = np.array([2,3,4]) np.stack([A,B], axis=0) . array([[1, 2, 3], [2, 3, 4]]) . np.stack([A,B], axis=1) . array([[1, 2], [2, 3], [3, 4]]) . A = np.arange(3*4*5).reshape(3,4,5) B = - A A.shape, B.shape . ((3, 4, 5), (3, 4, 5)) . np.stack([A,B], axis = 0) . array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [-10, -11, -12, -13, -14], [-15, -16, -17, -18, -19]], [[-20, -21, -22, -23, -24], [-25, -26, -27, -28, -29], [-30, -31, -32, -33, -34], [-35, -36, -37, -38, -39]], [[-40, -41, -42, -43, -44], [-45, -46, -47, -48, -49], [-50, -51, -52, -53, -54], [-55, -56, -57, -58, -59]]]]) . . np.stack([A,B], axis = 0).shape # axis = 0 &lt;==&gt; axis = -4 . (2, 3, 4, 5) . np.stack([A,B], axis = 1).shape # axis = 1 &lt;==&gt; axis = -3 . (3, 2, 4, 5) . np.stack([A,B], axis = 2).shape # axis = 2 &lt;==&gt; axis = -2 . (3, 4, 2, 5) . np.stack([A,B], axis = 3).shape # axis = 3 &lt;==&gt; axis = -1 . (3, 4, 5, 2) . Difference between np.concatenate and np.stack . np.concatenate는 축의 총 개수를 유지하면서 결합한다. . np.stack은 축의 개수를 하나 증가시키면서 결합한다. . np.vstack . A = np.arange(6) B = - A . np.vstack([A, B]) . array([[ 0, 1, 2, 3, 4, 5], [ 0, -1, -2, -3, -4, -5]]) . np.hstack . A = np.arange(6) B = - A . np.hstack([A, B]) . array([ 0, 1, 2, 3, 4, 5, 0, -1, -2, -3, -4, -5]) . np.append . A = np.arange(30).reshape(5,6) B = - np.arange(8).reshape(2,2,2) A.shape, B.shape . ((5, 6), (2, 2, 2)) . np.append(A, B) # 다 풀어서 더한다. . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 0, -1, -2, -3, -4, -5, -6, -7]) . A = np.arange(2*3*4).reshape(2,3,4) B = - A A, B . (array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]), array([[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]])) . A.shape, B.shape, np.append(A, B, axis = 0).shape . ((2, 3, 4), (2, 3, 4), (4, 3, 4)) . np.append(A, B, axis = 0) . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]]) . np.diff . A = np.array([1,2,4,6,7]) np.diff(A) . array([1, 2, 2, 1]) . np.diff(np.diff(A)) . array([ 1, 0, -1]) . np.diff(A, prepend=100) # np.diff(np.array([100] + A.tolist()) ), 즉 맨 앞에 100을 추가하여 차분 . array([-99, 1, 2, 2, 1]) . np.diff(A, append=100) # np.diff(np.array(A.tolist() + [100]) ), 즉 맨 뒤에 100을 추가하여 차분 . array([ 1, 2, 2, 1, 93]) . A = np.arange(24).reshape(4,6) A . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . np.diff(A, axis = 0) . array([[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6]]) . np.diff(A, axis = 1) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . np.where . A = np.array([0,0,0,1,0]) np.where(A==1) # 조건을 만족하는 인덱스를 반환 . (array([3], dtype=int64),) . np.random.seed(43052) A = np.random.randn(12).reshape(3,4) np.where(A&lt;0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 . (array([1, 1, 2, 2, 2], dtype=int64), array([2, 3, 0, 1, 3], dtype=int64)) . A[np.where(A&lt;0)] . array([-1.66307542, -1.38277318, -1.92684484, -1.4862163 , -0.03488725]) . np.where(A&lt;0, 0, A) # 조건이 True이면 0, False이면 A . array([[0.38342049, 1.0841745 , 1.14277825, 0.30789368], [0.23778744, 0.35595116, 0. , 0. ], [0. , 0. , 0.00692519, 0. ]]) . np.where(A&lt;0, 5, 1) # 조건이 True이면 0, False이면 1 . array([[1, 1, 1, 1], [1, 1, 5, 5], [5, 5, 1, 5]]) . np.argwhere . A = np.array([0,0,0,1,0]) np.argwhere(A==1) . array([[3]], dtype=int64) . np.random.seed(43052) A = np.random.randn(12).reshape(3,4) np.argwhere(A&lt;0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 . array([[1, 2], [1, 3], [2, 0], [2, 1], [2, 3]], dtype=int64) . A[np.argwhere(A&lt;0)] # 불가능 . IndexError Traceback (most recent call last) Input In [106], in &lt;cell line: 1&gt;() -&gt; 1 A[np.argwhere(A&lt;0)] IndexError: index 3 is out of bounds for axis 0 with size 3 . Difference between np.where and np.argwhere . np.where는 인덱스의 좌표를 읽는 가독성이 떨어지지만 조건에 맞는 원소를 출력하거나 처리하기에 좋다. . np.argwhere는 인덱스의 좌표를 읽는 가독성은 좋지만 조건에 맞는 원소를 출력하거나 처리하기에 좋지 못하다. . np.ix_ . A = np.arange(12).reshape(3,4) A . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . A[[0,1], [0,1]]이 A[0:2, 0:2]를 의미하게 하려면 아레와 같이 np.ix_를 사용한다. | . A[np.ix_([0,1],[0,1])] . array([[0, 1], [4, 5]]) . Operations . .sum . A = np.array([1,2,3]) A.sum() . 6 . .mean . A = np.array([1,2,3]) A.mean() . 2.0 . .min . A = np.array([1,2,3]) A.min() . 1 . .max . A = np.array([1,2,3]) A.max() . 3 . .prod . A = np.array([1,2,3]) A.prod() . 6 . .std . A = np.arange(1,20) A.std() # 분포를 n으로 나누는 것이 default이다. . 5.477225575051661 . A = A = np.array([1,2,3]) A.std(ddof=1) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.0 . A = A = np.array([1,2,3]) A.std(ddof=2) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.4142135623730951 . .argmin . A = np.array([1,2,3]) # 가장 작은 값의 인덱스를 리턴 A.argmin() . 0 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmin(axis = 0) . array([2, 1, 1, 1, 2], dtype=int64) . A.argmin(axis = 1) . array([4, 3, 4, 3], dtype=int64) . .argmax . A = np.array([1,2,3]) # 가장 큰 값의 인덱스를 리턴 A.argmax() . 2 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmax(axis = 0) . array([3, 0, 0, 2, 0], dtype=int64) . A.argmax(axis = 1) . array([2, 0, 3, 0], dtype=int64) . .cumsum . A = np.array([1,2,3,4]) A.cumsum() # 누적합 . array([ 1, 3, 6, 10]) . .cumprod . A = np.array([1,2,3,4]) A.cumprod() # 누적곱 . array([ 1, 2, 6, 24]) . A = np.array([2**i for i in np.arange(10)]) A . array([ 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]) . A.cumprod() . array([ 1, 2, 8, 64, 1024, 32768, 2097152, 268435456, 0, 0]) . Applications . Solving System of Equations . $ begin{cases} y+z+w = 3 x+z+w = 3 x+y+w = 3 x+y+z = 3 end{cases}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . v = &#39;x&#39;, &#39;y&#39;, &#39;z&#39; A = np.linalg.inv(np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])) B = np.array([3,3,3,3]).reshape(4,1) answer = A@B.reshape(-1) for v, i in zip(v, answer): print(v, &#39;:&#39;, i) . x : 1.0 y : 1.0 z : 1.0 .",
            "url": "https://stahangryum.github.io/Woo/python/highlight/2022/04/11/numpy_highlight.html",
            "relUrl": "/python/highlight/2022/04/11/numpy_highlight.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Pima Indians Diabetes Prediction",
            "content": "Reference . ref. https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database . Pima Indians Diabetes Prediction . Variable Definition . Pregnancies | 임신 횟수 | . Glucose | 포도당 부하 검사 수치 | . BloodPressure | 혈압 | . SkinThickness | 팔 삼두근 뒤쪽의 피하지방 측정값(mm) | . Inlulin | 혈청 인슐린(mu U/ml) | . BMI | 체질량지수$( frac{kg}{m^2})$ | . DiabetesPredigreeFunction | 당뇨 내력 가중치 값 | . Age | 나이 | . Outcome | 클래스 결정 값(0 또는 1) | . Packages . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import Binarizer import warnings warnings.filterwarnings(action=&#39;ignore&#39;) . Preprocessing . diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) diabetes_data.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes_data.Outcome.value_counts() . 0 500 1 268 Name: Outcome, dtype: int64 . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . Glucose, BloodPressure, SkinThickness, Insulin, Bmi은 0이면 안된다. | . zero_features = [&#39;Glucose&#39;, &#39;BloodPressure&#39;, &#39;SkinThickness&#39;, &#39;Insulin&#39;, &#39;BMI&#39;] for i, feature in enumerate(zero_features): plt.subplot(3,2,i+1) plt.hist(diabetes_data[feature]) . diabetes_data[diabetes_data[&#39;Glucose&#39;] == 0][&#39;Glucose&#39;].count() . 5 . for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature] == 0][feature].count() print(&#39;{0} 0 건수는 {1}, 퍼센트는 {2:.2f}%&#39;.format(feature, zero_count, 100*zero_count / diabetes_data[feature].count())) . Glucose 0 건수는 5, 퍼센트는 0.65% BloodPressure 0 건수는 35, 퍼센트는 4.56% SkinThickness 0 건수는 227, 퍼센트는 29.56% Insulin 0 건수는 374, 퍼센트는 48.70% BMI 0 건수는 11, 퍼센트는 1.43% . SkinThickness, Insulin feature가 0인 행을 지우면 데이터 손실이 너무 크므로 평균값으로 대체한다. . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features] = diabetes_data[zero_features].replace(0, mean_zero_features) . Prediction . def precision_recall_curve_plot(y_test, pred_proba_c1): precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1) plt.figure(figsize=(8,6)) threshold_boundary = thresholds.shape[0] plt.plot(thresholds, precisions[0:threshold_boundary], linestyle = &#39;-&#39;, label = &#39;precision&#39;) plt.plot(thresholds, recalls[0:threshold_boundary], label = &#39;recall&#39;) start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1), 2)) plt.xlabel(&#39;Threshold value&#39;); plt.ylabel(&#39;Precision and Recall value&#39;) plt.legend(); plt.grid() plt.show() def get_clf_eval(y_test, pred=None, pred_proba=None): # 모델 평가 함수 confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) f1 = f1_score(y_test, pred) roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) print(&#39;정확도 : {0:.3f}, 정밀도 : {1:.3f}, 재현율 : {2:.3f}, F1 : {3:.3f}, AUC : {4:.3f}&#39;.format( accuracy, precision, recall, f1, roc_auc)) def get_eval_by_threshold(y_test, pred_proba_c1, thresholds): for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_predict = binarizer.transform(pred_proba_c1) print(&#39;-&#39;) print(&#39;임계값:&#39;, round(custom_threshold,2)) get_clf_eval(y_test, custom_predict, pred_proba_c1) . . feature_name = diabetes_data.columns[:-1] target_name = diabetes_data.columns[-1] X = diabetes_data.loc[:, feature_name] y = diabetes_data.loc[:, target_name] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y) . lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba = lr_clf.predict_proba(X_test)[:, 1] get_clf_eval(y_test, pred, pred_proba) . 오차 행렬 [[90 10] [21 33]] 정확도 : 0.799, 정밀도 : 0.767, 재현율 : 0.611, F1 : 0.680, AUC : 0.845 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1] precision_recall_curve_plot(y_test, pred_proba_c1) . thresholds = np.arange(0.3, 0.5, 0.03) pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:, 1].reshape(-1,1), thresholds) . - 임계값: 0.3 오차 행렬 [[67 33] [11 43]] 정확도 : 0.714, 정밀도 : 0.566, 재현율 : 0.796, F1 : 0.662, AUC : 0.845 - 임계값: 0.33 오차 행렬 [[73 27] [12 42]] 정확도 : 0.747, 정밀도 : 0.609, 재현율 : 0.778, F1 : 0.683, AUC : 0.845 - 임계값: 0.36 오차 행렬 [[76 24] [15 39]] 정확도 : 0.747, 정밀도 : 0.619, 재현율 : 0.722, F1 : 0.667, AUC : 0.845 - 임계값: 0.39 오차 행렬 [[79 21] [17 37]] 정확도 : 0.753, 정밀도 : 0.638, 재현율 : 0.685, F1 : 0.661, AUC : 0.845 - 임계값: 0.42 오차 행렬 [[84 16] [18 36]] 정확도 : 0.779, 정밀도 : 0.692, 재현율 : 0.667, F1 : 0.679, AUC : 0.845 - 임계값: 0.45 오차 행렬 [[85 15] [18 36]] 정확도 : 0.786, 정밀도 : 0.706, 재현율 : 0.667, F1 : 0.686, AUC : 0.845 - 임계값: 0.48 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 . . binarizer = Binarizer(threshold=0.48) pred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) get_clf_eval(y_test, pred_th_048, pred_proba[:, 1]) . 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/07/pima.html",
            "relUrl": "/kaggle/2022/04/07/pima.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Titanic Survivor Prediction",
            "content": "Reference . ref. https://www.kaggle.com/c/titanic/ . Titanic Survivor Prediction . 타이타닉호 침몰 사고 당시 탑승자들의 정보를 활용하여 생존자를 예측하라. . Data Dictionary . Variable Definition Key . Survived | Survival | 0 = No, 1 = Yes | . Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . Sex | Sex | . Age | Age in years | . SibSp | # of siblings / spouses aboard the Titanic | . Parch | # of parents / children aboard the Titanic | . Ticket | Ticket number | . Fare | Passenger fare | . Cabin | Cabin number | . Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . Variable Notes . pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower . age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 . sibsp: The dataset defines family relations in this way... Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fiancés were ignored) . parch: The dataset defines family relations in this way... Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them. . Stage I . import . import numpy as np import pandas as pd . code . import os print(os.getcwd()) . C: Users godgk Desktop Project kaggle Titanic . train = pd.read_csv(&#39;data/train.csv&#39;) test = pd.read_csv(&#39;data/test.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . print(&#39;train data shape&#39;, train.shape) print(&#39;test data shape&#39;, test.shape) print(&#39;--[train infomation]--&#39;) print(train.info()) print(&#39;--[test infomation]--&#39;) print(test.info()) . train data shape (891, 12) test data shape (418, 11) --[train infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB None --[test infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB None . train.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . test.isnull().sum() . PassengerId 0 Pclass 0 Name 0 Sex 0 Age 86 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 . Stage II . import . import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set() . def pie_chart(feature): feature_ratio = train[feature].value_counts(sort=False) feature_size = feature_ratio.size feature_index = feature_ratio.index survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() plt.plot(aspect=&#39;auto&#39;) plt.pie(feature_ratio, labels=feature_index, autopct=&#39;%1.1f%%&#39;) plt.title(feature + &#39; &#39;s ratio in total&#39;) plt.show() for i, index in enumerate(feature_index): plt.subplot(1, feature_size + 1, i + 1, aspect=&#39;equal&#39;) plt.pie([survived[index], dead[index]], labels=[&#39;Survivied&#39;, &#39;Dead&#39;], autopct=&#39;%1.1f%%&#39;) plt.title(str(index) + &#39; &#39;s ratio&#39;) plt.show() . pie_chart(&quot;Sex&quot;) . 남성 탑승객이 여성 탑승객보다 많다. . | 여성 탑승객의 생존 비율이 남성 탑승객보다 높다. . | . pie_chart(&quot;Pclass&quot;) . 1등실 2등실 3등실 순으로 생존 비율이 높다. | . pie_chart(&quot;Embarked&quot;) . train[&#39;Ticket&#39;][0:50] . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 5 330877 6 17463 7 349909 8 347742 9 237736 10 PP 9549 11 113783 12 A/5. 2151 13 347082 14 350406 15 248706 16 382652 17 244373 18 345763 19 2649 20 239865 21 248698 22 330923 23 113788 24 349909 25 347077 26 2631 27 19950 28 330959 29 349216 30 PC 17601 31 PC 17569 32 335677 33 C.A. 24579 34 PC 17604 35 113789 36 2677 37 A./5. 2152 38 345764 39 2651 40 7546 41 11668 42 349253 43 SC/Paris 2123 44 330958 45 S.C./A.4. 23567 46 370371 47 14311 48 2662 49 349237 Name: Ticket, dtype: object . train.Ticket . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 ... 886 211536 887 112053 888 W./C. 6607 889 111369 890 370376 Name: Ticket, Length: 891, dtype: object . Stage 3 . def bar_chart(feature): survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() df = pd.DataFrame([survived, dead]) df.index = [&#39;Survived&#39;, &#39;Dead&#39;] df.plot(kind=&#39;bar&#39;, stacked=True, figsize=(10,5)) . bar_chart(&quot;SibSp&quot;) . bar_chart(&quot;Parch&quot;) . Data Preprocessing . train_and_test = [train, test] . Name Feature . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | Mr | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | Mrs | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | Miss | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | Mrs | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | Mr | . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Capt&#39;, &#39;Col&#39;, &#39;Countess&#39;, &#39;Don&#39;,&#39;Dona&#39;, &#39;Dr&#39;, &#39;Jonkheer&#39;,&#39;Lady&#39;,&#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;], &#39;Other&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Mlle&#39;, &#39;Ms&#39;], &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Master 0 | 40 | . Miss 185 | 0 | . Mr 0 | 517 | . Mrs 126 | 0 | . Other 3 | 20 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;).mean() . Survived . Title . Master 0.575000 | . Miss 0.702703 | . Mr 0.156673 | . Mrs 0.793651 | . Other 0.347826 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;, as_index = False).mean() # as_index = True이면 Title이 index로 작용한다. . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Other | 0.347826 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].astype(str) . Sex Feature . for dataset in train_and_test: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].astype(str) . Embarked Feature . train.Embarked.value_counts(dropna=False) . S 644 C 168 Q 77 NaN 2 Name: Embarked, dtype: int64 . for dataset in train_and_test: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(&#39;S&#39;) dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].astype(str) . Age Feature . Binning . train.Age.isna().sum() . 177 . for dataset in train_and_test: dataset[&#39;Age&#39;].fillna(dataset[&#39;Age&#39;].mean(), inplace=True) dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].astype(int) train[&#39;AgeBand&#39;] = pd.cut(train[&#39;Age&#39;], 5) train[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;], as_index=False).mean() . AgeBand Survived . 0 (-0.08, 16.0] | 0.550000 | . 1 (16.0, 32.0] | 0.344762 | . 2 (32.0, 48.0] | 0.403226 | . 3 (48.0, 64.0] | 0.434783 | . 4 (64.0, 80.0] | 0.090909 | . for dataset in train_and_test: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] = 4 dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].map( { 0:&#39;Child&#39;, 1:&#39;Young&#39;, 2:&#39;Middle&#39;, 3:&#39;Prime&#39;, 4:&#39;Old&#39; } ).astype(str) . Fare Feature . for dataset in train_and_test: print(dataset[&#39;Fare&#39;].isna().sum()) . 0 1 . train[[&#39;Pclass&#39;, &#39;Fare&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean() . Pclass Fare . 0 1 | 84.154687 | . 1 2 | 20.662183 | . 2 3 | 13.675550 | . test[test[&#39;Fare&#39;].isna()][&#39;Pclass&#39;] . 152 3 Name: Pclass, dtype: int64 . for dataset in train_and_test: dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].fillna(13.675) # Pclass가 3인 승객의 평균 Fare . train[&#39;FareBand&#39;] = pd.qcut(train[&#39;Fare&#39;], 5) for dataset in train_and_test: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.854, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.854) &amp; (dataset[&#39;Fare&#39;] &lt;= 10.5), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 10.5) &amp; (dataset[&#39;Fare&#39;] &lt;= 21.679), &#39;Fare&#39;] = 2 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 21.679) &amp; (dataset[&#39;Fare&#39;] &lt;= 39.688), &#39;Fare&#39;] = 3 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 39.688, &#39;Fare&#39;] = 4 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].map( { 0:&#39;XS&#39;, 1:&#39;S&#39;, 2:&#39;M&#39;, 3:&#39;L&#39;, 4:&#39;XL&#39; } ).astype(str) . SibSp &amp; Parch Feature (Family) . for dataset in train_and_test: dataset[&#39;Family&#39;] = dataset[&#39;Parch&#39;] + dataset[&#39;SibSp&#39;] dataset[&#39;Family&#39;] = dataset[&#39;Family&#39;].astype(int) . Other Feature . features_drop = [&#39;Name&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;SibSp&#39;, &#39;Parch&#39;] train = train.drop(features_drop, axis = 1) test = test.drop(features_drop, axis = 1) train = train.drop([&#39;PassengerId&#39;, &#39;AgeBand&#39;, &#39;FareBand&#39;], axis = 1) . train.head() . Survived Pclass Sex Age Fare Embarked Title Family . 0 0 | 3 | male | Young | XS | S | Mr | 1 | . 1 1 | 1 | female | Middle | XL | C | Mrs | 1 | . 2 1 | 3 | female | Young | S | S | Miss | 0 | . 3 1 | 1 | female | Middle | XL | S | Mrs | 1 | . 4 0 | 3 | male | Middle | S | S | Mr | 0 | . test.head() . PassengerId Pclass Sex Age Fare Embarked Title Family . 0 892 | 3 | male | Middle | XS | Q | Mr | 0 | . 1 893 | 3 | female | Middle | XS | S | Mrs | 1 | . 2 894 | 2 | male | Prime | S | Q | Mr | 0 | . 3 895 | 3 | male | Young | S | S | Mr | 0 | . 4 896 | 3 | female | Young | M | S | Mrs | 2 | . train = pd.get_dummies(train) test = pd.get_dummies(test) train_label = train[&#39;Survived&#39;] train_data = train.drop(&#39;Survived&#39;, axis = 1) test_data = test.drop(&#39;PassengerId&#39;, axis = 1).copy() . print(train_data.shape, train_label.shape, test_data.shape) . (891, 22) (891,) (418, 22) . train . Survived Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 0 | 3 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 1 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 3 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 4 0 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 887 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 888 0 | 3 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 889 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 890 0 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 891 rows × 23 columns . test . PassengerId Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 892 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 893 | 3 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 2 894 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 3 895 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 4 896 | 3 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 414 1306 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 415 1307 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 416 1308 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 417 1309 | 3 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 418 rows × 23 columns . Learning . import . !pip install scikit-learn . Requirement already satisfied: scikit-learn in c: users godgk anaconda3 envs py39r40 lib site-packages (1.0.2) Requirement already satisfied: numpy&gt;=1.14.6 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.20.3) Requirement already satisfied: scipy&gt;=1.1.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.7.1) Requirement already satisfied: joblib&gt;=0.11 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (3.1.0) . from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn.utils import shuffle . train_data, train_label = shuffle(train_data, train_label, random_state = 5) . def train_and_test(model): model.fit(train_data, train_label) prediction = model.predict(test_data) accuracy = round(model.score(train_data, train_label) * 100, 2) print(&quot;Accuracy : &quot;, accuracy, &quot;%&quot;) return prediction . log_pred = train_and_test(LogisticRegression()) # SVM svm_pred = train_and_test(SVC()) #kNN knn_pred_4 = train_and_test(KNeighborsClassifier(n_neighbors = 4)) # Random Forest rf_pred = train_and_test(RandomForestClassifier(n_estimators=100)) # Navie Bayes nb_pred = train_and_test(GaussianNB()) . Accuracy : 82.27 % Accuracy : 83.61 % Accuracy : 84.74 % Accuracy : 88.55 % Accuracy : 79.35 % . submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: rf_pred }) submission.to_csv(&#39;submission_rf.csv&#39;, index=False) .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/06/Titanic.html",
            "relUrl": "/kaggle/2022/04/06/Titanic.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Initial Settings",
            "content": "작성중 | . Window Setting . https://www.youtube.com/watch?v=SgqgBPx8zEE . Anaconda Setting . https://www.anaconda.com/products/individual . conda create -n py39r41 python=3.9 . | conda activate py39r41 . | conda install -c conda-forge r-essentials=4.1 . | conda install -c conda-forge jupyterlab . | R . | install.packages(&quot;IRkernel&quot;) . | library(IRkernel) . | installspec() . | q() . | pip install numpy . | pip install pandas . | pip install SciPy . | pip install matplotlib . | conda install -c conda-forge rise . | etc . | . . Git Setting . Git . | Git Bash . | GitHub Desktop . | . !git add . !git commit -m . !git push .",
            "url": "https://stahangryum.github.io/Woo/2022/03/15/settings.html",
            "relUrl": "/2022/03/15/settings.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Monte Carlo Integration with Python",
            "content": "Monte Carlo Integration . Problem . Find $ int_0^1 (x + sin( pi x)) ,dx$. . Solution I - Analytic Sol . $ int_0^1 (x + sin( pi x)) ,dx = left[ cfrac{1}{2} x^2- cfrac{1}{ pi}cos( pi x) right]_0^1 = cfrac{1}{2} + cfrac{1}{ pi} + cfrac{1}{ pi} = cfrac{1}{2}+ cfrac{2}{ pi}$ . Solution II - Monte Carlo Integration in Python . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . def function(x): return x + np.sin(np.pi*x) x = np.linspace(0, 1, 10000) y = [function(x) for x in x] plt.plot(x, y) plt.show() . def function(x): #함수 정의 return x + np.sin(np.pi*x) N = 5000 # Random Sampling 시행 횟수 width = 1 # 사각형의 가로 길이 height = 1.6 #사각형의 세로 길이 X = np.random.random(N) # 0~1까지의 x 좌표 Random Sampling을 N번 시행 rand_Y = height * np.random.random(N) # 그래프상 최솟값 ~ 최댓값까지의 y 좌표 Randon Sampling을 N번 시행 in_or_out = rand_Y &lt; function(X) # rand_Y &lt; FX (IN)이면 True, Y &gt; F (OUT)이면 False A = height * width * np.sum(in_or_out) / N # 영역 S의 넓이 print(&#39;이론적으로 구한 값 : {0} n샘플링으로 구한 값 : {1}&#39;.format(1/2 + 2/np.pi, A)) . 이론적으로 구한 값 : 1.1366197723675815 샘플링으로 구한 값 : 1.12864 . Visualization . from matplotlib.animation import FuncAnimation . color = list(map(lambda x: &#39;blue&#39; if x == True else &#39;red&#39;, in_or_out)) #색 정하기 x = np.linspace(0, 1, 10000) #함수 그리기 y = [function(x) for x in x] plt.plot(x, y, color = &#39;black&#39;) plt.scatter(X, rand_Y, color = color, s=1, label=&#39;A = {0}&#39;.format(np.round(A, 4))) plt.legend(loc = &#39;lower right&#39;) #범례(legend) 위치 plt.plot([0, width], [0, 0], color=&#39;black&#39;) # 사각형 영역 plt.plot([width, width], [0, height], color=&#39;black&#39;) plt.plot([0, width], [height, height], color=&#39;black&#39;) plt.plot([0, 0], [0, height], color=&#39;black&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() .",
            "url": "https://stahangryum.github.io/Woo/python/2021/10/03/monte_carlo.html",
            "relUrl": "/python/2021/10/03/monte_carlo.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Taylor and Maclaurin Series",
            "content": "Taylor Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(a)}{n!}(x-a)^n qquad quad= f(a) + cfrac{f&#39;(a)}{1!}(x-a) + cfrac{f&#39;&#39;(a)}{2!}(x-a) + cfrac{f&#39;&#39;&#39;(a)}{3!}(x-a)+ cdots $ . Maclaurin Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(0)}{n!}(x-0)^n qquad quad = f(a) + cfrac{f^{ prime}(0)}{1!}(x-0) + cfrac{f^{ prime prime}(0)}{2!}(x-0) + cfrac{f^{ prime prime prime}(0)}{3!}(x-0)+ cdots $ . Examples . $ cfrac{1}{1-x} = sum_{n=0}^ infty{x^n} = 1+x+x^2+x^3+ cdots qquad R = 1 , e^x = sum_{n=0}^ infty cfrac{x^n}{n!} = 1 + cfrac{x}{1!} + cfrac{x^2}{2!} + cfrac{x^3}{3!}+ cdots qquad R = infty , sin ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)!} = x - cfrac{x^3}{3!} + cfrac{x^5}{5!} - cfrac{x^7}{7!}+ cdots qquad R = infty , cos ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n}}{(2n)!} = 1 - cfrac{x^2}{2!} + cfrac{x^4}{4!} - cfrac{x^6}{6!}+ cdots qquad R = infty , tan^{-1} ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)} = x - cfrac{x^3}{3} + cfrac{x^5}{5} - cfrac{x^7}{7}+ cdots qquad R = 1 , ln(1+x) = sum_{n=1}^ infty(-1)^{n-1} cfrac{x^{n}}{n} = x - cfrac{x^2}{2} + cfrac{x^3}{3} - cfrac{x^4}{4}+ cdots qquad R = 1 $ .",
            "url": "https://stahangryum.github.io/Woo/mathematics/2021/10/02/taylor.html",
            "relUrl": "/mathematics/2021/10/02/taylor.html",
            "date": " • Oct 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Kim Jeewoo . Jeonbuk Nation University. Statistics . GitHub . | LinkedIn . | Blog . | . . Contact . stahangryum@gmail.com .",
          "url": "https://stahangryum.github.io/Woo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stahangryum.github.io/Woo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}