{
  
    
        "post0": {
            "title": "Santander Customer Satisfaction",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib . cust_df = pd.read_csv(&#39;dataset/santander/train_santander.csv&#39;, encoding = &#39;latin-1&#39;) cust_df.head(3) . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . 0 1 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 39205.17 | 0 | . 1 3 | 2 | 34 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 49278.03 | 0 | . 2 4 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 67333.77 | 0 | . 3 rows × 371 columns . cust_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 76020 entries, 0 to 76019 Columns: 371 entries, ID to TARGET dtypes: float64(111), int64(260) memory usage: 215.2 MB . cust_df.shape . (76020, 371) . print(cust_df[&#39;TARGET&#39;].value_counts()) unsatisfied_cnt = cust_df[cust_df[&#39;TARGET&#39;] == 1].TARGET.count() total_cnt = cust_df.TARGET.count() print(&#39;unsatisfied 비율 :&#39;, np.round(unsatisfied_cnt / total_cnt, 3)) . 0 73012 1 3008 Name: TARGET, dtype: int64 unsatisfied 비율 : 0.04 . 불만족 비율은 전체의 4%에 불과하다. | . cust_df.describe() . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . count 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | ... | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 7.602000e+04 | 76020.000000 | . mean 75964.050723 | -1523.199277 | 33.212865 | 86.208265 | 72.363067 | 119.529632 | 3.559130 | 6.472698 | 0.412946 | 0.567352 | ... | 7.935824 | 1.365146 | 12.215580 | 8.784074 | 31.505324 | 1.858575 | 76.026165 | 56.614351 | 1.172358e+05 | 0.039569 | . std 43781.947379 | 39033.462364 | 12.956486 | 1614.757313 | 339.315831 | 546.266294 | 93.155749 | 153.737066 | 30.604864 | 36.513513 | ... | 455.887218 | 113.959637 | 783.207399 | 538.439211 | 2013.125393 | 147.786584 | 4040.337842 | 2852.579397 | 1.826646e+05 | 0.194945 | . min 1.000000 | -999999.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 5.163750e+03 | 0.000000 | . 25% 38104.750000 | 2.000000 | 23.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.787061e+04 | 0.000000 | . 50% 76043.000000 | 2.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.064092e+05 | 0.000000 | . 75% 113748.750000 | 2.000000 | 40.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.187563e+05 | 0.000000 | . max 151838.000000 | 238.000000 | 105.000000 | 210000.000000 | 12888.030000 | 21024.810000 | 8237.820000 | 11073.570000 | 6600.000000 | 6600.000000 | ... | 50003.880000 | 20385.720000 | 138831.630000 | 91778.730000 | 438329.220000 | 24650.010000 | 681462.900000 | 397884.300000 | 2.203474e+07 | 1.000000 | . 8 rows × 371 columns . cust_df[&#39;var3&#39;].replace(-999999, 2, inplace = True) cust_df.drop(&#39;ID&#39;, axis = 1, inplace = True) X_features = cust_df.iloc[:, :-1] y_labels = cust_df.iloc[:, -1] print(&#39;피쳐 데이터 shape :&#39;, X_features.shape) . 피쳐 데이터 shape : (76020, 369) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size = 0.2, random_state = 0) train_cnt = y_train.count() test_cnt = y_test.count() print(&#39;학습 세트 Shape : {0}, 테스트 세트 Shape : {1}&#39;.format(X_train.shape, X_test.shape)) print(&#39; 학습 세트 레이블 값 분포 비율&#39;) print(y_train.value_counts()/train_cnt) print(&#39; n 테스트 세트 레이블 값 분포 비율&#39;) print(y_test.value_counts()/test_cnt) . 학습 세트 Shape : (60816, 369), 테스트 세트 Shape : (15204, 369) 학습 세트 레이블 값 분포 비율 0 0.960964 1 0.039036 Name: TARGET, dtype: float64 테스트 세트 레이블 값 분포 비율 0 0.9583 1 0.0417 Name: TARGET, dtype: float64 . X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0) . from xgboost import XGBClassifier from sklearn.metrics import roc_auc_score xgb_clf = XGBClassifier(n_estimators = 500, random_state = 156) # 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행 xgb_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = &#39;logloss&#39;, eval_set = [(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average =&#39;macro&#39;) print(&#39;ROC AUC :&#39;, xgb_roc_score) . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-logloss:0.47251 validation_1-logloss:0.47431 [1] validation_0-logloss:0.35180 validation_1-logloss:0.35486 [2] validation_0-logloss:0.27740 validation_1-logloss:0.28183 [3] validation_0-logloss:0.22891 validation_1-logloss:0.23460 [4] validation_0-logloss:0.19656 validation_1-logloss:0.20328 [5] validation_0-logloss:0.17445 validation_1-logloss:0.18210 [6] validation_0-logloss:0.15934 validation_1-logloss:0.16792 [7] validation_0-logloss:0.14870 validation_1-logloss:0.15822 [8] validation_0-logloss:0.14125 validation_1-logloss:0.15182 [9] validation_0-logloss:0.13608 validation_1-logloss:0.14740 [10] validation_0-logloss:0.13231 validation_1-logloss:0.14457 [11] validation_0-logloss:0.12940 validation_1-logloss:0.14263 [12] validation_0-logloss:0.12715 validation_1-logloss:0.14133 [13] validation_0-logloss:0.12546 validation_1-logloss:0.14054 [14] validation_0-logloss:0.12434 validation_1-logloss:0.14000 [15] validation_0-logloss:0.12319 validation_1-logloss:0.13974 [16] validation_0-logloss:0.12237 validation_1-logloss:0.13947 [17] validation_0-logloss:0.12181 validation_1-logloss:0.13941 [18] validation_0-logloss:0.12139 validation_1-logloss:0.13942 [19] validation_0-logloss:0.12091 validation_1-logloss:0.13939 [20] validation_0-logloss:0.12018 validation_1-logloss:0.13936 [21] validation_0-logloss:0.11999 validation_1-logloss:0.13934 [22] validation_0-logloss:0.11947 validation_1-logloss:0.13933 [23] validation_0-logloss:0.11929 validation_1-logloss:0.13930 [24] validation_0-logloss:0.11876 validation_1-logloss:0.13931 [25] validation_0-logloss:0.11847 validation_1-logloss:0.13924 [26] validation_0-logloss:0.11811 validation_1-logloss:0.13900 [27] validation_0-logloss:0.11783 validation_1-logloss:0.13903 [28] validation_0-logloss:0.11766 validation_1-logloss:0.13908 [29] validation_0-logloss:0.11697 validation_1-logloss:0.13931 [30] validation_0-logloss:0.11663 validation_1-logloss:0.13943 [31] validation_0-logloss:0.11587 validation_1-logloss:0.13948 [32] validation_0-logloss:0.11521 validation_1-logloss:0.13967 [33] validation_0-logloss:0.11460 validation_1-logloss:0.13973 [34] validation_0-logloss:0.11446 validation_1-logloss:0.13971 [35] validation_0-logloss:0.11440 validation_1-logloss:0.13974 [36] validation_0-logloss:0.11385 validation_1-logloss:0.13981 [37] validation_0-logloss:0.11368 validation_1-logloss:0.13986 [38] validation_0-logloss:0.11361 validation_1-logloss:0.13986 [39] validation_0-logloss:0.11351 validation_1-logloss:0.13986 [40] validation_0-logloss:0.11331 validation_1-logloss:0.13998 [41] validation_0-logloss:0.11291 validation_1-logloss:0.14007 [42] validation_0-logloss:0.11281 validation_1-logloss:0.14013 [43] validation_0-logloss:0.11269 validation_1-logloss:0.14011 [44] validation_0-logloss:0.11255 validation_1-logloss:0.14015 [45] validation_0-logloss:0.11250 validation_1-logloss:0.14011 [46] validation_0-logloss:0.11246 validation_1-logloss:0.14014 [47] validation_0-logloss:0.11238 validation_1-logloss:0.14017 [48] validation_0-logloss:0.11220 validation_1-logloss:0.14014 [49] validation_0-logloss:0.11200 validation_1-logloss:0.14022 [50] validation_0-logloss:0.11190 validation_1-logloss:0.14028 [51] validation_0-logloss:0.11108 validation_1-logloss:0.14027 [52] validation_0-logloss:0.11041 validation_1-logloss:0.14031 [53] validation_0-logloss:0.10983 validation_1-logloss:0.14020 [54] validation_0-logloss:0.10961 validation_1-logloss:0.14015 [55] validation_0-logloss:0.10949 validation_1-logloss:0.14020 [56] validation_0-logloss:0.10889 validation_1-logloss:0.14026 [57] validation_0-logloss:0.10875 validation_1-logloss:0.14030 [58] validation_0-logloss:0.10831 validation_1-logloss:0.14025 [59] validation_0-logloss:0.10778 validation_1-logloss:0.14045 [60] validation_0-logloss:0.10756 validation_1-logloss:0.14051 [61] validation_0-logloss:0.10745 validation_1-logloss:0.14064 [62] validation_0-logloss:0.10702 validation_1-logloss:0.14070 [63] validation_0-logloss:0.10694 validation_1-logloss:0.14072 [64] validation_0-logloss:0.10689 validation_1-logloss:0.14072 [65] validation_0-logloss:0.10683 validation_1-logloss:0.14070 [66] validation_0-logloss:0.10654 validation_1-logloss:0.14073 [67] validation_0-logloss:0.10648 validation_1-logloss:0.14073 [68] validation_0-logloss:0.10628 validation_1-logloss:0.14087 [69] validation_0-logloss:0.10622 validation_1-logloss:0.14086 [70] validation_0-logloss:0.10616 validation_1-logloss:0.14091 [71] validation_0-logloss:0.10609 validation_1-logloss:0.14090 [72] validation_0-logloss:0.10581 validation_1-logloss:0.14109 [73] validation_0-logloss:0.10575 validation_1-logloss:0.14106 [74] validation_0-logloss:0.10562 validation_1-logloss:0.14110 [75] validation_0-logloss:0.10557 validation_1-logloss:0.14119 [76] validation_0-logloss:0.10490 validation_1-logloss:0.14114 [77] validation_0-logloss:0.10436 validation_1-logloss:0.14136 [78] validation_0-logloss:0.10396 validation_1-logloss:0.14137 [79] validation_0-logloss:0.10374 validation_1-logloss:0.14146 [80] validation_0-logloss:0.10347 validation_1-logloss:0.14155 [81] validation_0-logloss:0.10301 validation_1-logloss:0.14175 [82] validation_0-logloss:0.10283 validation_1-logloss:0.14162 [83] validation_0-logloss:0.10250 validation_1-logloss:0.14169 [84] validation_0-logloss:0.10239 validation_1-logloss:0.14176 [85] validation_0-logloss:0.10214 validation_1-logloss:0.14184 [86] validation_0-logloss:0.10211 validation_1-logloss:0.14185 [87] validation_0-logloss:0.10206 validation_1-logloss:0.14185 [88] validation_0-logloss:0.10188 validation_1-logloss:0.14194 [89] validation_0-logloss:0.10178 validation_1-logloss:0.14200 [90] validation_0-logloss:0.10174 validation_1-logloss:0.14204 [91] validation_0-logloss:0.10162 validation_1-logloss:0.14209 [92] validation_0-logloss:0.10159 validation_1-logloss:0.14209 [93] validation_0-logloss:0.10122 validation_1-logloss:0.14221 [94] validation_0-logloss:0.10118 validation_1-logloss:0.14222 [95] validation_0-logloss:0.10113 validation_1-logloss:0.14231 [96] validation_0-logloss:0.10082 validation_1-logloss:0.14250 [97] validation_0-logloss:0.10077 validation_1-logloss:0.14253 [98] validation_0-logloss:0.10035 validation_1-logloss:0.14265 [99] validation_0-logloss:0.09991 validation_1-logloss:0.14271 [100] validation_0-logloss:0.09964 validation_1-logloss:0.14277 [101] validation_0-logloss:0.09931 validation_1-logloss:0.14291 [102] validation_0-logloss:0.09928 validation_1-logloss:0.14291 [103] validation_0-logloss:0.09885 validation_1-logloss:0.14307 [104] validation_0-logloss:0.09873 validation_1-logloss:0.14323 [105] validation_0-logloss:0.09853 validation_1-logloss:0.14331 [106] validation_0-logloss:0.09799 validation_1-logloss:0.14359 [107] validation_0-logloss:0.09796 validation_1-logloss:0.14366 [108] validation_0-logloss:0.09782 validation_1-logloss:0.14385 [109] validation_0-logloss:0.09776 validation_1-logloss:0.14388 [110] validation_0-logloss:0.09756 validation_1-logloss:0.14400 [111] validation_0-logloss:0.09752 validation_1-logloss:0.14398 [112] validation_0-logloss:0.09728 validation_1-logloss:0.14405 [113] validation_0-logloss:0.09701 validation_1-logloss:0.14429 [114] validation_0-logloss:0.09689 validation_1-logloss:0.14426 [115] validation_0-logloss:0.09668 validation_1-logloss:0.14434 [116] validation_0-logloss:0.09663 validation_1-logloss:0.14439 [117] validation_0-logloss:0.09649 validation_1-logloss:0.14446 [118] validation_0-logloss:0.09641 validation_1-logloss:0.14450 [119] validation_0-logloss:0.09613 validation_1-logloss:0.14447 [120] validation_0-logloss:0.09606 validation_1-logloss:0.14455 [121] validation_0-logloss:0.09593 validation_1-logloss:0.14451 [122] validation_0-logloss:0.09566 validation_1-logloss:0.14469 [123] validation_0-logloss:0.09547 validation_1-logloss:0.14479 [124] validation_0-logloss:0.09542 validation_1-logloss:0.14476 [125] validation_0-logloss:0.09524 validation_1-logloss:0.14471 ROC AUC : 0.8410117370942843 . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 xgb_clf = XGBClassifier(n_estimators = 100) params = {&#39;max_depth&#39;:[5,7], &#39;min_child_weight&#39;:[1,3], &#39;colsample_bytree&#39;:[0.5, 0.75] } gridcv = GridSearchCV(xgb_clf, param_grid=params, cv = 3) # 2*2*2*3 gridcv.fit(X_train, y_train, early_stopping_rounds = 30, eval_metric = &#39;auc&#39;, eval_set=[(X_tr, y_tr), (X_val, y_val)]) print(&#39;GridSearchCV 최적 파라미터:&#39;, gridcv.best_params_) xgb_rod_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1], average = &#39;macro&#39;) print(&#39;ROC AUC :&#39;, xgb_roc_score) . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79161 validation_1-auc:0.79321 [1] validation_0-auc:0.81865 validation_1-auc:0.81375 [2] validation_0-auc:0.82586 validation_1-auc:0.81846 [3] validation_0-auc:0.82789 validation_1-auc:0.82226 [4] validation_0-auc:0.83249 validation_1-auc:0.82677 [5] validation_0-auc:0.83477 validation_1-auc:0.83225 [6] validation_0-auc:0.83340 validation_1-auc:0.82654 [7] validation_0-auc:0.84223 validation_1-auc:0.83486 [8] validation_0-auc:0.84586 validation_1-auc:0.83682 [9] validation_0-auc:0.84557 validation_1-auc:0.83472 [10] validation_0-auc:0.84423 validation_1-auc:0.83181 [11] validation_0-auc:0.84428 validation_1-auc:0.82920 [12] validation_0-auc:0.85176 validation_1-auc:0.83433 [13] validation_0-auc:0.85540 validation_1-auc:0.83565 [14] validation_0-auc:0.85718 validation_1-auc:0.83696 [15] validation_0-auc:0.85851 validation_1-auc:0.83561 [16] validation_0-auc:0.85964 validation_1-auc:0.83578 [17] validation_0-auc:0.86091 validation_1-auc:0.83570 [18] validation_0-auc:0.86188 validation_1-auc:0.83595 [19] validation_0-auc:0.86249 validation_1-auc:0.83552 [20] validation_0-auc:0.86298 validation_1-auc:0.83452 [21] validation_0-auc:0.86375 validation_1-auc:0.83437 [22] validation_0-auc:0.86440 validation_1-auc:0.83516 [23] validation_0-auc:0.86554 validation_1-auc:0.83470 [24] validation_0-auc:0.86601 validation_1-auc:0.83492 [25] validation_0-auc:0.86700 validation_1-auc:0.83510 [26] validation_0-auc:0.86770 validation_1-auc:0.83412 [27] validation_0-auc:0.86852 validation_1-auc:0.83394 [28] validation_0-auc:0.86898 validation_1-auc:0.83441 [29] validation_0-auc:0.86914 validation_1-auc:0.83440 [30] validation_0-auc:0.86953 validation_1-auc:0.83380 [31] validation_0-auc:0.87051 validation_1-auc:0.83346 [32] validation_0-auc:0.87085 validation_1-auc:0.83334 [33] validation_0-auc:0.87112 validation_1-auc:0.83313 [34] validation_0-auc:0.87161 validation_1-auc:0.83383 [35] validation_0-auc:0.87173 validation_1-auc:0.83376 [36] validation_0-auc:0.87260 validation_1-auc:0.83340 [37] validation_0-auc:0.87310 validation_1-auc:0.83344 [38] validation_0-auc:0.87322 validation_1-auc:0.83343 [39] validation_0-auc:0.87339 validation_1-auc:0.83370 [40] validation_0-auc:0.87351 validation_1-auc:0.83373 [41] validation_0-auc:0.87411 validation_1-auc:0.83358 [42] validation_0-auc:0.87433 validation_1-auc:0.83325 [43] validation_0-auc:0.87432 validation_1-auc:0.83319 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80013 validation_1-auc:0.79685 [1] validation_0-auc:0.82084 validation_1-auc:0.81574 [2] validation_0-auc:0.82744 validation_1-auc:0.82189 [3] validation_0-auc:0.83029 validation_1-auc:0.82317 [4] validation_0-auc:0.83578 validation_1-auc:0.82564 [5] validation_0-auc:0.83777 validation_1-auc:0.83385 [6] validation_0-auc:0.83742 validation_1-auc:0.83162 [7] validation_0-auc:0.84373 validation_1-auc:0.83436 [8] validation_0-auc:0.84836 validation_1-auc:0.83664 [9] validation_0-auc:0.84790 validation_1-auc:0.83583 [10] validation_0-auc:0.84717 validation_1-auc:0.83268 [11] validation_0-auc:0.84654 validation_1-auc:0.83066 [12] validation_0-auc:0.85377 validation_1-auc:0.83579 [13] validation_0-auc:0.85800 validation_1-auc:0.83859 [14] validation_0-auc:0.85962 validation_1-auc:0.83984 [15] validation_0-auc:0.86143 validation_1-auc:0.84003 [16] validation_0-auc:0.86269 validation_1-auc:0.84049 [17] validation_0-auc:0.86399 validation_1-auc:0.84009 [18] validation_0-auc:0.86474 validation_1-auc:0.84034 [19] validation_0-auc:0.86662 validation_1-auc:0.84138 [20] validation_0-auc:0.86730 validation_1-auc:0.84100 [21] validation_0-auc:0.86821 validation_1-auc:0.84058 [22] validation_0-auc:0.86942 validation_1-auc:0.84128 [23] validation_0-auc:0.86992 validation_1-auc:0.84122 [24] validation_0-auc:0.87035 validation_1-auc:0.84116 [25] validation_0-auc:0.87091 validation_1-auc:0.84045 [26] validation_0-auc:0.87139 validation_1-auc:0.83974 [27] validation_0-auc:0.87296 validation_1-auc:0.83926 [28] validation_0-auc:0.87307 validation_1-auc:0.83943 [29] validation_0-auc:0.87330 validation_1-auc:0.84017 [30] validation_0-auc:0.87443 validation_1-auc:0.83949 [31] validation_0-auc:0.87467 validation_1-auc:0.83936 [32] validation_0-auc:0.87513 validation_1-auc:0.83943 [33] validation_0-auc:0.87519 validation_1-auc:0.83951 [34] validation_0-auc:0.87542 validation_1-auc:0.83953 [35] validation_0-auc:0.87552 validation_1-auc:0.83946 [36] validation_0-auc:0.87582 validation_1-auc:0.83936 [37] validation_0-auc:0.87604 validation_1-auc:0.83919 [38] validation_0-auc:0.87622 validation_1-auc:0.83874 [39] validation_0-auc:0.87670 validation_1-auc:0.83844 [40] validation_0-auc:0.87678 validation_1-auc:0.83859 [41] validation_0-auc:0.87711 validation_1-auc:0.83830 [42] validation_0-auc:0.87738 validation_1-auc:0.83823 [43] validation_0-auc:0.87752 validation_1-auc:0.83796 [44] validation_0-auc:0.87777 validation_1-auc:0.83765 [45] validation_0-auc:0.87785 validation_1-auc:0.83786 [46] validation_0-auc:0.87802 validation_1-auc:0.83761 [47] validation_0-auc:0.87840 validation_1-auc:0.83698 [48] validation_0-auc:0.87868 validation_1-auc:0.83699 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80039 validation_1-auc:0.80013 [1] validation_0-auc:0.82111 validation_1-auc:0.82026 [2] validation_0-auc:0.82749 validation_1-auc:0.82627 [3] validation_0-auc:0.83124 validation_1-auc:0.82830 [4] validation_0-auc:0.83475 validation_1-auc:0.82881 [5] validation_0-auc:0.83676 validation_1-auc:0.83385 [6] validation_0-auc:0.83648 validation_1-auc:0.83085 [7] validation_0-auc:0.84336 validation_1-auc:0.83472 [8] validation_0-auc:0.84624 validation_1-auc:0.83404 [9] validation_0-auc:0.84541 validation_1-auc:0.83287 [10] validation_0-auc:0.84554 validation_1-auc:0.83039 [11] validation_0-auc:0.84525 validation_1-auc:0.82995 [12] validation_0-auc:0.85144 validation_1-auc:0.83489 [13] validation_0-auc:0.85525 validation_1-auc:0.83803 [14] validation_0-auc:0.85745 validation_1-auc:0.84145 [15] validation_0-auc:0.85817 validation_1-auc:0.84082 [16] validation_0-auc:0.86006 validation_1-auc:0.84076 [17] validation_0-auc:0.86127 validation_1-auc:0.84139 [18] validation_0-auc:0.86194 validation_1-auc:0.84041 [19] validation_0-auc:0.86337 validation_1-auc:0.84100 [20] validation_0-auc:0.86386 validation_1-auc:0.84145 [21] validation_0-auc:0.86550 validation_1-auc:0.84030 [22] validation_0-auc:0.86690 validation_1-auc:0.84072 [23] validation_0-auc:0.86765 validation_1-auc:0.84077 [24] validation_0-auc:0.86827 validation_1-auc:0.84136 [25] validation_0-auc:0.86939 validation_1-auc:0.84120 [26] validation_0-auc:0.87045 validation_1-auc:0.84098 [27] validation_0-auc:0.87062 validation_1-auc:0.84148 [28] validation_0-auc:0.87072 validation_1-auc:0.84120 [29] validation_0-auc:0.87113 validation_1-auc:0.84147 [30] validation_0-auc:0.87115 validation_1-auc:0.84181 [31] validation_0-auc:0.87145 validation_1-auc:0.84172 [32] validation_0-auc:0.87226 validation_1-auc:0.84100 [33] validation_0-auc:0.87242 validation_1-auc:0.84149 [34] validation_0-auc:0.87255 validation_1-auc:0.84120 [35] validation_0-auc:0.87297 validation_1-auc:0.84095 [36] validation_0-auc:0.87348 validation_1-auc:0.84051 [37] validation_0-auc:0.87395 validation_1-auc:0.84084 [38] validation_0-auc:0.87433 validation_1-auc:0.84055 [39] validation_0-auc:0.87448 validation_1-auc:0.84048 [40] validation_0-auc:0.87465 validation_1-auc:0.84042 [41] validation_0-auc:0.87486 validation_1-auc:0.84034 [42] validation_0-auc:0.87518 validation_1-auc:0.84021 [43] validation_0-auc:0.87525 validation_1-auc:0.84022 [44] validation_0-auc:0.87595 validation_1-auc:0.83967 [45] validation_0-auc:0.87629 validation_1-auc:0.84004 [46] validation_0-auc:0.87704 validation_1-auc:0.83966 [47] validation_0-auc:0.87746 validation_1-auc:0.83963 [48] validation_0-auc:0.87774 validation_1-auc:0.83931 [49] validation_0-auc:0.87784 validation_1-auc:0.83925 [50] validation_0-auc:0.87826 validation_1-auc:0.83935 [51] validation_0-auc:0.87861 validation_1-auc:0.83920 [52] validation_0-auc:0.87950 validation_1-auc:0.83895 [53] validation_0-auc:0.88024 validation_1-auc:0.83876 [54] validation_0-auc:0.88117 validation_1-auc:0.83840 [55] validation_0-auc:0.88126 validation_1-auc:0.83834 [56] validation_0-auc:0.88145 validation_1-auc:0.83873 [57] validation_0-auc:0.88157 validation_1-auc:0.83860 [58] validation_0-auc:0.88178 validation_1-auc:0.83810 [59] validation_0-auc:0.88186 validation_1-auc:0.83774 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79210 validation_1-auc:0.79292 [1] validation_0-auc:0.81759 validation_1-auc:0.81404 [2] validation_0-auc:0.82567 validation_1-auc:0.81864 [3] validation_0-auc:0.82819 validation_1-auc:0.82244 [4] validation_0-auc:0.83233 validation_1-auc:0.82618 [5] validation_0-auc:0.83480 validation_1-auc:0.83163 [6] validation_0-auc:0.83342 validation_1-auc:0.82840 [7] validation_0-auc:0.84265 validation_1-auc:0.83512 [8] validation_0-auc:0.84614 validation_1-auc:0.83742 [9] validation_0-auc:0.84573 validation_1-auc:0.83475 [10] validation_0-auc:0.84426 validation_1-auc:0.83066 [11] validation_0-auc:0.84358 validation_1-auc:0.82937 [12] validation_0-auc:0.85089 validation_1-auc:0.83491 [13] validation_0-auc:0.85457 validation_1-auc:0.83785 [14] validation_0-auc:0.85645 validation_1-auc:0.83894 [15] validation_0-auc:0.85744 validation_1-auc:0.83784 [16] validation_0-auc:0.85870 validation_1-auc:0.83899 [17] validation_0-auc:0.86002 validation_1-auc:0.83854 [18] validation_0-auc:0.86091 validation_1-auc:0.83860 [19] validation_0-auc:0.86154 validation_1-auc:0.83818 [20] validation_0-auc:0.86189 validation_1-auc:0.83772 [21] validation_0-auc:0.86295 validation_1-auc:0.83703 [22] validation_0-auc:0.86334 validation_1-auc:0.83721 [23] validation_0-auc:0.86402 validation_1-auc:0.83581 [24] validation_0-auc:0.86456 validation_1-auc:0.83557 [25] validation_0-auc:0.86494 validation_1-auc:0.83534 [26] validation_0-auc:0.86516 validation_1-auc:0.83481 [27] validation_0-auc:0.86660 validation_1-auc:0.83557 [28] validation_0-auc:0.86784 validation_1-auc:0.83546 [29] validation_0-auc:0.86793 validation_1-auc:0.83545 [30] validation_0-auc:0.86840 validation_1-auc:0.83496 [31] validation_0-auc:0.86867 validation_1-auc:0.83481 [32] validation_0-auc:0.86884 validation_1-auc:0.83472 [33] validation_0-auc:0.86900 validation_1-auc:0.83482 [34] validation_0-auc:0.86907 validation_1-auc:0.83423 [35] validation_0-auc:0.86981 validation_1-auc:0.83350 [36] validation_0-auc:0.86996 validation_1-auc:0.83334 [37] validation_0-auc:0.87004 validation_1-auc:0.83365 [38] validation_0-auc:0.87022 validation_1-auc:0.83384 [39] validation_0-auc:0.87078 validation_1-auc:0.83373 [40] validation_0-auc:0.87094 validation_1-auc:0.83373 [41] validation_0-auc:0.87109 validation_1-auc:0.83359 [42] validation_0-auc:0.87173 validation_1-auc:0.83365 [43] validation_0-auc:0.87264 validation_1-auc:0.83386 [44] validation_0-auc:0.87336 validation_1-auc:0.83319 [45] validation_0-auc:0.87361 validation_1-auc:0.83318 [46] validation_0-auc:0.87406 validation_1-auc:0.83227 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.79931 validation_1-auc:0.79594 [1] validation_0-auc:0.81987 validation_1-auc:0.81503 [2] validation_0-auc:0.82734 validation_1-auc:0.82126 [3] validation_0-auc:0.83110 validation_1-auc:0.82302 [4] validation_0-auc:0.83608 validation_1-auc:0.82494 [5] validation_0-auc:0.83914 validation_1-auc:0.83100 [6] validation_0-auc:0.83828 validation_1-auc:0.82999 [7] validation_0-auc:0.84425 validation_1-auc:0.83439 [8] validation_0-auc:0.84749 validation_1-auc:0.83609 [9] validation_0-auc:0.84727 validation_1-auc:0.83597 [10] validation_0-auc:0.84703 validation_1-auc:0.83250 [11] validation_0-auc:0.84664 validation_1-auc:0.83237 [12] validation_0-auc:0.85343 validation_1-auc:0.83713 [13] validation_0-auc:0.85671 validation_1-auc:0.83887 [14] validation_0-auc:0.85824 validation_1-auc:0.83919 [15] validation_0-auc:0.85962 validation_1-auc:0.83905 [16] validation_0-auc:0.86089 validation_1-auc:0.84031 [17] validation_0-auc:0.86216 validation_1-auc:0.84051 [18] validation_0-auc:0.86264 validation_1-auc:0.84051 [19] validation_0-auc:0.86341 validation_1-auc:0.84030 [20] validation_0-auc:0.86379 validation_1-auc:0.83988 [21] validation_0-auc:0.86413 validation_1-auc:0.84020 [22] validation_0-auc:0.86513 validation_1-auc:0.84033 [23] validation_0-auc:0.86584 validation_1-auc:0.84016 [24] validation_0-auc:0.86638 validation_1-auc:0.84016 [25] validation_0-auc:0.86691 validation_1-auc:0.83991 [26] validation_0-auc:0.86798 validation_1-auc:0.83979 [27] validation_0-auc:0.86869 validation_1-auc:0.83952 [28] validation_0-auc:0.86881 validation_1-auc:0.83942 [29] validation_0-auc:0.86908 validation_1-auc:0.83912 [30] validation_0-auc:0.86934 validation_1-auc:0.83907 [31] validation_0-auc:0.86942 validation_1-auc:0.83896 [32] validation_0-auc:0.87000 validation_1-auc:0.83860 [33] validation_0-auc:0.87016 validation_1-auc:0.83878 [34] validation_0-auc:0.87050 validation_1-auc:0.83830 [35] validation_0-auc:0.87069 validation_1-auc:0.83825 [36] validation_0-auc:0.87118 validation_1-auc:0.83880 [37] validation_0-auc:0.87126 validation_1-auc:0.83883 [38] validation_0-auc:0.87138 validation_1-auc:0.83882 [39] validation_0-auc:0.87243 validation_1-auc:0.83833 [40] validation_0-auc:0.87267 validation_1-auc:0.83813 [41] validation_0-auc:0.87282 validation_1-auc:0.83811 [42] validation_0-auc:0.87356 validation_1-auc:0.83806 [43] validation_0-auc:0.87372 validation_1-auc:0.83815 [44] validation_0-auc:0.87384 validation_1-auc:0.83807 [45] validation_0-auc:0.87395 validation_1-auc:0.83813 [46] validation_0-auc:0.87450 validation_1-auc:0.83757 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80248 validation_1-auc:0.80001 [1] validation_0-auc:0.82249 validation_1-auc:0.81765 [2] validation_0-auc:0.82833 validation_1-auc:0.82524 [3] validation_0-auc:0.83371 validation_1-auc:0.82814 [4] validation_0-auc:0.83653 validation_1-auc:0.82856 [5] validation_0-auc:0.83838 validation_1-auc:0.83345 [6] validation_0-auc:0.83823 validation_1-auc:0.83165 [7] validation_0-auc:0.84386 validation_1-auc:0.83505 [8] validation_0-auc:0.84688 validation_1-auc:0.83507 [9] validation_0-auc:0.84634 validation_1-auc:0.83483 [10] validation_0-auc:0.84564 validation_1-auc:0.83324 [11] validation_0-auc:0.84501 validation_1-auc:0.83283 [12] validation_0-auc:0.85011 validation_1-auc:0.83693 [13] validation_0-auc:0.85299 validation_1-auc:0.83995 [14] validation_0-auc:0.85523 validation_1-auc:0.84250 [15] validation_0-auc:0.85608 validation_1-auc:0.84183 [16] validation_0-auc:0.85748 validation_1-auc:0.84319 [17] validation_0-auc:0.85895 validation_1-auc:0.84363 [18] validation_0-auc:0.85944 validation_1-auc:0.84311 [19] validation_0-auc:0.86102 validation_1-auc:0.84368 [20] validation_0-auc:0.86122 validation_1-auc:0.84367 [21] validation_0-auc:0.86196 validation_1-auc:0.84403 [22] validation_0-auc:0.86291 validation_1-auc:0.84498 [23] validation_0-auc:0.86385 validation_1-auc:0.84460 [24] validation_0-auc:0.86452 validation_1-auc:0.84460 [25] validation_0-auc:0.86534 validation_1-auc:0.84480 [26] validation_0-auc:0.86584 validation_1-auc:0.84441 [27] validation_0-auc:0.86653 validation_1-auc:0.84401 [28] validation_0-auc:0.86697 validation_1-auc:0.84422 [29] validation_0-auc:0.86770 validation_1-auc:0.84385 [30] validation_0-auc:0.86777 validation_1-auc:0.84407 [31] validation_0-auc:0.86803 validation_1-auc:0.84395 [32] validation_0-auc:0.86826 validation_1-auc:0.84381 [33] validation_0-auc:0.86862 validation_1-auc:0.84417 [34] validation_0-auc:0.86902 validation_1-auc:0.84385 [35] validation_0-auc:0.86959 validation_1-auc:0.84369 [36] validation_0-auc:0.87020 validation_1-auc:0.84297 [37] validation_0-auc:0.87047 validation_1-auc:0.84278 [38] validation_0-auc:0.87175 validation_1-auc:0.84286 [39] validation_0-auc:0.87269 validation_1-auc:0.84224 [40] validation_0-auc:0.87289 validation_1-auc:0.84197 [41] validation_0-auc:0.87294 validation_1-auc:0.84175 [42] validation_0-auc:0.87418 validation_1-auc:0.84148 [43] validation_0-auc:0.87431 validation_1-auc:0.84121 [44] validation_0-auc:0.87441 validation_1-auc:0.84127 [45] validation_0-auc:0.87458 validation_1-auc:0.84103 [46] validation_0-auc:0.87475 validation_1-auc:0.84119 [47] validation_0-auc:0.87529 validation_1-auc:0.84128 [48] validation_0-auc:0.87554 validation_1-auc:0.84050 [49] validation_0-auc:0.87572 validation_1-auc:0.84039 [50] validation_0-auc:0.87575 validation_1-auc:0.84062 [51] validation_0-auc:0.87605 validation_1-auc:0.84105 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80843 validation_1-auc:0.80885 [1] validation_0-auc:0.82920 validation_1-auc:0.82211 [2] validation_0-auc:0.83320 validation_1-auc:0.82400 [3] validation_0-auc:0.83625 validation_1-auc:0.82577 [4] validation_0-auc:0.84188 validation_1-auc:0.82897 [5] validation_0-auc:0.84455 validation_1-auc:0.83377 [6] validation_0-auc:0.84503 validation_1-auc:0.82916 [7] validation_0-auc:0.85319 validation_1-auc:0.83364 [8] validation_0-auc:0.85976 validation_1-auc:0.83390 [9] validation_0-auc:0.85952 validation_1-auc:0.82834 [10] validation_0-auc:0.85919 validation_1-auc:0.82378 [11] validation_0-auc:0.85956 validation_1-auc:0.82400 [12] validation_0-auc:0.86574 validation_1-auc:0.82888 [13] validation_0-auc:0.87027 validation_1-auc:0.83251 [14] validation_0-auc:0.87240 validation_1-auc:0.83311 [15] validation_0-auc:0.87365 validation_1-auc:0.83080 [16] validation_0-auc:0.87567 validation_1-auc:0.83134 [17] validation_0-auc:0.87777 validation_1-auc:0.83255 [18] validation_0-auc:0.87904 validation_1-auc:0.83149 [19] validation_0-auc:0.88037 validation_1-auc:0.83083 [20] validation_0-auc:0.88104 validation_1-auc:0.82964 [21] validation_0-auc:0.88159 validation_1-auc:0.82802 [22] validation_0-auc:0.88227 validation_1-auc:0.82806 [23] validation_0-auc:0.88255 validation_1-auc:0.82806 [24] validation_0-auc:0.88328 validation_1-auc:0.82840 [25] validation_0-auc:0.88353 validation_1-auc:0.82851 [26] validation_0-auc:0.88384 validation_1-auc:0.82899 [27] validation_0-auc:0.88509 validation_1-auc:0.82988 [28] validation_0-auc:0.88544 validation_1-auc:0.82886 [29] validation_0-auc:0.88569 validation_1-auc:0.82922 [30] validation_0-auc:0.88588 validation_1-auc:0.82962 [31] validation_0-auc:0.88682 validation_1-auc:0.82951 [32] validation_0-auc:0.88752 validation_1-auc:0.82858 [33] validation_0-auc:0.88762 validation_1-auc:0.82843 [34] validation_0-auc:0.88792 validation_1-auc:0.82804 [35] validation_0-auc:0.88865 validation_1-auc:0.82692 [36] validation_0-auc:0.88868 validation_1-auc:0.82609 [37] validation_0-auc:0.88901 validation_1-auc:0.82607 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81304 validation_1-auc:0.81746 [1] validation_0-auc:0.82882 validation_1-auc:0.82026 [2] validation_0-auc:0.83609 validation_1-auc:0.82474 [3] validation_0-auc:0.84041 validation_1-auc:0.82824 [4] validation_0-auc:0.84760 validation_1-auc:0.83130 [5] validation_0-auc:0.84938 validation_1-auc:0.83590 [6] validation_0-auc:0.85116 validation_1-auc:0.83167 [7] validation_0-auc:0.85828 validation_1-auc:0.83471 [8] validation_0-auc:0.86371 validation_1-auc:0.83640 [9] validation_0-auc:0.86365 validation_1-auc:0.83549 [10] validation_0-auc:0.86395 validation_1-auc:0.83127 [11] validation_0-auc:0.86437 validation_1-auc:0.82983 [12] validation_0-auc:0.87068 validation_1-auc:0.83421 [13] validation_0-auc:0.87545 validation_1-auc:0.83773 [14] validation_0-auc:0.87779 validation_1-auc:0.83843 [15] validation_0-auc:0.87893 validation_1-auc:0.83628 [16] validation_0-auc:0.88035 validation_1-auc:0.83878 [17] validation_0-auc:0.88227 validation_1-auc:0.83749 [18] validation_0-auc:0.88364 validation_1-auc:0.83710 [19] validation_0-auc:0.88528 validation_1-auc:0.83727 [20] validation_0-auc:0.88606 validation_1-auc:0.83670 [21] validation_0-auc:0.88672 validation_1-auc:0.83629 [22] validation_0-auc:0.88793 validation_1-auc:0.83586 [23] validation_0-auc:0.88875 validation_1-auc:0.83562 [24] validation_0-auc:0.88913 validation_1-auc:0.83589 [25] validation_0-auc:0.88932 validation_1-auc:0.83575 [26] validation_0-auc:0.89053 validation_1-auc:0.83424 [27] validation_0-auc:0.89116 validation_1-auc:0.83427 [28] validation_0-auc:0.89172 validation_1-auc:0.83384 [29] validation_0-auc:0.89244 validation_1-auc:0.83318 [30] validation_0-auc:0.89260 validation_1-auc:0.83224 [31] validation_0-auc:0.89294 validation_1-auc:0.83214 [32] validation_0-auc:0.89361 validation_1-auc:0.83111 [33] validation_0-auc:0.89396 validation_1-auc:0.83114 [34] validation_0-auc:0.89481 validation_1-auc:0.83121 [35] validation_0-auc:0.89548 validation_1-auc:0.83133 [36] validation_0-auc:0.89589 validation_1-auc:0.83039 [37] validation_0-auc:0.89614 validation_1-auc:0.83024 [38] validation_0-auc:0.89743 validation_1-auc:0.82952 [39] validation_0-auc:0.89749 validation_1-auc:0.82950 [40] validation_0-auc:0.89754 validation_1-auc:0.82932 [41] validation_0-auc:0.89813 validation_1-auc:0.82838 [42] validation_0-auc:0.89831 validation_1-auc:0.82849 [43] validation_0-auc:0.89841 validation_1-auc:0.82827 [44] validation_0-auc:0.89908 validation_1-auc:0.82824 [45] validation_0-auc:0.89919 validation_1-auc:0.82788 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81393 validation_1-auc:0.81377 [1] validation_0-auc:0.82962 validation_1-auc:0.82668 [2] validation_0-auc:0.83724 validation_1-auc:0.83017 [3] validation_0-auc:0.84075 validation_1-auc:0.83079 [4] validation_0-auc:0.84691 validation_1-auc:0.83337 [5] validation_0-auc:0.84896 validation_1-auc:0.83502 [6] validation_0-auc:0.84980 validation_1-auc:0.82858 [7] validation_0-auc:0.85918 validation_1-auc:0.83358 [8] validation_0-auc:0.86284 validation_1-auc:0.83470 [9] validation_0-auc:0.86365 validation_1-auc:0.83427 [10] validation_0-auc:0.86243 validation_1-auc:0.83264 [11] validation_0-auc:0.86248 validation_1-auc:0.83255 [12] validation_0-auc:0.86969 validation_1-auc:0.83531 [13] validation_0-auc:0.87452 validation_1-auc:0.83774 [14] validation_0-auc:0.87630 validation_1-auc:0.83936 [15] validation_0-auc:0.87826 validation_1-auc:0.83676 [16] validation_0-auc:0.87988 validation_1-auc:0.83852 [17] validation_0-auc:0.88289 validation_1-auc:0.83811 [18] validation_0-auc:0.88333 validation_1-auc:0.83735 [19] validation_0-auc:0.88506 validation_1-auc:0.83720 [20] validation_0-auc:0.88528 validation_1-auc:0.83718 [21] validation_0-auc:0.88547 validation_1-auc:0.83646 [22] validation_0-auc:0.88632 validation_1-auc:0.83706 [23] validation_0-auc:0.88770 validation_1-auc:0.83714 [24] validation_0-auc:0.88867 validation_1-auc:0.83742 [25] validation_0-auc:0.88905 validation_1-auc:0.83753 [26] validation_0-auc:0.89065 validation_1-auc:0.83634 [27] validation_0-auc:0.89158 validation_1-auc:0.83565 [28] validation_0-auc:0.89214 validation_1-auc:0.83460 [29] validation_0-auc:0.89345 validation_1-auc:0.83413 [30] validation_0-auc:0.89377 validation_1-auc:0.83373 [31] validation_0-auc:0.89392 validation_1-auc:0.83396 [32] validation_0-auc:0.89410 validation_1-auc:0.83435 [33] validation_0-auc:0.89416 validation_1-auc:0.83412 [34] validation_0-auc:0.89437 validation_1-auc:0.83386 [35] validation_0-auc:0.89513 validation_1-auc:0.83338 [36] validation_0-auc:0.89553 validation_1-auc:0.83232 [37] validation_0-auc:0.89589 validation_1-auc:0.83223 [38] validation_0-auc:0.89609 validation_1-auc:0.83222 [39] validation_0-auc:0.89636 validation_1-auc:0.83187 [40] validation_0-auc:0.89652 validation_1-auc:0.83146 [41] validation_0-auc:0.89655 validation_1-auc:0.83131 [42] validation_0-auc:0.89789 validation_1-auc:0.83068 [43] validation_0-auc:0.89792 validation_1-auc:0.83069 [44] validation_0-auc:0.89889 validation_1-auc:0.83038 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80901 validation_1-auc:0.80653 [1] validation_0-auc:0.82713 validation_1-auc:0.82150 [2] validation_0-auc:0.83227 validation_1-auc:0.82513 [3] validation_0-auc:0.83319 validation_1-auc:0.82525 [4] validation_0-auc:0.83786 validation_1-auc:0.82805 [5] validation_0-auc:0.84104 validation_1-auc:0.82979 [6] validation_0-auc:0.84432 validation_1-auc:0.82639 [7] validation_0-auc:0.85301 validation_1-auc:0.83411 [8] validation_0-auc:0.85882 validation_1-auc:0.83754 [9] validation_0-auc:0.85838 validation_1-auc:0.83437 [10] validation_0-auc:0.85606 validation_1-auc:0.83252 [11] validation_0-auc:0.85677 validation_1-auc:0.83031 [12] validation_0-auc:0.86256 validation_1-auc:0.83311 [13] validation_0-auc:0.86712 validation_1-auc:0.83500 [14] validation_0-auc:0.86926 validation_1-auc:0.83593 [15] validation_0-auc:0.87031 validation_1-auc:0.83404 [16] validation_0-auc:0.87119 validation_1-auc:0.83472 [17] validation_0-auc:0.87276 validation_1-auc:0.83454 [18] validation_0-auc:0.87365 validation_1-auc:0.83418 [19] validation_0-auc:0.87495 validation_1-auc:0.83324 [20] validation_0-auc:0.87498 validation_1-auc:0.83267 [21] validation_0-auc:0.87527 validation_1-auc:0.83259 [22] validation_0-auc:0.87572 validation_1-auc:0.83274 [23] validation_0-auc:0.87659 validation_1-auc:0.83362 [24] validation_0-auc:0.87704 validation_1-auc:0.83315 [25] validation_0-auc:0.87743 validation_1-auc:0.83338 [26] validation_0-auc:0.87762 validation_1-auc:0.83358 [27] validation_0-auc:0.87818 validation_1-auc:0.83337 [28] validation_0-auc:0.87822 validation_1-auc:0.83346 [29] validation_0-auc:0.87890 validation_1-auc:0.83331 [30] validation_0-auc:0.87903 validation_1-auc:0.83315 [31] validation_0-auc:0.87993 validation_1-auc:0.83277 [32] validation_0-auc:0.88063 validation_1-auc:0.83284 [33] validation_0-auc:0.88096 validation_1-auc:0.83339 [34] validation_0-auc:0.88210 validation_1-auc:0.83309 [35] validation_0-auc:0.88207 validation_1-auc:0.83317 [36] validation_0-auc:0.88224 validation_1-auc:0.83314 [37] validation_0-auc:0.88240 validation_1-auc:0.83292 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81176 validation_1-auc:0.80947 [1] validation_0-auc:0.82651 validation_1-auc:0.82286 [2] validation_0-auc:0.83551 validation_1-auc:0.82712 [3] validation_0-auc:0.83820 validation_1-auc:0.82810 [4] validation_0-auc:0.84733 validation_1-auc:0.82952 [5] validation_0-auc:0.84903 validation_1-auc:0.83409 [6] validation_0-auc:0.84836 validation_1-auc:0.83191 [7] validation_0-auc:0.85387 validation_1-auc:0.83486 [8] validation_0-auc:0.85876 validation_1-auc:0.83709 [9] validation_0-auc:0.85840 validation_1-auc:0.83730 [10] validation_0-auc:0.85787 validation_1-auc:0.83417 [11] validation_0-auc:0.85814 validation_1-auc:0.83328 [12] validation_0-auc:0.86431 validation_1-auc:0.83684 [13] validation_0-auc:0.86878 validation_1-auc:0.83901 [14] validation_0-auc:0.87119 validation_1-auc:0.83987 [15] validation_0-auc:0.87268 validation_1-auc:0.83789 [16] validation_0-auc:0.87455 validation_1-auc:0.83903 [17] validation_0-auc:0.87645 validation_1-auc:0.83873 [18] validation_0-auc:0.87724 validation_1-auc:0.83908 [19] validation_0-auc:0.87799 validation_1-auc:0.83966 [20] validation_0-auc:0.87882 validation_1-auc:0.83958 [21] validation_0-auc:0.87902 validation_1-auc:0.83960 [22] validation_0-auc:0.87951 validation_1-auc:0.83985 [23] validation_0-auc:0.88042 validation_1-auc:0.83903 [24] validation_0-auc:0.88118 validation_1-auc:0.83938 [25] validation_0-auc:0.88183 validation_1-auc:0.83941 [26] validation_0-auc:0.88279 validation_1-auc:0.83943 [27] validation_0-auc:0.88430 validation_1-auc:0.83947 [28] validation_0-auc:0.88447 validation_1-auc:0.83972 [29] validation_0-auc:0.88487 validation_1-auc:0.83903 [30] validation_0-auc:0.88567 validation_1-auc:0.83956 [31] validation_0-auc:0.88560 validation_1-auc:0.83942 [32] validation_0-auc:0.88572 validation_1-auc:0.83903 [33] validation_0-auc:0.88598 validation_1-auc:0.83902 [34] validation_0-auc:0.88633 validation_1-auc:0.83882 [35] validation_0-auc:0.88642 validation_1-auc:0.83890 [36] validation_0-auc:0.88707 validation_1-auc:0.83877 [37] validation_0-auc:0.88742 validation_1-auc:0.83862 [38] validation_0-auc:0.88755 validation_1-auc:0.83835 [39] validation_0-auc:0.88788 validation_1-auc:0.83760 [40] validation_0-auc:0.88777 validation_1-auc:0.83781 [41] validation_0-auc:0.88796 validation_1-auc:0.83789 [42] validation_0-auc:0.88804 validation_1-auc:0.83796 [43] validation_0-auc:0.88868 validation_1-auc:0.83769 [44] validation_0-auc:0.88942 validation_1-auc:0.83764 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81519 validation_1-auc:0.81115 [1] validation_0-auc:0.83201 validation_1-auc:0.82366 [2] validation_0-auc:0.83718 validation_1-auc:0.83029 [3] validation_0-auc:0.84145 validation_1-auc:0.83163 [4] validation_0-auc:0.84628 validation_1-auc:0.83410 [5] validation_0-auc:0.84792 validation_1-auc:0.83694 [6] validation_0-auc:0.84780 validation_1-auc:0.83116 [7] validation_0-auc:0.85599 validation_1-auc:0.83759 [8] validation_0-auc:0.85905 validation_1-auc:0.83700 [9] validation_0-auc:0.85860 validation_1-auc:0.83638 [10] validation_0-auc:0.85875 validation_1-auc:0.83594 [11] validation_0-auc:0.85921 validation_1-auc:0.83691 [12] validation_0-auc:0.86560 validation_1-auc:0.84075 [13] validation_0-auc:0.86941 validation_1-auc:0.84350 [14] validation_0-auc:0.87102 validation_1-auc:0.84520 [15] validation_0-auc:0.87174 validation_1-auc:0.84423 [16] validation_0-auc:0.87350 validation_1-auc:0.84460 [17] validation_0-auc:0.87528 validation_1-auc:0.84395 [18] validation_0-auc:0.87593 validation_1-auc:0.84331 [19] validation_0-auc:0.87733 validation_1-auc:0.84275 [20] validation_0-auc:0.87769 validation_1-auc:0.84252 [21] validation_0-auc:0.87822 validation_1-auc:0.84160 [22] validation_0-auc:0.87989 validation_1-auc:0.84207 [23] validation_0-auc:0.88086 validation_1-auc:0.84223 [24] validation_0-auc:0.88139 validation_1-auc:0.84238 [25] validation_0-auc:0.88186 validation_1-auc:0.84258 [26] validation_0-auc:0.88258 validation_1-auc:0.84240 [27] validation_0-auc:0.88359 validation_1-auc:0.84183 [28] validation_0-auc:0.88402 validation_1-auc:0.84147 [29] validation_0-auc:0.88415 validation_1-auc:0.84140 [30] validation_0-auc:0.88455 validation_1-auc:0.84080 [31] validation_0-auc:0.88538 validation_1-auc:0.84070 [32] validation_0-auc:0.88563 validation_1-auc:0.84055 [33] validation_0-auc:0.88610 validation_1-auc:0.84024 [34] validation_0-auc:0.88631 validation_1-auc:0.83977 [35] validation_0-auc:0.88637 validation_1-auc:0.83959 [36] validation_0-auc:0.88644 validation_1-auc:0.83935 [37] validation_0-auc:0.88728 validation_1-auc:0.83898 [38] validation_0-auc:0.88802 validation_1-auc:0.83814 [39] validation_0-auc:0.88815 validation_1-auc:0.83806 [40] validation_0-auc:0.88815 validation_1-auc:0.83811 [41] validation_0-auc:0.88838 validation_1-auc:0.83807 [42] validation_0-auc:0.88883 validation_1-auc:0.83753 [43] validation_0-auc:0.88902 validation_1-auc:0.83781 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81007 validation_1-auc:0.80693 [1] validation_0-auc:0.82137 validation_1-auc:0.81877 [2] validation_0-auc:0.82976 validation_1-auc:0.82498 [3] validation_0-auc:0.83120 validation_1-auc:0.82212 [4] validation_0-auc:0.83382 validation_1-auc:0.82481 [5] validation_0-auc:0.83696 validation_1-auc:0.82672 [6] validation_0-auc:0.83976 validation_1-auc:0.83016 [7] validation_0-auc:0.84177 validation_1-auc:0.83330 [8] validation_0-auc:0.84585 validation_1-auc:0.83282 [9] validation_0-auc:0.84984 validation_1-auc:0.83519 [10] validation_0-auc:0.85146 validation_1-auc:0.83530 [11] validation_0-auc:0.85113 validation_1-auc:0.83380 [12] validation_0-auc:0.85502 validation_1-auc:0.83622 [13] validation_0-auc:0.85797 validation_1-auc:0.83644 [14] validation_0-auc:0.85990 validation_1-auc:0.83686 [15] validation_0-auc:0.86114 validation_1-auc:0.83639 [16] validation_0-auc:0.86158 validation_1-auc:0.83602 [17] validation_0-auc:0.86285 validation_1-auc:0.83501 [18] validation_0-auc:0.86405 validation_1-auc:0.83454 [19] validation_0-auc:0.86498 validation_1-auc:0.83497 [20] validation_0-auc:0.86595 validation_1-auc:0.83417 [21] validation_0-auc:0.86757 validation_1-auc:0.83454 [22] validation_0-auc:0.86810 validation_1-auc:0.83466 [23] validation_0-auc:0.86830 validation_1-auc:0.83461 [24] validation_0-auc:0.86859 validation_1-auc:0.83422 [25] validation_0-auc:0.86941 validation_1-auc:0.83371 [26] validation_0-auc:0.86986 validation_1-auc:0.83392 [27] validation_0-auc:0.87053 validation_1-auc:0.83330 [28] validation_0-auc:0.87105 validation_1-auc:0.83367 [29] validation_0-auc:0.87111 validation_1-auc:0.83371 [30] validation_0-auc:0.87152 validation_1-auc:0.83435 [31] validation_0-auc:0.87181 validation_1-auc:0.83437 [32] validation_0-auc:0.87286 validation_1-auc:0.83459 [33] validation_0-auc:0.87304 validation_1-auc:0.83470 [34] validation_0-auc:0.87347 validation_1-auc:0.83407 [35] validation_0-auc:0.87393 validation_1-auc:0.83319 [36] validation_0-auc:0.87464 validation_1-auc:0.83300 [37] validation_0-auc:0.87469 validation_1-auc:0.83311 [38] validation_0-auc:0.87502 validation_1-auc:0.83281 [39] validation_0-auc:0.87594 validation_1-auc:0.83273 [40] validation_0-auc:0.87620 validation_1-auc:0.83299 [41] validation_0-auc:0.87747 validation_1-auc:0.83274 [42] validation_0-auc:0.87754 validation_1-auc:0.83254 [43] validation_0-auc:0.87846 validation_1-auc:0.83286 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.80863 validation_1-auc:0.80010 [1] validation_0-auc:0.82349 validation_1-auc:0.81717 [2] validation_0-auc:0.82654 validation_1-auc:0.81737 [3] validation_0-auc:0.82988 validation_1-auc:0.82281 [4] validation_0-auc:0.83570 validation_1-auc:0.82554 [5] validation_0-auc:0.83917 validation_1-auc:0.82930 [6] validation_0-auc:0.84492 validation_1-auc:0.83396 [7] validation_0-auc:0.84657 validation_1-auc:0.83569 [8] validation_0-auc:0.84837 validation_1-auc:0.83476 [9] validation_0-auc:0.85010 validation_1-auc:0.83841 [10] validation_0-auc:0.85017 validation_1-auc:0.83887 [11] validation_0-auc:0.85091 validation_1-auc:0.83723 [12] validation_0-auc:0.85584 validation_1-auc:0.83976 [13] validation_0-auc:0.85900 validation_1-auc:0.84063 [14] validation_0-auc:0.86059 validation_1-auc:0.84054 [15] validation_0-auc:0.86167 validation_1-auc:0.84086 [16] validation_0-auc:0.86303 validation_1-auc:0.84085 [17] validation_0-auc:0.86383 validation_1-auc:0.83947 [18] validation_0-auc:0.86462 validation_1-auc:0.83971 [19] validation_0-auc:0.86559 validation_1-auc:0.84059 [20] validation_0-auc:0.86650 validation_1-auc:0.83981 [21] validation_0-auc:0.86762 validation_1-auc:0.84030 [22] validation_0-auc:0.86865 validation_1-auc:0.84050 [23] validation_0-auc:0.86916 validation_1-auc:0.83978 [24] validation_0-auc:0.86953 validation_1-auc:0.84033 [25] validation_0-auc:0.86992 validation_1-auc:0.84000 [26] validation_0-auc:0.87005 validation_1-auc:0.83998 [27] validation_0-auc:0.87115 validation_1-auc:0.83964 [28] validation_0-auc:0.87205 validation_1-auc:0.83972 [29] validation_0-auc:0.87328 validation_1-auc:0.83984 [30] validation_0-auc:0.87360 validation_1-auc:0.83929 [31] validation_0-auc:0.87367 validation_1-auc:0.83938 [32] validation_0-auc:0.87441 validation_1-auc:0.83918 [33] validation_0-auc:0.87490 validation_1-auc:0.83990 [34] validation_0-auc:0.87594 validation_1-auc:0.84011 [35] validation_0-auc:0.87618 validation_1-auc:0.83988 [36] validation_0-auc:0.87648 validation_1-auc:0.83991 [37] validation_0-auc:0.87657 validation_1-auc:0.83991 [38] validation_0-auc:0.87676 validation_1-auc:0.83987 [39] validation_0-auc:0.87696 validation_1-auc:0.83973 [40] validation_0-auc:0.87705 validation_1-auc:0.83990 [41] validation_0-auc:0.87724 validation_1-auc:0.83941 [42] validation_0-auc:0.87781 validation_1-auc:0.83934 [43] validation_0-auc:0.87810 validation_1-auc:0.83924 [44] validation_0-auc:0.87848 validation_1-auc:0.83882 [45] validation_0-auc:0.87863 validation_1-auc:0.83888 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.82005 validation_1-auc:0.81815 [1] validation_0-auc:0.82547 validation_1-auc:0.82159 [2] validation_0-auc:0.83019 validation_1-auc:0.82631 [3] validation_0-auc:0.83230 validation_1-auc:0.82660 [4] validation_0-auc:0.83488 validation_1-auc:0.82988 [5] validation_0-auc:0.83888 validation_1-auc:0.83262 [6] validation_0-auc:0.84242 validation_1-auc:0.83408 [7] validation_0-auc:0.84581 validation_1-auc:0.83560 [8] validation_0-auc:0.84775 validation_1-auc:0.83617 [9] validation_0-auc:0.84989 validation_1-auc:0.83746 [10] validation_0-auc:0.85052 validation_1-auc:0.83816 [11] validation_0-auc:0.84982 validation_1-auc:0.83603 [12] validation_0-auc:0.85408 validation_1-auc:0.83825 [13] validation_0-auc:0.85547 validation_1-auc:0.83955 [14] validation_0-auc:0.85818 validation_1-auc:0.84292 [15] validation_0-auc:0.85990 validation_1-auc:0.84361 [16] validation_0-auc:0.86142 validation_1-auc:0.84287 [17] validation_0-auc:0.86247 validation_1-auc:0.84280 [18] validation_0-auc:0.86276 validation_1-auc:0.84297 [19] validation_0-auc:0.86368 validation_1-auc:0.84290 [20] validation_0-auc:0.86488 validation_1-auc:0.84279 [21] validation_0-auc:0.86540 validation_1-auc:0.84307 [22] validation_0-auc:0.86631 validation_1-auc:0.84285 [23] validation_0-auc:0.86687 validation_1-auc:0.84289 [24] validation_0-auc:0.86777 validation_1-auc:0.84289 [25] validation_0-auc:0.86830 validation_1-auc:0.84279 [26] validation_0-auc:0.86862 validation_1-auc:0.84237 [27] validation_0-auc:0.87011 validation_1-auc:0.84232 [28] validation_0-auc:0.87063 validation_1-auc:0.84224 [29] validation_0-auc:0.87063 validation_1-auc:0.84199 [30] validation_0-auc:0.87108 validation_1-auc:0.84246 [31] validation_0-auc:0.87190 validation_1-auc:0.84252 [32] validation_0-auc:0.87275 validation_1-auc:0.84147 [33] validation_0-auc:0.87302 validation_1-auc:0.84149 [34] validation_0-auc:0.87350 validation_1-auc:0.84118 [35] validation_0-auc:0.87371 validation_1-auc:0.84115 [36] validation_0-auc:0.87407 validation_1-auc:0.84113 [37] validation_0-auc:0.87475 validation_1-auc:0.84038 [38] validation_0-auc:0.87529 validation_1-auc:0.84009 [39] validation_0-auc:0.87540 validation_1-auc:0.83988 [40] validation_0-auc:0.87555 validation_1-auc:0.83984 [41] validation_0-auc:0.87579 validation_1-auc:0.83991 [42] validation_0-auc:0.87630 validation_1-auc:0.83942 [43] validation_0-auc:0.87664 validation_1-auc:0.83926 [44] validation_0-auc:0.87713 validation_1-auc:0.83916 [45] validation_0-auc:0.87763 validation_1-auc:0.83868 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81105 validation_1-auc:0.80637 [1] validation_0-auc:0.82008 validation_1-auc:0.81881 [2] validation_0-auc:0.82922 validation_1-auc:0.82532 [3] validation_0-auc:0.83159 validation_1-auc:0.82594 [4] validation_0-auc:0.83378 validation_1-auc:0.82618 [5] validation_0-auc:0.83671 validation_1-auc:0.82887 [6] validation_0-auc:0.84111 validation_1-auc:0.83302 [7] validation_0-auc:0.84227 validation_1-auc:0.83380 [8] validation_0-auc:0.84422 validation_1-auc:0.83346 [9] validation_0-auc:0.84742 validation_1-auc:0.83581 [10] validation_0-auc:0.84984 validation_1-auc:0.83563 [11] validation_0-auc:0.84933 validation_1-auc:0.83344 [12] validation_0-auc:0.85285 validation_1-auc:0.83653 [13] validation_0-auc:0.85494 validation_1-auc:0.83796 [14] validation_0-auc:0.85653 validation_1-auc:0.83880 [15] validation_0-auc:0.85803 validation_1-auc:0.83841 [16] validation_0-auc:0.85922 validation_1-auc:0.83773 [17] validation_0-auc:0.85983 validation_1-auc:0.83709 [18] validation_0-auc:0.86162 validation_1-auc:0.83622 [19] validation_0-auc:0.86232 validation_1-auc:0.83513 [20] validation_0-auc:0.86287 validation_1-auc:0.83518 [21] validation_0-auc:0.86374 validation_1-auc:0.83543 [22] validation_0-auc:0.86416 validation_1-auc:0.83540 [23] validation_0-auc:0.86459 validation_1-auc:0.83510 [24] validation_0-auc:0.86482 validation_1-auc:0.83477 [25] validation_0-auc:0.86526 validation_1-auc:0.83484 [26] validation_0-auc:0.86545 validation_1-auc:0.83473 [27] validation_0-auc:0.86568 validation_1-auc:0.83481 [28] validation_0-auc:0.86578 validation_1-auc:0.83485 [29] validation_0-auc:0.86654 validation_1-auc:0.83501 [30] validation_0-auc:0.86666 validation_1-auc:0.83465 [31] validation_0-auc:0.86790 validation_1-auc:0.83486 [32] validation_0-auc:0.86802 validation_1-auc:0.83488 [33] validation_0-auc:0.86809 validation_1-auc:0.83473 [34] validation_0-auc:0.86821 validation_1-auc:0.83483 [35] validation_0-auc:0.86828 validation_1-auc:0.83508 [36] validation_0-auc:0.86861 validation_1-auc:0.83435 [37] validation_0-auc:0.86866 validation_1-auc:0.83425 [38] validation_0-auc:0.86892 validation_1-auc:0.83451 [39] validation_0-auc:0.86913 validation_1-auc:0.83425 [40] validation_0-auc:0.86939 validation_1-auc:0.83430 [41] validation_0-auc:0.86940 validation_1-auc:0.83443 [42] validation_0-auc:0.86949 validation_1-auc:0.83436 [43] validation_0-auc:0.87013 validation_1-auc:0.83441 [44] validation_0-auc:0.87059 validation_1-auc:0.83365 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81067 validation_1-auc:0.81109 [1] validation_0-auc:0.82045 validation_1-auc:0.81627 [2] validation_0-auc:0.82760 validation_1-auc:0.82116 [3] validation_0-auc:0.82925 validation_1-auc:0.81730 [4] validation_0-auc:0.83628 validation_1-auc:0.82554 [5] validation_0-auc:0.83889 validation_1-auc:0.82992 [6] validation_0-auc:0.84258 validation_1-auc:0.83304 [7] validation_0-auc:0.84515 validation_1-auc:0.83327 [8] validation_0-auc:0.84797 validation_1-auc:0.83479 [9] validation_0-auc:0.84982 validation_1-auc:0.83737 [10] validation_0-auc:0.84996 validation_1-auc:0.83746 [11] validation_0-auc:0.84929 validation_1-auc:0.83715 [12] validation_0-auc:0.85506 validation_1-auc:0.83957 [13] validation_0-auc:0.85817 validation_1-auc:0.84131 [14] validation_0-auc:0.85945 validation_1-auc:0.84041 [15] validation_0-auc:0.86040 validation_1-auc:0.83984 [16] validation_0-auc:0.86127 validation_1-auc:0.83954 [17] validation_0-auc:0.86170 validation_1-auc:0.83947 [18] validation_0-auc:0.86276 validation_1-auc:0.83945 [19] validation_0-auc:0.86327 validation_1-auc:0.84019 [20] validation_0-auc:0.86381 validation_1-auc:0.84075 [21] validation_0-auc:0.86454 validation_1-auc:0.84078 [22] validation_0-auc:0.86530 validation_1-auc:0.84164 [23] validation_0-auc:0.86598 validation_1-auc:0.84128 [24] validation_0-auc:0.86656 validation_1-auc:0.84078 [25] validation_0-auc:0.86721 validation_1-auc:0.84069 [26] validation_0-auc:0.86745 validation_1-auc:0.84066 [27] validation_0-auc:0.86808 validation_1-auc:0.84017 [28] validation_0-auc:0.86914 validation_1-auc:0.84027 [29] validation_0-auc:0.86951 validation_1-auc:0.84014 [30] validation_0-auc:0.86972 validation_1-auc:0.84016 [31] validation_0-auc:0.86996 validation_1-auc:0.83992 [32] validation_0-auc:0.87072 validation_1-auc:0.84001 [33] validation_0-auc:0.87090 validation_1-auc:0.83997 [34] validation_0-auc:0.87111 validation_1-auc:0.83969 [35] validation_0-auc:0.87145 validation_1-auc:0.83964 [36] validation_0-auc:0.87215 validation_1-auc:0.84006 [37] validation_0-auc:0.87242 validation_1-auc:0.83987 [38] validation_0-auc:0.87262 validation_1-auc:0.83995 [39] validation_0-auc:0.87270 validation_1-auc:0.84021 [40] validation_0-auc:0.87275 validation_1-auc:0.84066 [41] validation_0-auc:0.87323 validation_1-auc:0.84095 [42] validation_0-auc:0.87372 validation_1-auc:0.84074 [43] validation_0-auc:0.87433 validation_1-auc:0.84057 [44] validation_0-auc:0.87440 validation_1-auc:0.84028 [45] validation_0-auc:0.87511 validation_1-auc:0.84011 [46] validation_0-auc:0.87553 validation_1-auc:0.83972 [47] validation_0-auc:0.87606 validation_1-auc:0.83880 [48] validation_0-auc:0.87630 validation_1-auc:0.83876 [49] validation_0-auc:0.87629 validation_1-auc:0.83900 [50] validation_0-auc:0.87637 validation_1-auc:0.83902 [51] validation_0-auc:0.87649 validation_1-auc:0.83930 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81835 validation_1-auc:0.81691 [1] validation_0-auc:0.82862 validation_1-auc:0.82346 [2] validation_0-auc:0.83280 validation_1-auc:0.82893 [3] validation_0-auc:0.83563 validation_1-auc:0.82931 [4] validation_0-auc:0.83780 validation_1-auc:0.83200 [5] validation_0-auc:0.83975 validation_1-auc:0.83280 [6] validation_0-auc:0.84205 validation_1-auc:0.83374 [7] validation_0-auc:0.84453 validation_1-auc:0.83256 [8] validation_0-auc:0.84638 validation_1-auc:0.83384 [9] validation_0-auc:0.84986 validation_1-auc:0.83670 [10] validation_0-auc:0.85058 validation_1-auc:0.83825 [11] validation_0-auc:0.84986 validation_1-auc:0.83646 [12] validation_0-auc:0.85321 validation_1-auc:0.83744 [13] validation_0-auc:0.85478 validation_1-auc:0.83942 [14] validation_0-auc:0.85613 validation_1-auc:0.84091 [15] validation_0-auc:0.85709 validation_1-auc:0.84170 [16] validation_0-auc:0.85891 validation_1-auc:0.84239 [17] validation_0-auc:0.86023 validation_1-auc:0.84215 [18] validation_0-auc:0.86146 validation_1-auc:0.84247 [19] validation_0-auc:0.86202 validation_1-auc:0.84237 [20] validation_0-auc:0.86268 validation_1-auc:0.84152 [21] validation_0-auc:0.86342 validation_1-auc:0.84132 [22] validation_0-auc:0.86492 validation_1-auc:0.84044 [23] validation_0-auc:0.86602 validation_1-auc:0.84073 [24] validation_0-auc:0.86688 validation_1-auc:0.84082 [25] validation_0-auc:0.86779 validation_1-auc:0.84074 [26] validation_0-auc:0.86849 validation_1-auc:0.84076 [27] validation_0-auc:0.86910 validation_1-auc:0.84096 [28] validation_0-auc:0.86931 validation_1-auc:0.84113 [29] validation_0-auc:0.86974 validation_1-auc:0.84187 [30] validation_0-auc:0.87070 validation_1-auc:0.84167 [31] validation_0-auc:0.87108 validation_1-auc:0.84174 [32] validation_0-auc:0.87123 validation_1-auc:0.84166 [33] validation_0-auc:0.87153 validation_1-auc:0.84142 [34] validation_0-auc:0.87214 validation_1-auc:0.84153 [35] validation_0-auc:0.87289 validation_1-auc:0.84147 [36] validation_0-auc:0.87329 validation_1-auc:0.84136 [37] validation_0-auc:0.87345 validation_1-auc:0.84116 [38] validation_0-auc:0.87355 validation_1-auc:0.84114 [39] validation_0-auc:0.87411 validation_1-auc:0.84087 [40] validation_0-auc:0.87419 validation_1-auc:0.84088 [41] validation_0-auc:0.87540 validation_1-auc:0.84065 [42] validation_0-auc:0.87576 validation_1-auc:0.84078 [43] validation_0-auc:0.87598 validation_1-auc:0.84097 [44] validation_0-auc:0.87646 validation_1-auc:0.84047 [45] validation_0-auc:0.87666 validation_1-auc:0.84048 [46] validation_0-auc:0.87670 validation_1-auc:0.84016 [47] validation_0-auc:0.87719 validation_1-auc:0.84000 [48] validation_0-auc:0.87796 validation_1-auc:0.83922 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): . [0] validation_0-auc:0.81685 validation_1-auc:0.81075 [1] validation_0-auc:0.82791 validation_1-auc:0.82283 [2] validation_0-auc:0.83537 validation_1-auc:0.82615 [3] validation_0-auc:0.83996 validation_1-auc:0.82712 [4] validation_0-auc:0.84558 validation_1-auc:0.82791 [5] validation_0-auc:0.84781 validation_1-auc:0.82977 [6] validation_0-auc:0.85151 validation_1-auc:0.83373 [7] validation_0-auc:0.85510 validation_1-auc:0.83444 [8] validation_0-auc:0.85998 validation_1-auc:0.83601 [9] validation_0-auc:0.86238 validation_1-auc:0.83804 [10] validation_0-auc:0.86435 validation_1-auc:0.83584 [11] validation_0-auc:0.86583 validation_1-auc:0.83093 [12] validation_0-auc:0.87079 validation_1-auc:0.83235 [13] validation_0-auc:0.87454 validation_1-auc:0.83253 [14] validation_0-auc:0.87642 validation_1-auc:0.83254 [15] validation_0-auc:0.87856 validation_1-auc:0.83218 [16] validation_0-auc:0.87973 validation_1-auc:0.83171 [17] validation_0-auc:0.88122 validation_1-auc:0.83115 [18] validation_0-auc:0.88256 validation_1-auc:0.83119 [19] validation_0-auc:0.88330 validation_1-auc:0.83139 [20] validation_0-auc:0.88408 validation_1-auc:0.83082 [21] validation_0-auc:0.88505 validation_1-auc:0.83044 [22] validation_0-auc:0.88631 validation_1-auc:0.83025 [23] validation_0-auc:0.88670 validation_1-auc:0.83047 [24] validation_0-auc:0.88740 validation_1-auc:0.82903 [25] validation_0-auc:0.88770 validation_1-auc:0.82895 [26] validation_0-auc:0.88793 validation_1-auc:0.82913 [27] validation_0-auc:0.88808 validation_1-auc:0.82881 [28] validation_0-auc:0.88830 validation_1-auc:0.82901 [29] validation_0-auc:0.88834 validation_1-auc:0.82910 [30] validation_0-auc:0.88894 validation_1-auc:0.82854 [31] validation_0-auc:0.88898 validation_1-auc:0.82859 [32] validation_0-auc:0.88914 validation_1-auc:0.82837 [33] validation_0-auc:0.88935 validation_1-auc:0.82847 [34] validation_0-auc:0.89037 validation_1-auc:0.82891 [35] validation_0-auc:0.89097 validation_1-auc:0.82869 [36] validation_0-auc:0.89158 validation_1-auc:0.82814 [37] validation_0-auc:0.89167 validation_1-auc:0.82822 [38] validation_0-auc:0.89184 validation_1-auc:0.82764 [39] validation_0-auc:0.89187 validation_1-auc:0.82734 . C: Users woo anaconda3 envs py39r41 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) C: Users woo anaconda3 envs py39r41 lib site-packages xgboost data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)): .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/05/10/santander.html",
            "relUrl": "/kaggle/2022/05/10/santander.html",
            "date": " • May 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "from sklearn.preprocessing import PolynomialFeatures import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score %matplotlib inline . def true_fun(X): return np.cos(1.5 * np.pi * X) # X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다. np.random.seed(0) n_samples = 30 X = np.sort(np.random.rand(n_samples)) # y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. y = true_fun(X) + np.random.randn(n_samples) * 0.1 . X . array([0.0202184 , 0.07103606, 0.0871293 , 0.11827443, 0.14335329, 0.38344152, 0.41466194, 0.4236548 , 0.43758721, 0.46147936, 0.52184832, 0.52889492, 0.54488318, 0.5488135 , 0.56804456, 0.60276338, 0.63992102, 0.64589411, 0.71518937, 0.77815675, 0.78052918, 0.79172504, 0.79915856, 0.83261985, 0.87001215, 0.891773 , 0.92559664, 0.94466892, 0.96366276, 0.97861834]) . y . array([ 1.0819082 , 0.87027612, 1.14386208, 0.70322051, 0.78494746, -0.25265944, -0.22066063, -0.26595867, -0.4562644 , -0.53001927, -0.86481449, -0.99462675, -0.87458603, -0.83407054, -0.77090649, -0.83476183, -1.03080067, -1.02544303, -1.0788268 , -1.00713288, -1.03009698, -0.63623922, -0.86230652, -0.75328767, -0.70023795, -0.41043495, -0.50486767, -0.27907117, -0.25994628, -0.06189804]) . polynomial_features = PolynomialFeatures(degree=4, include_bias = False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;polynomial_features&quot;, polynomial_features), (&quot;linear_regression&quot;, linear_regression)]) pipeline.fit(X.reshape(-1, 1), y) . Pipeline(steps=[(&#39;polynomial_features&#39;, PolynomialFeatures(degree=4, include_bias=False)), (&#39;linear_regression&#39;, LinearRegression())]) . cof = pipeline.named_steps[&#39;linear_regression&#39;].coef_ . cof . array([ 0.46754142, -17.78954475, 23.5926603 , -7.26289872]) . cof[0] . 0.4675414165618556 . PolynomialFeatures(degree=4).fit_transform(X) . ValueError Traceback (most recent call last) Input In [12], in &lt;cell line: 1&gt;() -&gt; 1 PolynomialFeatures(degree=4).fit_transform(X) File ~ anaconda3 envs py39r41 lib site-packages sklearn base.py:852, in TransformerMixin.fit_transform(self, X, y, **fit_params) 848 # non-optimized default implementation; override when a better 849 # method is possible for a given clustering algorithm 850 if y is None: 851 # fit method of arity 1 (unsupervised transformation) --&gt; 852 return self.fit(X, **fit_params).transform(X) 853 else: 854 # fit method of arity 2 (supervised transformation) 855 return self.fit(X, y, **fit_params).transform(X) File ~ anaconda3 envs py39r41 lib site-packages sklearn preprocessing _polynomial.py:287, in PolynomialFeatures.fit(self, X, y) 270 def fit(self, X, y=None): 271 &#34;&#34;&#34; 272 Compute number of output features. 273 (...) 285 Fitted transformer. 286 &#34;&#34;&#34; --&gt; 287 _, n_features = self._validate_data(X, accept_sparse=True).shape 289 if isinstance(self.degree, numbers.Integral): 290 if self.degree &lt; 0: File ~ anaconda3 envs py39r41 lib site-packages sklearn base.py:566, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, **check_params) 564 raise ValueError(&#34;Validation should be done on X, y or both.&#34;) 565 elif not no_val_X and no_val_y: --&gt; 566 X = check_array(X, **check_params) 567 out = X 568 elif no_val_X and not no_val_y: File ~ anaconda3 envs py39r41 lib site-packages sklearn utils validation.py:769, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator) 767 # If input is 1D raise error 768 if array.ndim == 1: --&gt; 769 raise ValueError( 770 &#34;Expected 2D array, got 1D array instead: narray={}. n&#34; 771 &#34;Reshape your data either using array.reshape(-1, 1) if &#34; 772 &#34;your data has a single feature or array.reshape(1, -1) &#34; 773 &#34;if it contains a single sample.&#34;.format(array) 774 ) 776 # make sure we actually converted to numeric: 777 if dtype_numeric and array.dtype.kind in &#34;OUSV&#34;: ValueError: Expected 2D array, got 1D array instead: array=[0.0202184 0.07103606 0.0871293 0.11827443 0.14335329 0.38344152 0.41466194 0.4236548 0.43758721 0.46147936 0.52184832 0.52889492 0.54488318 0.5488135 0.56804456 0.60276338 0.63992102 0.64589411 0.71518937 0.77815675 0.78052918 0.79172504 0.79915856 0.83261985 0.87001215 0.891773 0.92559664 0.94466892 0.96366276 0.97861834]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. .",
            "url": "https://stahangryum.github.io/Woo/2022/05/10/Untitled.html",
            "relUrl": "/2022/05/10/Untitled.html",
            "date": " • May 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "R Errors",
            "content": "some &#39;x&#39; not counted; maybe &#39;breaks&#39; do not span range of &#39;x&#39; .",
            "url": "https://stahangryum.github.io/Woo/r/2022/04/26/Rerror.html",
            "relUrl": "/r/2022/04/26/Rerror.html",
            "date": " • Apr 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA HIGHLIGHT",
            "content": "library(tidyverse) . Warning message: &#34;패키지 &#39;tidyverse&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Attaching packages - tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.6 v dplyr 1.0.8 v tidyr 1.2.0 v stringr 1.4.0 v readr 2.1.2 v forcats 0.5.1 Warning message: &#34;패키지 &#39;ggplot2&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tibble&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;tidyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;readr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;purrr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;dplyr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;stringr&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; Warning message: &#34;패키지 &#39;forcats&#39;는 R 버전 4.1.3에서 작성되었습니다&#34; -- Conflicts - tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() . Ch.1 R Language . 생략 | . Ch.2 Introduction to EDA . 자료분석은 대체로 두 가지 단계로 나뉜다. . 탐색적 자료분석(데이터의 구조와 특징 파악) . | 확증적 자료분석(모형, 재현성 평가) . | . EDA(Exploratory Data Analysis) . EDA는 데이터 특징과 내재하는 구조적 관계를 알아내기 위한 기법들을 통칭한다. . | 데이터를 특정한 모형에 적합시키기 보다는 데이터를 있는 그대로 보려는 데에 중점을 둔다. . | . 4 Themes . EDA에서는 네 가지 주제가 때로는 홀로, 때로는 얽혀서 나타난다. . 저항성(resistance)의 강조(예를 들어 평균보다는 일부자료의 파손에 저항적인 중위수가 바람직한 대표값 측도로서 선호된다.) . | 잔차(residual) 계산 . | 자료변수의 재표현(re-expression)을 통한 다각적 시도 . | 그래프를 통한 현시성(revelation) . | . Summary (2) . 자료분석은 탐색적 자료분석과 확증적 자료분석의 두 단계로 나눌 수 있다. . | EDA는 자료의 구조 및 특징의 파악을 목적으로 한다. 이를 위하여 효과적이고 신뢰성 있는 데이터의 요약과 그래프적 기법이 사용된다. . | EDA의 네 개 주제는 저항성, 잔차, 재표현, 현시성이다. . | 통계적 모형은 &#39;진실&#39;로서가 아니라 &#39;주요 사례&#39;로서 의미가 있다. 또한 모형과 데이터는 일방통행이 아닌 쌍방통행으로 이해되어야 한다. 데이터와 분석도 사이클을 이룬다. . | . Ch.3 Stem and Leaf . Stem and Leaf . 데이터의 값을 십 단위인 줄기(stem)와 일 단위인 잎(leaf)으로 분리한다. . - 장점 . 사분위수나 중앙값을 찾기 쉽다. . | 분포의 전체적인 모양(봉우리 개수, 대칭분포, 치우친 방향)을 쉽게 알 수 있다. . | 이상값 유무를 파악할 수 있다. . | . - 단점 . 자료가 많은 경우에는 부적합하고 자료의 수가 50개 이하일 때 적합하다. | . exam1 = read.table(&#39;dataset/EDA/exam1.txt&#39;, header = TRUE) head(exam1) . A data.frame: 6 × 2 hwscore . &lt;int&gt;&lt;int&gt; . 10 | 54 | . 20 | 51 | . 31 | 52 | . 40 | 82 | . 51 | 37 | . 61 | 41 | . stem(exam1$score) . The decimal point is 1 digit(s) to the right of the | 0 | 00 1 | 058 2 | 1333458889 3 | 0355789 4 | 11133456678 5 | 11122233344456688 6 | 147779 7 | 33478 8 | 29 9 | 09 . 이 줄기그림의 주요 특징은 이봉분포의 모습을 보인다는 점이다. 이것은 자료가 2개의 군집으로 되어 있음을 말한다. | . exam1[exam1$hw == 0, ]$score %&gt;% stem # 과제 미제출자 exam1[exam1$hw == 1, ]$score %&gt;% stem # 과제 제출자 . The decimal point is 1 digit(s) to the right of the | 0 | 00 1 | 05 2 | 13334589 3 | 0355 4 | 13378 5 | 122333446 6 | 4 7 | 3 8 | 29 The decimal point is 1 digit(s) to the right of the | 1 | 8 2 | 88 3 | 789 4 | 114566 5 | 11245688 6 | 17779 7 | 3478 8 | 9 | 09 . 과제 제출 여부에 따라서 줄기-잎 그림을 두 개 그려본 결과 흥미로운 점을 확인했다. . 과제 미제출자에 경우 여전히 이봉분포를 띄고 있으나 과제 제출자의 경우 단봉분포를 띄고 있다. . 이것은 과제물 미제출 그룹이 서로 이질적인 어떤 두 집단(50점대를 중심으로 하는 집단과 20점대를 중심으로 하는 집단)의 혼합임을 말하여 준다. . 아마 전자는 학업습관이 불성실하나 학업능력은 우수한 집단이고 후자는 학업습관과 학업능력 모두 좋지 않은 학생들의 집단이 아닐까 생각해본다. . Tip :R에서 줄기 수를 줄이거나 늘이려면 stem() 함수 내 scale 파라미터를 조정하면 된다. . stem(exam1$score, scale = 0.5) # 줄기 수를 절반으로 줄인다. . The decimal point is 1 digit(s) to the right of the | 0 | 00058 2 | 13334588890355789 4 | 1113345667811122233344456688 6 | 14777933478 8 | 2909 . 이 줄기 그림은 단봉분포의 형태를 취한다. 이것은 너무 단순하여 이 자료의 주요 특성을 잃은 것으로 볼 수 있다. . 즉 2개의 봉우리를 구분하지 못하고 1개만 본 것이다. . stem(exam1$score, scale = 2) # 줄기 수를 두 배로 늘인다. . The decimal point is 1 digit(s) to the right of the | 0 | 00 0 | 1 | 0 1 | 58 2 | 13334 2 | 58889 3 | 03 3 | 55789 4 | 111334 4 | 56678 5 | 111222333444 5 | 56688 6 | 14 6 | 7779 7 | 334 7 | 78 8 | 2 8 | 9 9 | 0 9 | 9 . 줄기 수를 늘였더니 더 많은 봉우리를 볼 수 있다. . 일반적으로 줄기 수를 늘이면 늘일수록 많은 수의 봉우리를 보게 되고 그 반대로 줄기 수를 줄이면 줄일수록 적은 수의 봉우리를 보게 된다. . Compared with Histogram . hist(exam1$score) . 줄기 - 잎 히스토그램 . 정보 손실 | 손실되지 않음 | 손실됨 | . 줄기 수 변환 | 쉬움 | 어려움 | . 구간의 폭 | 조정 불가능 | 조정 가능 | . Summary (3) . 줄기 그림은 히스토그램과 마찬가지로 자료 분포의 특성을 그래프화한 것이다. 줄기 그림은 히스토그램에 비하여 정보의 보전 면에서 우수하며 쉽게 구간(줄기) 수를 늘이거나 줄일 수 있다. 그러나 구간(줄기)의 선정시 제약이 따른다. . | 적절한 줄기 그림을 그리기 위하여 여러 개의 그림을 그려보고 비교해 보아야 한다. 계획된 시행착오가 필요하다. . | 줄기 그림에서는 다음과 같은 자료의 특성을 관찰할 수 있다. . 군집의 수 | 집중도가 높은 구간 | 대칭성 여부 | 자료의 범위 및 산포 | 특이점의 존재여부 | . | . Ch.4 Numerical Summary and Box Plot . Mean and Median . . 한 쪽 꼬리가 긴 분포에서 평균값은 쉽게 휘둘리지만 중앙값은 쉽게 휘둘리지 않다. 따라서 중앙값이 대표값으로서 적합하다. | . $ begin{cases} X_{ frac{N+1}{2}} qquad if N = odd dfrac{X_{ frac{N}{2}} + X_{ frac{N}{2} + 1}}{2} qquad if N = even end{cases}$ . 중간값의 깊이 $d(M)$은 . $d(M) = dfrac{(N+1)}{2}$ . Five Number Summary . 아래 4분위수 : $H_L$ 중간값 : $M$ 위 4분위수 : $H_U$ . 다섯 숫자 요약 = (min, $H_L, M, H_U$, max) = (최솟값, 제 1사분위수, 중앙값, 제 3사분위수, 최댓값) . summary(exam1$score) . Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00 33.00 48.00 47.23 58.00 99.00 . Skewness . 왜도 = $SKEW = dfrac{(H_U - M) - (M - H_L)}{(H_U - M) + (M - H_L)}$ . 왜도 &lt; 0 이면 왼쪽으로 기울어진 분포 . 왜도 &gt; 0 이면 오른쪽으로 기울어진 분포 . Quantiles . low = quantile(exam1$score, c(1/2, 1/4, 1/8, 1/16), type = 8) %&gt;% as.numeric %&gt;% round(2)# 중간값, 아래 4분위수, 아래 8분위수, 아래 16분위수 high = quantile(exam1$score, c(1/2, 3/4, 7/8, 15/16), type = 8) %&gt;% as.numeric %&gt;% round(2) # 중간값, 위 4분위수, 위 8분위수, 위 16분위수 values = cbind(low, high, (low+high)/2, high-low) colnames(values) = c(&#39;low&#39;, &#39;high&#39;, &#39;mid&#39;, &#39;spr&#39;) rownames(values) = c(&#39;M&#39;, &#39;H&#39;, &#39;E&#39;, &#39;D&#39;) values . A matrix: 4 × 4 of type dbl lowhighmidspr . M48.00 | 48.00 | 48.00 | 0.00 | . H32.00 | 58.00 | 45.00 | 26.00 | . E23.00 | 73.00 | 48.00 | 50.00 | . D16.25 | 80.33 | 48.29 | 64.08 | . KURTO . Ch.5 Data Re-Expression . &#47729;&#49849;, &#47196;&#44536;, &#51648;&#49688; &#48320;&#54872;&#50640; &#51032;&#54620; &#51116;&#54364;&#54788; . Standardization . 표준화 변환이란 통상적으로 한 자료묶음의 평균이 0, 표준편차가 1이 되도록 하는 선형변환을 말한다. | . $x_1, x_2, dots, x_n$을 자료 값이라고 할 때 이것의 표준화변환 $z_1, z_2, dots, z_n$은 다음과 같이 정한다. . $z_i = dfrac{x_i - bar{x}}{s_x}, , i = 1,2, dots,n. qquad(1)$ . 그런데 (1)은 로버스트하지 않은 $ bar{x}$와 $s_x$에 의존하므로 EDA의 관점에서는 믿고 사용하기 어렵다. [^1] . 왜냐하면 표본평균과 표본표준편차는 극단적인 이상점에 의해 크게 변동될 수 있기 때문이다. | 그러나 중앙값 또는 사분위수범위(IQR)은 비교적 로버스트하다. | . 즉, 평균 $ bar{x}$ 대신에 중앙값 $med_x$를, 표준편차 $s_x$ 대신에 사분위수범위 $IQR$을 보정한 $ tilde{ sigma_x} = dfrac{IQR}{1.35}$을 쓰는 것이 좋을 것이다. . 따라서 로버스트 표준화 변환은 다음과 같다. . $ bar{z_i} = dfrac{x_i - med_x}{ tilde{ sigma_x}} , i = 1,2, dots,n. qquad(1)$ . 표준화 변환을 사용하는 예시 상황은 다음과 같다. . [1] &#34;A 그룹 학생 100명의 시험 X 점수는 N(40,10)으로부터 생성되었다.&#34; [1] &#34;B 그룹 학생 90명의 시험 Y 점수는 N(40,10)으로부터, 나머지 10명의 시험 Y 점수 N(80,5)으로부터 생성되었다.&#34; . par(mfrow = c(1,2)) X_group &lt;- rnorm(100, 40, 10) Y_group &lt;- c(rnorm(90,40,10), rnorm(10,80,5)) z_X &lt;- (X_group-mean(X_group))/sd(X_group) z_Y &lt;- (Y_group-mean(Y_group))/sd(Y_group) hist(z_X, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Standardized X&#39;) hist(z_Y, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Standardized Y&#39;) . par(mfrow = c(1,2)) robust_z_X &lt;- (X_group-median(X_group))/(IQR(X_group)/1.35) robust_z_Y &lt;- (Y_group-median(Y_group))/(IQR(Y_group)/1.35) hist(robust_z_X, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Robust Standardized X&#39;) hist(robust_z_Y, breaks = seq(-6, 6, 0.5), freq = F, ylim = c(0, 0.7), main = &#39;Robust Standardized Y&#39;) . Summary (5) . 선형변환 $ax+b , (a &gt; 0)$은 분포의 형태를 바꾸지 않는다. 그러나 비선형변환은 분포의 형태를 바꾼다. | . 변환의 사다리는 $x^p$ 꼴의 파워(power, 멱승)형 변환을 일컫는데 변환의 사다리를 내려가면 $(p &lt; 1)$ 오른쪽 꼬리가 짧아진다. $p=0$에 해당하는 변환은 로그변환이다. | . 자료의 재표현은 분포의 대칭화를 위하여, 또는 자료묶음들의 산포를 균일화하기 위한 목적으로 실행된다. | . 자료의 재표현은 자료 해석을 풍부하게 한다. | . Ch.6 QQ-Plot . Various Patterns . Normal Distribution . 정규분포로부터의 모의생성 자료에 대한 정규확률 플롯 대체로 직선적인 경향선을 확인할 수 있다. | 직선의 절편과 기울기가 각각 100과 15 근처임을 확인할 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- rnorm(40, 100, 15) qqnorm(x1) qqline(x1) x2 &lt;- rnorm(4000, 100, 15) qqnorm(x2) qqline(x2) . Mixture Normal Distribution . 혼합 정규분포로부터의 모의생성 자료에 대한 정규확률 플롯 중앙에서 밀도가 낮다. | 우상과 좌하 부분에서 강한 곡선성을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- c(rnorm(20, 70, 15), rnorm(20, 130, 15)) qqnorm(x1) x2 &lt;- c(rnorm(2000, 70, 15), rnorm(2000, 130, 15)) qqnorm(x2) . Data with outliers . 특이값이 내재하는 모의생성 자료에 대한 정규확률 플롯 25, 175가 이상점으로 존재한다. | 25는 주경향선보다 아래에 있다. | 175는 주경향선보다 위에 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- rnorm(38, 100, 15) outliers &lt;- c(25, 175) qqnorm(c(x1, outliers)) x2 &lt;- rnorm(3800, 100, 15) qqnorm(c(x2, outliers)) . Short Tail . 꼬리가 짧은 분포로부터의 모의생성 자료에 대한 정규확률 플롯 플롯의 전체적 모양이 비스듬한 S자 성장곡선의 형태를 취하고 있음을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- runif(40, 80, 120) qqnorm(x1) x2 &lt;- runif(4000, 80, 120) qqnorm(x2) . Long Tail . 꼬리가 긴 분포로부터의 모의생성 자료에 대한 정규확률 플롯 플롯의 전체적인 모양이 비스듬한 역 S자 성장곡선의 형태를 취하고 있음을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- c(rexp(20,1), -rexp(20,1)) qqnorm(x1) x2 &lt;- c(rexp(2000,1), -rexp(2000,1)) qqnorm(x2) . Right Skewed . 큰 값 쪽으로 긴 꼬리를 뻗은 기울어진 분포의 경우 비스듬히 기울어진 J자 곡선의 형태임을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- exp(rnorm(40, 5, 1)) qqnorm(x1) x2 &lt;- exp(rnorm(4000, 5, 1)) qqnorm(x2) . Left Skewed . 작은 값 쪽으로 긴 꼬리를 뻗은 기울어진 분포의 경우 비스듬히 기울어진 역 J자 곡선의 형태임을 볼 수 있다. | . | . par(mfrow = c(1,2)) x1 &lt;- 1500 - exp(rnorm(40,5,1)) qqnorm(x1) x2 &lt;- 1500 - exp(rnorm(40,5,1)) qqnorm(x2) . Example . 백혈병 환자 21명의 생존시간에 관한 다음의 자료를 지수분포에 적합하여 보자. | . leukemia &lt;- c(1,1,2,2,3,4,4,5,5,8,8,8,8,11,11,12,12,15,17,22,23) leukemia_quant = ((1:length(leukemia)) - 1/3) / (length(leukemia) + 1/3) x &lt;- -log(1-leukemia_quant) y &lt;- sort(leukemia) plot(y ~ x) . library(lattice) qqmath(leukemia) . T . x &lt;- rbeta(800,2,3) * 100 y &lt;- rbeta(1200,3,2) * 100 . round(quantile(x), 1) . &lt;dl class=dl-inline&gt;0%225%23.250%37.475%52.7100%95.6&lt;/dl&gt; round(quantile(y), 1) . &lt;dl class=dl-inline&gt;0%4.825%44.250%62.175%75.9100%99.2&lt;/dl&gt; par(mfrow=c(1,2)) qqplot(x, y, xlim = c(0,100)) qqplot(x, y, xlim = c(0,100), type = &#39;l&#39;) . ref ppt . par(mfrow=c(1,2)) income&lt;-c(1940,3100,3818,4448,5088,5844,6640,7689,9257,14519) hist(income, main = &#39;original&#39;) hist(log(income), main = &#39;log transform&#39;) summary(income) summary(log(income)) . Min. 1st Qu. Median Mean 3rd Qu. Max. 1940 3976 5466 6234 7427 14519 . Min. 1st Qu. Median Mean 3rd Qu. Max. 7.570 8.286 8.604 8.593 8.911 9.583 . 오른쪽 꼬리가 길었으나 로그 변환으로 완화됨 | . curve(x^(1/16), col = &#39;blue&#39;) par(new=T) curve(log(x), col = &#39;red&#39;) . [^1] 로버스트(robust) 한 통계량은 이상치/에러값으로 부터 영향을 크게 받지 않는 (건장한) 통계량 .",
            "url": "https://stahangryum.github.io/Woo/eda/r/2022/04/12/eda.html",
            "relUrl": "/eda/r/2022/04/12/eda.html",
            "date": " • Apr 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "numpy Highlight",
            "content": "Reference . ref. https://numpy.org/doc/stable/user/index.html# . ref. https://github.com/guebin/IP2022 . NumPy . Numerical Python | . Import . import numpy as np . Array Creation(ndarray) . N Dimensional Array | . A = np.array([1,2,3]) A . array([1, 2, 3]) . mylist = [3, 4, 5] array_from_list = np.array(mylist) array_from_list . array([3, 4, 5]) . mytuple = (4,5,6) array_from_tuple = np.array(mytuple) array_from_tuple . array([4, 5, 6]) . np.array(range(10)) . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . np.linspace(0,1,12) # 0 ~ 1을 12등분하여 만듬 (끝점을 포함) . array([0. , 0.09090909, 0.18181818, 0.27272727, 0.36363636, 0.45454545, 0.54545455, 0.63636364, 0.72727273, 0.81818182, 0.90909091, 1. ]) . len(np.linspace(0,1,12)) . 12 . np.arange(5) . array([0, 1, 2, 3, 4]) . np.arange(1,6) . array([1, 2, 3, 4, 5]) . np.zeros(3) # 0을 3개 . array([0., 0., 0.]) . np.zeros((3,3)) . array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]) . np.ones(3) # 1을 3개 . array([1., 1., 1.]) . np.ones((3,3)) . array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) . np.eye(3) # 단위 행렬 . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.diag([1,2,3]) # 대각선이 1,2,3인 행렬 . array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) . Broadcasting . A+1 # 덧셈 . array([2, 3, 4]) . A-4 # 뺄셈 . array([-3, -2, -1]) . A*2 # 곱셈 . array([2, 4, 6]) . A/2 # 나눗셈 . array([0.5, 1. , 1.5]) . A**2 # 제곱 . array([1, 4, 9]) . A%2 # 나머지 . array([1, 0, 1], dtype=int32) . np.sqrt(A) . array([1. , 1.41421356, 1.73205081]) . np.log(A) . array([0. , 0.69314718, 1.09861229]) . np.exp(A) . array([ 2.71828183, 7.3890561 , 20.08553692]) . np.sin(A) . array([0.84147098, 0.90929743, 0.14112001]) . A = np.array([11,22,33,44,55,66]) . Indexing . A[2] . 33 . A[5] . 66 . A[1:4] . array([22, 33, 44]) . A[[0,2,4]] . array([11, 33, 55]) . A[[True, False, True, False, False, True]] . array([11, 33, 66]) . A &lt; 33 . array([ True, True, False, False, False, False]) . A[A&lt;33] . array([11, 22]) . Matrix Indexing . A2 = np.array([[1,2,3,4],[-1,-2,-3,-4],[5,6,7,8],[-5,-6,-7,-8]]) A2 . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[1][3] . -4 . A2[1,3] . -4 . A2[0, 0:2] . array([1, 2]) . A2[0] . array([1, 2, 3, 4]) . A2[0, 2:] . array([3, 4]) . A2[:, :] . array([[ 1, 2, 3, 4], [-1, -2, -3, -4], [ 5, 6, 7, 8], [-5, -6, -7, -8]]) . A2[[0,2], :] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . A2[[0,2]] . array([[1, 2, 3, 4], [5, 6, 7, 8]]) . Useful Functions . np.random . {python} np.random. . np.random.rand . {python} np.random.rand(N) . np.random.rand(10) # 0~1 사이에서 10개의 난수 생성 . array([0.74265347, 0.12865142, 0.9512475 , 0.24358922, 0.03553823, 0.2295192 , 0.12775839, 0.0919773 , 0.50947895, 0.74584544]) . np.random.rand(10)*2 # (0~1)*2 = 0~2 사이에서 10개의 난수 생성 . array([0.70426213, 1.54703597, 0.73553232, 1.35319966, 1.7224698 , 1.43787541, 1.62631319, 1.54416732, 0.3511027 , 1.63909984]) . np.random.rand(10)+1 # 1~2 사이에서 10개의 난수 생성 . array([1.8882947 , 1.21487037, 1.87248472, 1.8506064 , 1.42443757, 1.795348 , 1.37847909, 1.24870616, 1.66128972, 1.7608405 ]) . np.random.rand(10)*2+1 # 1~3 사이에서 10개의 난수 생성 . array([2.16303879, 1.27818637, 1.32514334, 1.6161346 , 2.03926784, 1.50032755, 1.16183896, 1.51923212, 2.67152899, 1.2616228 ]) . np.random.randn . {python} np.random.randn(N) . np.random.randn(10) # 표준정규분포에서 10개 추출 . array([ 0.16284596, -1.41505923, -0.87931282, -1.96742692, -0.17715718, -0.18035526, 1.31177136, -1.02100905, -0.3559429 , 0.40319735]) . np.random.randn(10)*2 # 평균이 0이고 표준편차가 2인 정규분포 . array([ 0.29806358, 2.00020956, -0.5111455 , -3.0789904 , 2.98176489, 3.77815177, -1.25610359, -1.54689973, -2.11675118, 0.5415075 ]) . np.random.randn(10)*2 + 3 # 평균이 0이고 표준편차가 3인 정규분포 . array([ 0.19911518, 2.68233421, 3.80413328, 2.60169535, 2.48309103, 5.28444139, 5.77762188, 5.52430879, -0.17405269, 3.34573411]) . np.random.randint . 중복을 허용하지 않고 정수를 생성한다. | . np.random.randint(7) # [0,7)의 범위에서 정수 한 개 생성 . 1 . np.random.randint(7, size = (20,)) . array([1, 0, 2, 1, 4, 4, 0, 2, 3, 1, 3, 5, 2, 5, 6, 5, 6, 1, 1, 2]) . np.random.randint(7, size = (2,2)) # [0,7)의 범위 무작위 정수가 원소인 2x2 행렬을 생성한다. . array([[2, 1], [4, 6]]) . np.random.randint(low=10, high=20, size=(2,5)) # [10, 20)의 범위에서 정수 한 개 생성 . array([[16, 14, 13, 13, 11], [12, 11, 18, 14, 15]]) . Warning :np.random.randint(high = N)은 사용할 수 없다. . np.random.choice . 복원추출이 default | . np.random.choice(5,20) . array([3, 2, 4, 2, 0, 1, 1, 2, 1, 4, 3, 3, 3, 1, 0, 0, 0, 0, 2, 2]) . np.random.choice([&#39;apple&#39;, &#39;orange&#39;, &#39;banana&#39;], 20) . array([&#39;banana&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;banana&#39;, &#39;apple&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;banana&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;orange&#39;, &#39;apple&#39;, &#39;banana&#39;], dtype=&#39;&lt;U6&#39;) . np.random.choice(5, 2, replace = False) # Replace = False로 하면 비복원추출을 시행한다. . array([2, 3]) . np.random.binomial . np.random.binomial(n=10, p=0.2, size = (5,)) . array([2, 3, 3, 2, 2]) . np.random.normal . {python} np.random.normal(loc = mean, scale = stdev, size) default : np.random.normal(loc = 0, scale = 1, size = None) . np.random.normal(2, 3, 10) # 평균이 2이고 표준편차가 3인 정규분포 . array([-2.2383093 , 4.8091005 , 0.15352256, 4.9793976 , 0.33729707, 6.12970338, 5.18818041, -1.62767898, -1.60791145, 0.97458435]) . np.random.uniform . np.random.uniform(low=2, high=4, size = (5,)) # 균등분포 . array([2.51568342, 2.3858703 , 2.42784566, 3.65082018, 2.5756164 ]) . np.random.poisson(lam=5, size=(5,)) . array([2, 5, 4, 3, 3]) . np.random.poisson . np.random.poisson(lam=5, size=(5,)) . array([3, 7, 3, 4, 4]) . .corrcoef . np.random.seed(43052) x= np.random.randn(10000) y= np.random.randn(10000)*2 z= np.random.randn(10000)*0.5 np.corrcoef([x,y,z]).round(2) . array([[ 1. , -0.01, 0.01], [-0.01, 1. , 0. ], [ 0.01, 0. , 1. ]]) . .cov . np.cov([x,y,z]).round(2) . array([[ 0.99, -0.02, 0. ], [-0.02, 4.06, 0. ], [ 0. , 0. , 0.25]]) . .reshape . . Tip: R의 dim 함수와 유사하다. . A = np.array([11,22,33,44,55,66]) A . array([11, 22, 33, 44, 55, 66]) . A.reshape(2,3) . array([[11, 22, 33], [44, 55, 66]]) . A . array([11, 22, 33, 44, 55, 66]) . A = A.reshape(2,3) A . array([[11, 22, 33], [44, 55, 66]]) . note :reshape with -1 . A = np.arange(24) A . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . A.reshape(2,-1) # 행의 수가 2인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . A.reshape(4, -1) # 행의 수가 4인 행렬, 열은 알아서 맞춰 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . A.reshape(-1, 4) # 열의 수가 4인 행렬, 행은 알아서 맞춰 . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . A.reshape(-1) # 다시 길이가 24인 벡터로 . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . .shape . A = np.array(3.14) # Scalar, 0d array A.shape . () . A = np.array([3.14]) # Vector, 1d array A.shape . (1,) . A = np.array([[3.14]]) # Matrix, 2d array A.shape . (1, 1) . A = np.array([[[3.14]]]) # Tensor, 3d array A.shape . (1, 1, 1) . A = np.array([[1,2,3],[2,5,6],[4,4,2]]) A.shape . (3, 3) . .T . A = np.arange(4).reshape(2,2) A . array([[0, 1], [2, 3]]) . A.T # 전치행렬 . array([[0, 2], [1, 3]]) . np.linalg.inv . np.linalg.inv(A) # 역행렬 . array([[-1.5, 0.5], [ 1. , 0. ]]) . @ . A @ np.linalg.inv(A) . array([[1., 0.], [0., 1.]]) . np.concatenate . A = np.array([1,2]) B = np.array([4,5]) np.concatenate([A, B]) . array([1, 2, 4, 5]) . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B]) # axis = 0이 생략되어 있다. . array([[ 0, 1], [ 2, 3], [10, 11], [12, 13]]) . A=np.array(range(4)).reshape(2,2) B=np.array(range(2)).reshape(2,1) np.concatenate([a,b],axis=1) # 꼭 같은 차원일 필요는 없고, 붙여지는 부분의 길이만 같으면 됨 . NameError Traceback (most recent call last) Input In [82], in &lt;cell line: 3&gt;() 1 A=np.array(range(4)).reshape(2,2) 2 B=np.array(range(2)).reshape(2,1) -&gt; 3 np.concatenate([a,b],axis=1) NameError: name &#39;a&#39; is not defined . A = np.arange(4).reshape(2,2) B = np.arange(10,14).reshape(2,2) np.concatenate([A, B], axis=1) . A = np.array(range(2*3*4)).reshape(2,3,4) B = - A A, B . np.concatenate([A,B], axis=0) . np.concatenate([A,B], axis=1) . np.concatenate([A,B], axis=2) . np.stack . Warning : . A = np.array([1,2,3]) B = np.array([2,3,4]) np.concatenate([A,B], axis = 1) . AxisError Traceback (most recent call last) Input In [83], in &lt;cell line: 3&gt;() 1 A = np.array([1,2,3]) 2 B = np.array([2,3,4]) -&gt; 3 np.concatenate([A,B], axis = 1) File &lt;__array_function__ internals&gt;:180, in concatenate(*args, **kwargs) AxisError: axis 1 is out of bounds for array of dimension 1 . A = np.array([1,2,3]) B = np.array([2,3,4]) np.stack([A,B], axis=0) . array([[1, 2, 3], [2, 3, 4]]) . np.stack([A,B], axis=1) . array([[1, 2], [2, 3], [3, 4]]) . A = np.arange(3*4*5).reshape(3,4,5) B = - A A.shape, B.shape . ((3, 4, 5), (3, 4, 5)) . np.stack([A,B], axis = 0) . array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [-10, -11, -12, -13, -14], [-15, -16, -17, -18, -19]], [[-20, -21, -22, -23, -24], [-25, -26, -27, -28, -29], [-30, -31, -32, -33, -34], [-35, -36, -37, -38, -39]], [[-40, -41, -42, -43, -44], [-45, -46, -47, -48, -49], [-50, -51, -52, -53, -54], [-55, -56, -57, -58, -59]]]]) . . np.stack([A,B], axis = 0).shape # axis = 0 &lt;==&gt; axis = -4 . (2, 3, 4, 5) . np.stack([A,B], axis = 1).shape # axis = 1 &lt;==&gt; axis = -3 . (3, 2, 4, 5) . np.stack([A,B], axis = 2).shape # axis = 2 &lt;==&gt; axis = -2 . (3, 4, 2, 5) . np.stack([A,B], axis = 3).shape # axis = 3 &lt;==&gt; axis = -1 . (3, 4, 5, 2) . Difference between np.concatenate and np.stack . np.concatenate는 축의 총 개수를 유지하면서 결합한다. . np.stack은 축의 개수를 하나 증가시키면서 결합한다. . np.vstack . A = np.arange(6) B = - A . np.vstack([A, B]) . array([[ 0, 1, 2, 3, 4, 5], [ 0, -1, -2, -3, -4, -5]]) . np.hstack . A = np.arange(6) B = - A . np.hstack([A, B]) . array([ 0, 1, 2, 3, 4, 5, 0, -1, -2, -3, -4, -5]) . np.append . A = np.arange(30).reshape(5,6) B = - np.arange(8).reshape(2,2,2) A.shape, B.shape . ((5, 6), (2, 2, 2)) . np.append(A, B) # 다 풀어서 더한다. . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 0, -1, -2, -3, -4, -5, -6, -7]) . A = np.arange(2*3*4).reshape(2,3,4) B = - A A, B . (array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]), array([[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]])) . A.shape, B.shape, np.append(A, B, axis = 0).shape . ((2, 3, 4), (2, 3, 4), (4, 3, 4)) . np.append(A, B, axis = 0) . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]]) . np.diff . A = np.array([1,2,4,6,7]) np.diff(A) . array([1, 2, 2, 1]) . np.diff(np.diff(A)) . array([ 1, 0, -1]) . np.diff(A, prepend=100) # np.diff(np.array([100] + A.tolist()) ), 즉 맨 앞에 100을 추가하여 차분 . array([-99, 1, 2, 2, 1]) . np.diff(A, append=100) # np.diff(np.array(A.tolist() + [100]) ), 즉 맨 뒤에 100을 추가하여 차분 . array([ 1, 2, 2, 1, 93]) . A = np.arange(24).reshape(4,6) A . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . np.diff(A, axis = 0) . array([[6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6]]) . np.diff(A, axis = 1) . array([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]) . np.where . A = np.array([0,0,0,1,0]) np.where(A==1) # 조건을 만족하는 인덱스를 반환 . (array([3], dtype=int64),) . np.random.seed(43052) A = np.random.randn(12).reshape(3,4) np.where(A&lt;0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 . (array([1, 1, 2, 2, 2], dtype=int64), array([2, 3, 0, 1, 3], dtype=int64)) . A[np.where(A&lt;0)] . array([-1.66307542, -1.38277318, -1.92684484, -1.4862163 , -0.03488725]) . np.where(A&lt;0, 0, A) # 조건이 True이면 0, False이면 A . array([[0.38342049, 1.0841745 , 1.14277825, 0.30789368], [0.23778744, 0.35595116, 0. , 0. ], [0. , 0. , 0.00692519, 0. ]]) . np.where(A&lt;0, 5, 1) # 조건이 True이면 0, False이면 1 . array([[1, 1, 1, 1], [1, 1, 5, 5], [5, 5, 1, 5]]) . np.argwhere . A = np.array([0,0,0,1,0]) np.argwhere(A==1) . array([[3]], dtype=int64) . np.random.seed(43052) A = np.random.randn(12).reshape(3,4) np.argwhere(A&lt;0) # 조건을 만족하는 인덱스가 (1,2), (1,3), (2,0), (2,1), (2,3) 이라는 의미 . array([[1, 2], [1, 3], [2, 0], [2, 1], [2, 3]], dtype=int64) . A[np.argwhere(A&lt;0)] # 불가능 . IndexError Traceback (most recent call last) Input In [106], in &lt;cell line: 1&gt;() -&gt; 1 A[np.argwhere(A&lt;0)] IndexError: index 3 is out of bounds for axis 0 with size 3 . Difference between np.where and np.argwhere . np.where는 인덱스의 좌표를 읽는 가독성이 떨어지지만 조건에 맞는 원소를 출력하거나 처리하기에 좋다. . np.argwhere는 인덱스의 좌표를 읽는 가독성은 좋지만 조건에 맞는 원소를 출력하거나 처리하기에 좋지 못하다. . np.ix_ . A = np.arange(12).reshape(3,4) A . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . A[[0,1], [0,1]]이 A[0:2, 0:2]를 의미하게 하려면 아레와 같이 np.ix_를 사용한다. | . A[np.ix_([0,1],[0,1])] . array([[0, 1], [4, 5]]) . Operations . .sum . A = np.array([1,2,3]) A.sum() . 6 . .mean . A = np.array([1,2,3]) A.mean() . 2.0 . .min . A = np.array([1,2,3]) A.min() . 1 . .max . A = np.array([1,2,3]) A.max() . 3 . .prod . A = np.array([1,2,3]) A.prod() . 6 . .std . A = np.arange(1,20) A.std() # 분포를 n으로 나누는 것이 default이다. . 5.477225575051661 . A = A = np.array([1,2,3]) A.std(ddof=1) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.0 . A = A = np.array([1,2,3]) A.std(ddof=2) # ddof 옵션을 사용하여 분포를 n-1로 나눈다. . 1.4142135623730951 . .argmin . A = np.array([1,2,3]) # 가장 작은 값의 인덱스를 리턴 A.argmin() . 0 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmin(axis = 0) . array([2, 1, 1, 1, 2], dtype=int64) . A.argmin(axis = 1) . array([4, 3, 4, 3], dtype=int64) . .argmax . A = np.array([1,2,3]) # 가장 큰 값의 인덱스를 리턴 A.argmax() . 2 . np.random.seed(43052) A = np.random.randn(4*5).reshape(4,5) A . array([[ 0.38342049, 1.0841745 , 1.14277825, 0.30789368, 0.23778744], [ 0.35595116, -1.66307542, -1.38277318, -1.92684484, -1.4862163 ], [ 0.00692519, -0.03488725, -0.34357323, 0.70895648, -1.55100608], [ 1.34565583, -0.05654272, -0.83017342, -1.46395159, -0.35459593]]) . A.argmax(axis = 0) . array([3, 0, 0, 2, 0], dtype=int64) . A.argmax(axis = 1) . array([2, 0, 3, 0], dtype=int64) . .cumsum . A = np.array([1,2,3,4]) A.cumsum() # 누적합 . array([ 1, 3, 6, 10]) . .cumprod . A = np.array([1,2,3,4]) A.cumprod() # 누적곱 . array([ 1, 2, 6, 24]) . A = np.array([2**i for i in np.arange(10)]) A . array([ 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]) . A.cumprod() . array([ 1, 2, 8, 64, 1024, 32768, 2097152, 268435456, 0, 0]) . Applications . Solving System of Equations . $ begin{cases} y+z+w = 3 x+z+w = 3 x+y+w = 3 x+y+z = 3 end{cases}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix} begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . $ begin{bmatrix} x y z w end{bmatrix} = begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 1 &amp; 0 &amp; 1 &amp; 1 1 &amp; 1 &amp; 0 &amp; 1 1 &amp; 1 &amp; 1 &amp; 0 end{bmatrix}^{ ,-1} begin{bmatrix} 3 3 3 3 end{bmatrix}$ . v = &#39;x&#39;, &#39;y&#39;, &#39;z&#39; A = np.linalg.inv(np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])) B = np.array([3,3,3,3]).reshape(4,1) answer = A@B.reshape(-1) for v, i in zip(v, answer): print(v, &#39;:&#39;, i) . x : 1.0 y : 1.0 z : 1.0 .",
            "url": "https://stahangryum.github.io/Woo/python/python%20tutorial/2022/04/11/numpy_highlight.html",
            "relUrl": "/python/python%20tutorial/2022/04/11/numpy_highlight.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Pima Indians Diabetes Prediction",
            "content": "Reference . ref. https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database . Pima Indians Diabetes Prediction . Variable Definition . Pregnancies | 임신 횟수 | . Glucose | 포도당 부하 검사 수치 | . BloodPressure | 혈압 | . SkinThickness | 팔 삼두근 뒤쪽의 피하지방 측정값(mm) | . Inlulin | 혈청 인슐린(mu U/ml) | . BMI | 체질량지수$( frac{kg}{m^2})$ | . DiabetesPredigreeFunction | 당뇨 내력 가중치 값 | . Age | 나이 | . Outcome | 클래스 결정 값(0 또는 1) | . Packages . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import Binarizer import warnings warnings.filterwarnings(action=&#39;ignore&#39;) . Preprocessing . diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) diabetes_data.head() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . 3 1 | 89 | 66 | 23 | 94 | 28.1 | 0.167 | 21 | 0 | . 4 0 | 137 | 40 | 35 | 168 | 43.1 | 2.288 | 33 | 1 | . diabetes_data.Outcome.value_counts() . 0 500 1 268 Name: Outcome, dtype: int64 . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . Glucose, BloodPressure, SkinThickness, Insulin, Bmi은 0이면 안된다. | . zero_features = [&#39;Glucose&#39;, &#39;BloodPressure&#39;, &#39;SkinThickness&#39;, &#39;Insulin&#39;, &#39;BMI&#39;] for i, feature in enumerate(zero_features): plt.subplot(3,2,i+1) plt.hist(diabetes_data[feature]) . diabetes_data[diabetes_data[&#39;Glucose&#39;] == 0][&#39;Glucose&#39;].count() . 5 . for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature] == 0][feature].count() print(&#39;{0} 0 건수는 {1}, 퍼센트는 {2:.2f}%&#39;.format(feature, zero_count, 100*zero_count / diabetes_data[feature].count())) . Glucose 0 건수는 5, 퍼센트는 0.65% BloodPressure 0 건수는 35, 퍼센트는 4.56% SkinThickness 0 건수는 227, 퍼센트는 29.56% Insulin 0 건수는 374, 퍼센트는 48.70% BMI 0 건수는 11, 퍼센트는 1.43% . SkinThickness, Insulin feature가 0인 행을 지우면 데이터 손실이 너무 크므로 평균값으로 대체한다. . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features] = diabetes_data[zero_features].replace(0, mean_zero_features) . Prediction . def precision_recall_curve_plot(y_test, pred_proba_c1): precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1) plt.figure(figsize=(8,6)) threshold_boundary = thresholds.shape[0] plt.plot(thresholds, precisions[0:threshold_boundary], linestyle = &#39;-&#39;, label = &#39;precision&#39;) plt.plot(thresholds, recalls[0:threshold_boundary], label = &#39;recall&#39;) start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1), 2)) plt.xlabel(&#39;Threshold value&#39;); plt.ylabel(&#39;Precision and Recall value&#39;) plt.legend(); plt.grid() plt.show() def get_clf_eval(y_test, pred=None, pred_proba=None): # 모델 평가 함수 confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) f1 = f1_score(y_test, pred) roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) print(&#39;정확도 : {0:.3f}, 정밀도 : {1:.3f}, 재현율 : {2:.3f}, F1 : {3:.3f}, AUC : {4:.3f}&#39;.format( accuracy, precision, recall, f1, roc_auc)) def get_eval_by_threshold(y_test, pred_proba_c1, thresholds): for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_predict = binarizer.transform(pred_proba_c1) print(&#39;-&#39;) print(&#39;임계값:&#39;, round(custom_threshold,2)) get_clf_eval(y_test, custom_predict, pred_proba_c1) . . feature_name = diabetes_data.columns[:-1] target_name = diabetes_data.columns[-1] X = diabetes_data.loc[:, feature_name] y = diabetes_data.loc[:, target_name] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y) . lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba = lr_clf.predict_proba(X_test)[:, 1] get_clf_eval(y_test, pred, pred_proba) . 오차 행렬 [[90 10] [21 33]] 정확도 : 0.799, 정밀도 : 0.767, 재현율 : 0.611, F1 : 0.680, AUC : 0.845 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1] precision_recall_curve_plot(y_test, pred_proba_c1) . thresholds = np.arange(0.3, 0.5, 0.03) pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:, 1].reshape(-1,1), thresholds) . - 임계값: 0.3 오차 행렬 [[67 33] [11 43]] 정확도 : 0.714, 정밀도 : 0.566, 재현율 : 0.796, F1 : 0.662, AUC : 0.845 - 임계값: 0.33 오차 행렬 [[73 27] [12 42]] 정확도 : 0.747, 정밀도 : 0.609, 재현율 : 0.778, F1 : 0.683, AUC : 0.845 - 임계값: 0.36 오차 행렬 [[76 24] [15 39]] 정확도 : 0.747, 정밀도 : 0.619, 재현율 : 0.722, F1 : 0.667, AUC : 0.845 - 임계값: 0.39 오차 행렬 [[79 21] [17 37]] 정확도 : 0.753, 정밀도 : 0.638, 재현율 : 0.685, F1 : 0.661, AUC : 0.845 - 임계값: 0.42 오차 행렬 [[84 16] [18 36]] 정확도 : 0.779, 정밀도 : 0.692, 재현율 : 0.667, F1 : 0.679, AUC : 0.845 - 임계값: 0.45 오차 행렬 [[85 15] [18 36]] 정확도 : 0.786, 정밀도 : 0.706, 재현율 : 0.667, F1 : 0.686, AUC : 0.845 - 임계값: 0.48 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 . . binarizer = Binarizer(threshold=0.48) pred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) get_clf_eval(y_test, pred_th_048, pred_proba[:, 1]) . 오차 행렬 [[89 11] [19 35]] 정확도 : 0.805, 정밀도 : 0.761, 재현율 : 0.648, F1 : 0.700, AUC : 0.845 .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/07/pima.html",
            "relUrl": "/kaggle/2022/04/07/pima.html",
            "date": " • Apr 7, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Titanic Survivor Prediction",
            "content": "Reference . ref. https://www.kaggle.com/c/titanic/ . Titanic Survivor Prediction . 타이타닉호 침몰 사고 당시 탑승자들의 정보를 활용하여 생존자를 예측하라. . Data Dictionary . Variable Definition Key . Survived | Survival | 0 = No, 1 = Yes | . Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd | . Sex | Sex | . Age | Age in years | . SibSp | # of siblings / spouses aboard the Titanic | . Parch | # of parents / children aboard the Titanic | . Ticket | Ticket number | . Fare | Passenger fare | . Cabin | Cabin number | . Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | . Variable Notes . pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower . age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 . sibsp: The dataset defines family relations in this way... Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fiancés were ignored) . parch: The dataset defines family relations in this way... Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them. . Stage I . import . import numpy as np import pandas as pd . code . import os print(os.getcwd()) . C: Users godgk Desktop Project kaggle Titanic . train = pd.read_csv(&#39;data/train.csv&#39;) test = pd.read_csv(&#39;data/test.csv&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . print(&#39;train data shape&#39;, train.shape) print(&#39;test data shape&#39;, test.shape) print(&#39;--[train infomation]--&#39;) print(train.info()) print(&#39;--[test infomation]--&#39;) print(test.info()) . train data shape (891, 12) test data shape (418, 11) --[train infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB None --[test infomation]-- &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB None . train.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . test.isnull().sum() . PassengerId 0 Pclass 0 Name 0 Sex 0 Age 86 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 . Stage II . import . import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set() . def pie_chart(feature): feature_ratio = train[feature].value_counts(sort=False) feature_size = feature_ratio.size feature_index = feature_ratio.index survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() plt.plot(aspect=&#39;auto&#39;) plt.pie(feature_ratio, labels=feature_index, autopct=&#39;%1.1f%%&#39;) plt.title(feature + &#39; &#39;s ratio in total&#39;) plt.show() for i, index in enumerate(feature_index): plt.subplot(1, feature_size + 1, i + 1, aspect=&#39;equal&#39;) plt.pie([survived[index], dead[index]], labels=[&#39;Survivied&#39;, &#39;Dead&#39;], autopct=&#39;%1.1f%%&#39;) plt.title(str(index) + &#39; &#39;s ratio&#39;) plt.show() . pie_chart(&quot;Sex&quot;) . 남성 탑승객이 여성 탑승객보다 많다. . | 여성 탑승객의 생존 비율이 남성 탑승객보다 높다. . | . pie_chart(&quot;Pclass&quot;) . 1등실 2등실 3등실 순으로 생존 비율이 높다. | . pie_chart(&quot;Embarked&quot;) . train[&#39;Ticket&#39;][0:50] . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 5 330877 6 17463 7 349909 8 347742 9 237736 10 PP 9549 11 113783 12 A/5. 2151 13 347082 14 350406 15 248706 16 382652 17 244373 18 345763 19 2649 20 239865 21 248698 22 330923 23 113788 24 349909 25 347077 26 2631 27 19950 28 330959 29 349216 30 PC 17601 31 PC 17569 32 335677 33 C.A. 24579 34 PC 17604 35 113789 36 2677 37 A./5. 2152 38 345764 39 2651 40 7546 41 11668 42 349253 43 SC/Paris 2123 44 330958 45 S.C./A.4. 23567 46 370371 47 14311 48 2662 49 349237 Name: Ticket, dtype: object . train.Ticket . 0 A/5 21171 1 PC 17599 2 STON/O2. 3101282 3 113803 4 373450 ... 886 211536 887 112053 888 W./C. 6607 889 111369 890 370376 Name: Ticket, Length: 891, dtype: object . Stage 3 . def bar_chart(feature): survived = train[train[&#39;Survived&#39;] == 1][feature].value_counts() dead = train[train[&#39;Survived&#39;] == 0][feature].value_counts() df = pd.DataFrame([survived, dead]) df.index = [&#39;Survived&#39;, &#39;Dead&#39;] df.plot(kind=&#39;bar&#39;, stacked=True, figsize=(10,5)) . bar_chart(&quot;SibSp&quot;) . bar_chart(&quot;Parch&quot;) . Data Preprocessing . train_and_test = [train, test] . Name Feature . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset.Name.str.extract(&#39; ([A-Za-z]+) .&#39;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Title . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | Mr | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | Mrs | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | Miss | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | Mrs | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | Mr | . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Capt 0 | 1 | . Col 0 | 2 | . Countess 1 | 0 | . Don 0 | 1 | . Dr 1 | 6 | . Jonkheer 0 | 1 | . Lady 1 | 0 | . Major 0 | 2 | . Master 0 | 40 | . Miss 182 | 0 | . Mlle 2 | 0 | . Mme 1 | 0 | . Mr 0 | 517 | . Mrs 125 | 0 | . Ms 1 | 0 | . Rev 0 | 6 | . Sir 0 | 1 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Capt&#39;, &#39;Col&#39;, &#39;Countess&#39;, &#39;Don&#39;,&#39;Dona&#39;, &#39;Dr&#39;, &#39;Jonkheer&#39;,&#39;Lady&#39;,&#39;Major&#39;, &#39;Rev&#39;, &#39;Sir&#39;], &#39;Other&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace([&#39;Mlle&#39;, &#39;Ms&#39;], &#39;Miss&#39;) dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].replace(&#39;Mme&#39;, &#39;Mrs&#39;) . pd.crosstab(train[&#39;Title&#39;], train[&#39;Sex&#39;]) . Sex female male . Title . Master 0 | 40 | . Miss 185 | 0 | . Mr 0 | 517 | . Mrs 126 | 0 | . Other 3 | 20 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;).mean() . Survived . Title . Master 0.575000 | . Miss 0.702703 | . Mr 0.156673 | . Mrs 0.793651 | . Other 0.347826 | . train[[&#39;Title&#39;, &#39;Survived&#39;]].groupby(&#39;Title&#39;, as_index = False).mean() # as_index = True이면 Title이 index로 작용한다. . Title Survived . 0 Master | 0.575000 | . 1 Miss | 0.702703 | . 2 Mr | 0.156673 | . 3 Mrs | 0.793651 | . 4 Other | 0.347826 | . for dataset in train_and_test: dataset[&#39;Title&#39;] = dataset[&#39;Title&#39;].astype(str) . Sex Feature . for dataset in train_and_test: dataset[&#39;Sex&#39;] = dataset[&#39;Sex&#39;].astype(str) . Embarked Feature . train.Embarked.value_counts(dropna=False) . S 644 C 168 Q 77 NaN 2 Name: Embarked, dtype: int64 . for dataset in train_and_test: dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].fillna(&#39;S&#39;) dataset[&#39;Embarked&#39;] = dataset[&#39;Embarked&#39;].astype(str) . Age Feature . Binning . train.Age.isna().sum() . 177 . for dataset in train_and_test: dataset[&#39;Age&#39;].fillna(dataset[&#39;Age&#39;].mean(), inplace=True) dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].astype(int) train[&#39;AgeBand&#39;] = pd.cut(train[&#39;Age&#39;], 5) train[[&#39;AgeBand&#39;, &#39;Survived&#39;]].groupby([&#39;AgeBand&#39;], as_index=False).mean() . AgeBand Survived . 0 (-0.08, 16.0] | 0.550000 | . 1 (16.0, 32.0] | 0.344762 | . 2 (32.0, 48.0] | 0.403226 | . 3 (48.0, 64.0] | 0.434783 | . 4 (64.0, 80.0] | 0.090909 | . for dataset in train_and_test: dataset.loc[ dataset[&#39;Age&#39;] &lt;= 16, &#39;Age&#39;] = 0 dataset.loc[(dataset[&#39;Age&#39;] &gt; 16) &amp; (dataset[&#39;Age&#39;] &lt;= 32), &#39;Age&#39;] = 1 dataset.loc[(dataset[&#39;Age&#39;] &gt; 32) &amp; (dataset[&#39;Age&#39;] &lt;= 48), &#39;Age&#39;] = 2 dataset.loc[(dataset[&#39;Age&#39;] &gt; 48) &amp; (dataset[&#39;Age&#39;] &lt;= 64), &#39;Age&#39;] = 3 dataset.loc[ dataset[&#39;Age&#39;] &gt; 64, &#39;Age&#39;] = 4 dataset[&#39;Age&#39;] = dataset[&#39;Age&#39;].map( { 0:&#39;Child&#39;, 1:&#39;Young&#39;, 2:&#39;Middle&#39;, 3:&#39;Prime&#39;, 4:&#39;Old&#39; } ).astype(str) . Fare Feature . for dataset in train_and_test: print(dataset[&#39;Fare&#39;].isna().sum()) . 0 1 . train[[&#39;Pclass&#39;, &#39;Fare&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean() . Pclass Fare . 0 1 | 84.154687 | . 1 2 | 20.662183 | . 2 3 | 13.675550 | . test[test[&#39;Fare&#39;].isna()][&#39;Pclass&#39;] . 152 3 Name: Pclass, dtype: int64 . for dataset in train_and_test: dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].fillna(13.675) # Pclass가 3인 승객의 평균 Fare . train[&#39;FareBand&#39;] = pd.qcut(train[&#39;Fare&#39;], 5) for dataset in train_and_test: dataset.loc[ dataset[&#39;Fare&#39;] &lt;= 7.854, &#39;Fare&#39;] = 0 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 7.854) &amp; (dataset[&#39;Fare&#39;] &lt;= 10.5), &#39;Fare&#39;] = 1 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 10.5) &amp; (dataset[&#39;Fare&#39;] &lt;= 21.679), &#39;Fare&#39;] = 2 dataset.loc[(dataset[&#39;Fare&#39;] &gt; 21.679) &amp; (dataset[&#39;Fare&#39;] &lt;= 39.688), &#39;Fare&#39;] = 3 dataset.loc[ dataset[&#39;Fare&#39;] &gt; 39.688, &#39;Fare&#39;] = 4 dataset[&#39;Fare&#39;] = dataset[&#39;Fare&#39;].map( { 0:&#39;XS&#39;, 1:&#39;S&#39;, 2:&#39;M&#39;, 3:&#39;L&#39;, 4:&#39;XL&#39; } ).astype(str) . SibSp &amp; Parch Feature (Family) . for dataset in train_and_test: dataset[&#39;Family&#39;] = dataset[&#39;Parch&#39;] + dataset[&#39;SibSp&#39;] dataset[&#39;Family&#39;] = dataset[&#39;Family&#39;].astype(int) . Other Feature . features_drop = [&#39;Name&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;SibSp&#39;, &#39;Parch&#39;] train = train.drop(features_drop, axis = 1) test = test.drop(features_drop, axis = 1) train = train.drop([&#39;PassengerId&#39;, &#39;AgeBand&#39;, &#39;FareBand&#39;], axis = 1) . train.head() . Survived Pclass Sex Age Fare Embarked Title Family . 0 0 | 3 | male | Young | XS | S | Mr | 1 | . 1 1 | 1 | female | Middle | XL | C | Mrs | 1 | . 2 1 | 3 | female | Young | S | S | Miss | 0 | . 3 1 | 1 | female | Middle | XL | S | Mrs | 1 | . 4 0 | 3 | male | Middle | S | S | Mr | 0 | . test.head() . PassengerId Pclass Sex Age Fare Embarked Title Family . 0 892 | 3 | male | Middle | XS | Q | Mr | 0 | . 1 893 | 3 | female | Middle | XS | S | Mrs | 1 | . 2 894 | 2 | male | Prime | S | Q | Mr | 0 | . 3 895 | 3 | male | Young | S | S | Mr | 0 | . 4 896 | 3 | female | Young | M | S | Mrs | 2 | . train = pd.get_dummies(train) test = pd.get_dummies(test) train_label = train[&#39;Survived&#39;] train_data = train.drop(&#39;Survived&#39;, axis = 1) test_data = test.drop(&#39;PassengerId&#39;, axis = 1).copy() . print(train_data.shape, train_label.shape, test_data.shape) . (891, 22) (891,) (418, 22) . train . Survived Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 0 | 3 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 1 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 1 | 3 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 3 1 | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 4 0 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 887 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 888 0 | 3 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 889 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 890 0 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 891 rows × 23 columns . test . PassengerId Pclass Family Sex_female Sex_male Age_Child Age_Middle Age_Old Age_Prime Age_Young ... Fare_XL Fare_XS Embarked_C Embarked_Q Embarked_S Title_Master Title_Miss Title_Mr Title_Mrs Title_Other . 0 892 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 893 | 3 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 2 894 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 3 895 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 4 896 | 3 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 414 1306 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | ... | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 415 1307 | 3 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 416 1308 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 417 1309 | 3 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | ... | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 418 rows × 23 columns . Learning . import . !pip install scikit-learn . Requirement already satisfied: scikit-learn in c: users godgk anaconda3 envs py39r40 lib site-packages (1.0.2) Requirement already satisfied: numpy&gt;=1.14.6 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.20.3) Requirement already satisfied: scipy&gt;=1.1.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.7.1) Requirement already satisfied: joblib&gt;=0.11 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in c: users godgk anaconda3 envs py39r40 lib site-packages (from scikit-learn) (3.1.0) . from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn.utils import shuffle . train_data, train_label = shuffle(train_data, train_label, random_state = 5) . def train_and_test(model): model.fit(train_data, train_label) prediction = model.predict(test_data) accuracy = round(model.score(train_data, train_label) * 100, 2) print(&quot;Accuracy : &quot;, accuracy, &quot;%&quot;) return prediction . log_pred = train_and_test(LogisticRegression()) # SVM svm_pred = train_and_test(SVC()) #kNN knn_pred_4 = train_and_test(KNeighborsClassifier(n_neighbors = 4)) # Random Forest rf_pred = train_and_test(RandomForestClassifier(n_estimators=100)) # Navie Bayes nb_pred = train_and_test(GaussianNB()) . Accuracy : 82.27 % Accuracy : 83.61 % Accuracy : 84.74 % Accuracy : 88.55 % Accuracy : 79.35 % . submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: rf_pred }) submission.to_csv(&#39;submission_rf.csv&#39;, index=False) .",
            "url": "https://stahangryum.github.io/Woo/kaggle/2022/04/06/Titanic.html",
            "relUrl": "/kaggle/2022/04/06/Titanic.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "My Setting",
            "content": "미완 | . Window Setting . Anaconda Setting . https://www.anaconda.com/products/individual . conda create -n py39r41 python=3.9 . | conda activate py39r41 . | conda install -c conda-forge r-essentials=4.1 . | conda install -c conda-forge jupyterlab . | R . | install.packages(&quot;IRkernel&quot;) . | library(IRkernel) . | installspec() . | q() . | pip install numpy . | pip install pandas . | pip install SciPy . | pip install matplotlib . | etc . | . . Git Setting . Git . | Git Bash . | GitHub Desktop . | . {python} !git add . !git commit -m . !git push .",
            "url": "https://stahangryum.github.io/Woo/2022/03/15/anaconda_install.html",
            "relUrl": "/2022/03/15/anaconda_install.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Monte Carlo Integration with Python",
            "content": "Monte Carlo Integration . Problem . Find $ int_0^1 (x + sin( pi x)) ,dx$. . Solution I - Analytic Sol . $ int_0^1 (x + sin( pi x)) ,dx = left[ cfrac{1}{2} x^2- cfrac{1}{ pi}cos( pi x) right]_0^1 = cfrac{1}{2} + cfrac{1}{ pi} + cfrac{1}{ pi} = cfrac{1}{2}+ cfrac{2}{ pi}$ . Solution II - Monte Carlo Integration in Python . import numpy as np import matplotlib.pyplot as plt %matplotlib inline . def function(x): return x + np.sin(np.pi*x) x = np.linspace(0, 1, 10000) y = [function(x) for x in x] plt.plot(x, y) plt.show() . def function(x): #함수 정의 return x + np.sin(np.pi*x) N = 5000 # Random Sampling 시행 횟수 width = 1 # 사각형의 가로 길이 height = 1.6 #사각형의 세로 길이 X = np.random.random(N) # 0~1까지의 x 좌표 Random Sampling을 N번 시행 rand_Y = height * np.random.random(N) # 그래프상 최솟값 ~ 최댓값까지의 y 좌표 Randon Sampling을 N번 시행 in_or_out = rand_Y &lt; function(X) # rand_Y &lt; FX (IN)이면 True, Y &gt; F (OUT)이면 False A = height * width * np.sum(in_or_out) / N # 영역 S의 넓이 print(&#39;이론적으로 구한 값 : {0} n샘플링으로 구한 값 : {1}&#39;.format(1/2 + 2/np.pi, A)) . 이론적으로 구한 값 : 1.1366197723675815 샘플링으로 구한 값 : 1.12864 . Visualization . from matplotlib.animation import FuncAnimation . color = list(map(lambda x: &#39;blue&#39; if x == True else &#39;red&#39;, in_or_out)) #색 정하기 x = np.linspace(0, 1, 10000) #함수 그리기 y = [function(x) for x in x] plt.plot(x, y, color = &#39;black&#39;) plt.scatter(X, rand_Y, color = color, s=1, label=&#39;A = {0}&#39;.format(np.round(A, 4))) plt.legend(loc = &#39;lower right&#39;) #범례(legend) 위치 plt.plot([0, width], [0, 0], color=&#39;black&#39;) # 사각형 영역 plt.plot([width, width], [0, height], color=&#39;black&#39;) plt.plot([0, width], [height, height], color=&#39;black&#39;) plt.plot([0, 0], [0, height], color=&#39;black&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.show() .",
            "url": "https://stahangryum.github.io/Woo/python/2021/10/03/%EB%AA%AC%ED%85%8C-%EC%B9%B4%EB%A5%BC%EB%A1%9C.html",
            "relUrl": "/python/2021/10/03/%EB%AA%AC%ED%85%8C-%EC%B9%B4%EB%A5%BC%EB%A1%9C.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Taylor & Maclaurin Series",
            "content": "Taylor Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(a)}{n!}(x-a)^n qquad quad= f(a) + cfrac{f&#39;(a)}{1!}(x-a) + cfrac{f&#39;&#39;(a)}{2!}(x-a) + cfrac{f&#39;&#39;&#39;(a)}{3!}(x-a)+ cdots $ . Maclaurin Series . $ f(x) = sum_{n=0}^ infty cfrac{f^{(n)}(0)}{n!}(x-0)^n qquad quad = f(a) + cfrac{f^{ prime}(0)}{1!}(x-0) + cfrac{f^{ prime prime}(0)}{2!}(x-0) + cfrac{f^{ prime prime prime}(0)}{3!}(x-0)+ cdots $ . Examples . $ cfrac{1}{1-x} = sum_{n=0}^ infty{x^n} = 1+x+x^2+x^3+ cdots qquad R = 1 , e^x = sum_{n=0}^ infty cfrac{x^n}{n!} = 1 + cfrac{x}{1!} + cfrac{x^2}{2!} + cfrac{x^3}{3!}+ cdots qquad R = infty , sin ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)!} = x - cfrac{x^3}{3!} + cfrac{x^5}{5!} - cfrac{x^7}{7!}+ cdots qquad R = infty , cos ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n}}{(2n)!} = 1 - cfrac{x^2}{2!} + cfrac{x^4}{4!} - cfrac{x^6}{6!}+ cdots qquad R = infty , tan^{-1} ,x = sum_{n=0}^ infty(-1)^n cfrac{x^{2n+1}}{(2n+1)} = x - cfrac{x^3}{3} + cfrac{x^5}{5} - cfrac{x^7}{7}+ cdots qquad R = 1 , ln(1+x) = sum_{n=1}^ infty(-1)^{n-1} cfrac{x^{n}}{n} = x - cfrac{x^2}{2} + cfrac{x^3}{3} - cfrac{x^4}{4}+ cdots qquad R = 1 $ .",
            "url": "https://stahangryum.github.io/Woo/calculus/2021/10/02/%ED%85%8C%EC%9D%BC%EB%9F%AC-%EA%B8%89%EC%88%98.html",
            "relUrl": "/calculus/2021/10/02/%ED%85%8C%EC%9D%BC%EB%9F%AC-%EA%B8%89%EC%88%98.html",
            "date": " • Oct 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://stahangryum.github.io/Woo/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://stahangryum.github.io/Woo/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Kim Jeewoo . Jeonbuk Nation University. Statistics . GitHub . | LinkedIn . | Blog . | . . Contact . stahangryum@gmail.com .",
          "url": "https://stahangryum.github.io/Woo/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stahangryum.github.io/Woo/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}